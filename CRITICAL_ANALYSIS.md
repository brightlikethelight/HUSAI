# üö® CRITICAL ANALYSIS: SAE Stability Research

**Date:** December 5, 2025  
**Status:** ‚ö†Ô∏è MAJOR ISSUES DISCOVERED  
**Action Required:** Significant revisions needed before publication

---

## üî¥ CRITICAL FINDING: Random Baseline = Trained SAEs

### The Discovery

We ran a random baseline experiment comparing PWMCC between **randomly initialized SAEs** (no training):

```
Random SAE PWMCC:   0.3000 ¬± 0.0007
Trained SAE PWMCC:  0.3000 ¬± 0.001
Difference:         ~0.0000
```

**This means our trained SAEs show NO MORE feature consistency than random initialization!**

### Implications

1. **The 0.30 PWMCC is NOT evidence of learned structure** - it's simply the expected value for random vectors in this dimensional space

2. **Our main finding needs complete reinterpretation:**
   - OLD interpretation: "SAEs learn unstable features (0.30 vs 0.70 threshold)"
   - NEW interpretation: "SAEs don't learn ANY consistent features beyond random chance"

3. **This is actually a MORE severe finding** - the problem isn't low stability, it's ZERO stability above baseline

### Why This Happens

For random unit vectors in d-dimensional space:
- Expected |cos_sim| between two random vectors ‚âà sqrt(2/œÄ) / sqrt(d) ‚âà 0.07 for d=128
- But with 1024 features, we take the MAX over 1024 comparisons
- The max of 1024 random cosines is much higher (~0.30)
- This is a statistical artifact, not learned structure

---

## üü° Paper Inaccuracies Found

### Issue 1: Wrong Expansion Factor

**Paper says:** "expansion factor 32√ó (d_sae=4096)"  
**Actual:** expansion factor 8√ó (d_sae=1024)

**Location:** Paper Section 3.1, line ~81

### Issue 2: Wrong Cohen's d Interpretation

**Paper says:** "Effect size: Cohen's d = 1.92" (implies negligible)  
**Reality:** Cohen's d = 1.92 is a LARGE effect size

**Correct interpretation:** "Cohen's d = 1.92 (large effect size), but the absolute difference of 0.002 PWMCC is practically negligible"

### Issue 3: Sparsity Levels NOT Matched

**Paper implies:** Matched comparison between TopK and ReLU  
**Actual:**
- TopK L0 = 32 (exactly 32 active features)
- ReLU L0 = 427 (13√ó more active features!)

This is a significant confound that should be acknowledged.

### Issue 4: Missing Random Baseline

The paper compares to Paulo & Belrose's 0.30 finding but doesn't establish what random chance would give. Now we know: random = 0.30.

---

## üü¢ What's Still Valid

Despite the critical issues, some findings remain valid:

1. **Architecture independence is real:** Both TopK and ReLU give ~0.30, matching random baseline
   - This confirms neither architecture learns stable features

2. **Tight variance is real:** std < 0.001 across seeds
   - This confirms the phenomenon is systematic, not noisy

3. **Decoupling is real:** High reconstruction (EV > 0.92) with random-level stability
   - This is actually MORE concerning than originally thought

4. **Literature validation:** Our 0.30 matches Paulo & Belrose
   - But now we know this might be random baseline for them too!

---

## üìä Revised Interpretation

### Original Narrative (INCORRECT)
> "SAEs learn features with 0.30 consistency, which is low compared to the 0.70 threshold, but represents some learned structure."

### Revised Narrative (CORRECT)
> "SAEs trained with standard methods show feature consistency indistinguishable from random initialization (PWMCC ‚âà 0.30 for both). This suggests current SAE training does NOT produce reproducible features - the decoder weights are essentially as random after training as before."

### This Changes the Paper's Message

**Old message:** "Feature stability is low (0.30) but exists"  
**New message:** "Feature stability is ZERO above random baseline"

This is actually a **stronger and more important finding**, but requires complete reframing.

---

## üî¨ Recommended Next Experiments

### PRIORITY 1: Verify Random Baseline Finding ‚≠ê‚≠ê‚≠ê

```python
# Test with different d_model and d_sae combinations
# Verify the 0.30 is due to max-over-many-features effect
configs = [
    (128, 512),   # Smaller d_sae
    (128, 2048),  # Larger d_sae  
    (256, 1024),  # Larger d_model
    (64, 1024),   # Smaller d_model
]
```

**Hypothesis:** Random PWMCC should scale with d_sae (more features = higher max)

### PRIORITY 2: Alternative Stability Metrics ‚≠ê‚≠ê‚≠ê

PWMCC may be flawed for this comparison. Try:

1. **Hungarian matching + threshold:** Count features with match > 0.7
2. **Mutual nearest neighbors:** Only count if A‚ÜíB AND B‚ÜíA
3. **Activation correlation:** Compare which inputs activate which features
4. **Subspace overlap:** Do SAEs span similar subspaces?

### PRIORITY 3: Feature-Level Analysis ‚≠ê‚≠ê

```python
# For each feature in SAE1:
# 1. Find best match in SAE2
# 2. Check if match is significantly above random
# 3. Count "truly matched" features (not just max-of-random)

threshold = random_baseline + 3 * random_std  # ~0.30 + 0.002 = 0.302
truly_matched = (max_similarities > threshold).sum()
```

### PRIORITY 4: Activation-Based Stability ‚≠ê‚≠ê

Instead of comparing decoder weights, compare:
- Which inputs activate which features
- Do the same inputs activate "matched" features across seeds?

```python
# For each input x:
# 1. Get top-k active features in SAE1
# 2. Get top-k active features in SAE2
# 3. Check if matched features are both active
```

### PRIORITY 5: Matched Sparsity Experiment ‚≠ê

Train ReLU with higher L1 to match TopK's L0=32:
```python
l1_values = [0.005, 0.01, 0.02, 0.05, 0.1]
# Find L1 that gives L0 ‚âà 32
# Then compare PWMCC
```

### PRIORITY 6: Training Dynamics Analysis ‚≠ê

Track PWMCC during training:
```python
# Every 100 steps:
# 1. Save checkpoint
# 2. Compute PWMCC vs other seeds at same step
# 3. Plot PWMCC over training

# Question: Does PWMCC ever rise above random, then fall back?
```

---

## üìù Paper Revision Plan

### Option A: Reframe as Stronger Negative Result

**New title:** "Sparse Autoencoders Learn No Reproducible Features: A Random Baseline Analysis"

**New abstract:** 
> We show that SAE feature consistency (PWMCC ‚âà 0.30) is indistinguishable from randomly initialized SAEs, suggesting current training methods produce no reproducible structure. This is a more severe finding than previously reported low stability...

**Pros:** Honest, important finding, publishable  
**Cons:** Requires significant rewrite

### Option B: Add Random Baseline as Key Contribution

Keep current structure but add:
1. Random baseline experiment as new contribution
2. Reinterpret all findings in light of baseline
3. Discuss implications for field

**Pros:** Less rewriting  
**Cons:** May seem like we missed something obvious

### Option C: Investigate Further Before Publishing

Run additional experiments to:
1. Find metrics where trained > random
2. Understand why training doesn't improve stability
3. Propose solutions

**Pros:** More complete story  
**Cons:** Delays publication

**Recommendation:** Option B or C depending on timeline

---

## üéØ Immediate Action Items

### TODAY (2-3 hours)

1. ‚úÖ Run random baseline experiment (DONE - critical finding!)
2. [ ] Verify with different d_sae values
3. [ ] Try alternative stability metrics
4. [ ] Update paper with corrections

### THIS WEEK

5. [ ] Run activation-based stability analysis
6. [ ] Track PWMCC during training
7. [ ] Decide on paper revision strategy
8. [ ] Discuss with collaborators

### BEFORE SUBMISSION

9. [ ] Complete paper revisions
10. [ ] Add random baseline to all figures
11. [ ] Reframe narrative appropriately
12. [ ] Get external feedback

---

## üìö Key Files

### Results
- `results/analysis/random_baseline.json` - Random baseline results
- `results/analysis/feature_stability.json` - TopK results
- `results/analysis/relu_feature_stability.json` - ReLU results

### Code
- `scripts/compute_random_baseline.py` - Random baseline experiment
- `src/analysis/feature_matching.py` - PWMCC implementation

### Documentation
- `paper/sae_stability_paper.md` - Paper draft (NEEDS REVISION)
- `CRITICAL_ANALYSIS.md` - This document

---

## üí° Silver Lining

While this finding invalidates our original interpretation, it's actually a **more important discovery**:

1. **Novel contribution:** First to show SAE stability = random baseline
2. **Explains literature:** Paulo & Belrose's 0.30 might also be random
3. **Actionable:** Clear target for improvement (beat random!)
4. **Publishable:** Negative results are valuable

The key is to **reframe honestly** rather than hide the finding.

---

**Last updated:** December 5, 2025  
**Status:** Critical revision needed  
**Next action:** Verify random baseline, try alternative metrics
