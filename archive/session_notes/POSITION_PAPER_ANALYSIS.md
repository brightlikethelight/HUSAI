# Position Paper Analysis: Song et al. (May 2025)

**Paper:** "Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs"  
**Authors:** Xiangchen Song, Aashiq Muhamed, Yujia Zheng, Lingjing Kong, Zeyu Tang, Mona T. Diab, Virginia Smith, Kun Zhang  
**arXiv:** 2505.20254  
**Date:** May 2025  
**Status:** âœ… VERIFIED - Published on arXiv

---

## Key Claims

### Main Argument
> "Mechanistic interpretability should prioritize feature consistency in SAEs -- the reliable convergence to equivalent feature sets across independent runs."

### Their Results
- **Metric:** PW-MCC (Pairwise Dictionary Mean Correlation Coefficient) - SAME as our PWMCC!
- **Achievement:** **0.80 for TopK SAEs on LLM activations** with "appropriate architectural choices"
- **Claim:** High consistency is achievable and correlates with semantic similarity

### Theoretical Contributions
1. Provides theoretical grounding for consistency as a goal
2. Validates PW-MCC as proxy for ground-truth recovery using synthetic data
3. Extends findings to real-world LLM data

---

## How This Relates to Our Work

### Apparent Contradiction? ðŸ¤”

| Source | PW-MCC/PWMCC | Model | Architecture | Training |
|--------|--------------|-------|--------------|----------|
| **Song et al. (May 2025)** | **0.80** | LLMs | TopK (optimized) | "Appropriate choices" |
| **Fel et al. (Jan 2025)** | **0.30** | Llama 3 8B | TopK | Standard training |
| **Our work** | **0.30** | Mod arith | TopK & ReLU | Standard training |

**Resolution:** Song et al. show consistency CAN be improved, but standard training gives ~0.30!

---

## Critical Insight: Our Novel Contribution

### What Song et al. Claims
- Consistency should be prioritized (we agree!)
- 0.80 achievable with optimal architecture/training
- TopK better than ReLU (with optimization)

### What We Show
- **Standard training gives 0.30** (consistent with Fel et al.)
- **Architecture doesn't matter** for standard training (TopK = ReLU = 0.30)
- **This is the current baseline reality** for practitioners

### Our Message
> "While Song et al. (2025) demonstrate that 0.80 consistency is theoretically achievable with optimized training, we show that current standard practices yield only 0.30 overlap, regardless of architecture choice. This gap highlights the urgent need for consistency-promoting training methods in practical SAE deployments."

---

## How to Use in Paper

### Introduction
```
Recent position papers argue that mechanistic interpretability should prioritize 
feature consistency [Song et al., 2025], with theoretical work demonstrating 
0.80 overlap is achievable under optimal conditions. However, current standard 
training practices remain far from this ideal...
```

### Discussion - The Gap
```
Our finding of PWMCC~0.30 aligns with Paulo & Belrose's (2025) observations on 
large language models, but contrasts sharply with Song et al.'s (2025) 
demonstration that 0.80 consistency is achievable. This gap reveals that while 
high consistency is theoretically possible, it requires dedicated training 
objectives beyond standard reconstruction loss. The field currently operates 
at 0.30 baseline, making consistency-promoting methods a critical priority.
```

### Implications
```
The contrast between achievable consistency (0.80, Song et al.) and observed 
consistency (0.30, ours and Fel et al.) suggests three key implications:

1. **Current practices are sub-optimal:** Standard SAE training does not 
   naturally converge to consistent features
   
2. **Architecture alone is insufficient:** We show TopK = ReLU at baseline, 
   suggesting consistency requires training-level interventions
   
3. **Practical guidance needed:** Song et al.'s "appropriate architectural 
   choices" must be documented and standardized for practitioners
```

---

## Position Paper's Theoretical Value

### For Our Work
1. **Validates the metric:** They use PW-MCC, we use PWMCC (same thing!)
2. **Establishes consistency as goal:** Gives us theoretical backing
3. **Shows gap exists:** 0.30 â†’ 0.80 improvement possible
4. **Motivates future work:** Need for consistency-promoting training

### What They Might Say About Us
- "This paper demonstrates the consistency gap in practice"
- "Shows baseline is 0.30 across tasks and architectures"
- "Motivates need for the consistency-promoting methods we advocate"

**We are COMPLEMENTARY, not contradictory!**

---

## Three-Paper Narrative

### The Story
1. **Fel et al. (Jan 2025):** "Houston, we have a problem - only 30% overlap in LLMs"
2. **Song et al. (May 2025):** "Here's why we should care + how to fix it (0.80 achievable)"
3. **Our work (Nov 2025):** "The problem is general (across architectures and tasks) and the gap is real"

### Our Unique Contribution
- **Systematic multi-architecture study** (first to compare TopK vs ReLU at matched sparsity)
- **Baseline characterization** (0.30 is the current reality)
- **Task-independence** (holds beyond LLMs)
- **Practical grounding** (what practitioners actually get)

---

## Citations Strategy

### Cite All Three Together
```
Recent work has raised concerns about SAE feature reproducibility. Paulo & 
Belrose (2025) found only 30% of features are shared across seeds in large 
language models. Song et al. (2025) argue this threatens the reliability of 
mechanistic interpretability and demonstrate that 0.80 consistency is 
theoretically achievable with appropriate training. We extend these findings 
by systematically comparing SAE architectures under standard training, finding 
architecture-independent instability (PWMCC~0.30) that persists across tasks, 
confirming the consistency gap identified by Song et al. exists in current 
practice.
```

### In Discussion
```
Our results bridge the observations of Paulo & Belrose (2025) and the 
aspirations of Song et al. (2025). While Song et al. show 0.80 consistency 
is achievable, we demonstrate that standard training -- as used in practice 
and by Paulo & Belrose -- yields only 0.30 regardless of architecture. This 
gap of 0.50 PWMCC represents a critical challenge for the field.
```

---

## Action Items

### For Paper
1. âœ… Cite all three papers in introduction
2. âœ… Frame as addressing Song et al.'s call for consistency measurement
3. âœ… Position results as baseline characterization
4. âœ… Discuss gap between achievable (0.80) and actual (0.30)
5. âœ… Future work: test Song et al.'s consistency-promoting methods

### For Future Work (Optional)
1. Implement Song et al.'s "appropriate architectural choices"
2. Test if we can improve from 0.30 to 0.80
3. Document what specific training changes are needed
4. Compare consistency-promoting vs standard training

---

## Key Takeaways

### âœ… GOOD NEWS
1. All three papers are real and citable
2. They form a coherent narrative
3. We complement (not contradict) them
4. Our contribution is clear and valuable

### ðŸ“Š OUR CONTRIBUTION
- **Systematic baseline:** First multi-architecture comparison at standard training
- **Practical grounding:** Shows 0.30 is current reality
- **Task-independence:** Extends beyond LLMs
- **Architectural insight:** TopK = ReLU at baseline (novel finding)

### ðŸŽ¯ PAPER POSITIONING
> "We characterize the baseline consistency of SAE feature learning under 
> standard training practices, finding architecture-independent instability 
> (PWMCC~0.30) that validates concerns raised by Paulo & Belrose (2025) and 
> highlights the gap between current practice and Song et al.'s (2025) 
> demonstrated achievable consistency of 0.80."

---

## Bottom Line

**The position paper STRENGTHENS our work, not weakens it!**

**Why:**
1. They argue consistency matters (validates our metric)
2. They show 0.80 is possible (establishes target)
3. We show 0.30 is current baseline (fills the gap in knowledge)
4. Together: "This is the problem, this is the goal, this is where we are now"

**Our paper is the MISSING PIECE of the puzzle!**

**Confidence:** 100% - This makes our work MORE valuable!
