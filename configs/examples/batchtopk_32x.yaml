# Example configuration for BatchTopK SAE with 32x expansion
# This config demonstrates training a BatchTopK SAE with very large feature set

experiment_name: batchtopk_32x_seed999
wandb_project: husai-sae-stability
save_dir: results/batchtopk_32x_seed999
checkpoint_frequency: 0  # Only save final model (no intermediate checkpoints)
log_frequency: 200  # Log metrics every 200 training steps

# Dataset configuration
dataset:
  modulus: 113  # Prime modulus for modular arithmetic
  num_samples: 200000  # Large dataset for big model
  train_split: 0.8  # 80% train, 20% validation
  seed: 999  # Different seed for diversity

# Transformer architecture (model we'll train SAE on)
transformer:
  n_layers: 2  # Same 2-layer transformer
  d_model: 128  # 128-dimensional residual stream
  n_heads: 4  # 4 attention heads
  d_mlp: 512  # 4x expansion in MLP
  vocab_size: 117  # modulus + 4 (BOS, EOS, EQUALS, PLUS tokens)
  max_seq_len: 7  # Sequence format: [BOS, a, +, b, =, c, EOS]
  activation: gelu_fast  # Faster GELU variant

# SAE configuration
sae:
  architecture: batchtopk  # BatchTopK SAE (batch-level top-k)
  input_dim: 128  # Must match transformer d_model
  expansion_factor: 32  # 32x expansion (128 -> 4096 features!)
  sparsity_level: 128  # Keep top 128 features across batch
  learning_rate: 0.00005  # Very low LR for stability
  batch_size: 1024  # Large batch size for BatchTopK
  num_epochs: 30  # Many epochs for convergence
  k: 128  # Number of active features (BatchTopK specific)
  seed: 999  # Random seed for reproducibility
