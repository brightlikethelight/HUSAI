{
  "run_metadata": {
    "timestamp_utc": "2026-02-16T01:48:19+00:00",
    "git_commit": "10c2789b0c296d7eb2660979759614a5e5225b32",
    "command": "python scripts/experiments/run_ood_stress_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed123/sae_final.pt --architecture topk --sae-release husai_cycle5_ood --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_ood_cycle5 --model-cache-path /tmp/sae_bench_model_cache --output-dir results/experiments/phase4e_ood_stress_b200 --force-rerun",
    "config_hash": "c653f59455d0c72aaab21d3abda17abbfcaa67ff3e4898151ed59ea17d840900",
    "run_id": "run_20260216T014105Z"
  },
  "config": {
    "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed123/sae_final.pt",
    "architecture": "topk",
    "sae_release": "husai_cycle5_ood",
    "model_name": "pythia-70m-deduped",
    "hook_layer": 0,
    "hook_name": "blocks.0.hook_resid_pre",
    "reg_type": "l1",
    "setting": "normal",
    "ks": "1,2,5",
    "device": "cuda",
    "dtype": "float32",
    "results_path": "/tmp/husai_saebench_probe_results_ood_cycle5",
    "model_cache_path": "/tmp/sae_bench_model_cache",
    "output_dir": "/workspace/HUSAI/results/experiments/phase4e_ood_stress_b200",
    "id_dataset_count": 81,
    "ood_dataset_count": 32,
    "ood_domains": [
      "social-security",
      "spam",
      "temp",
      "third-party",
      "toxic",
      "trial-court",
      "truthqa",
      "twt",
      "us",
      "virtue",
      "wikidata",
      "wikidatais",
      "wikidatapolitical",
      "wikidatasex",
      "world"
    ]
  },
  "datasets": {
    "id": [
      "100_news_fake",
      "105_click_bait",
      "106_hate_hate",
      "107_hate_offensive",
      "110_aimade_humangpt3",
      "113_movie_sent",
      "114_nyc_borough_Manhattan",
      "115_nyc_borough_Brooklyn",
      "116_nyc_borough_Bronx",
      "126_art_type_book",
      "127_art_type_song",
      "128_art_type_movie",
      "129_arith_mc_A",
      "133_context_type_Causality",
      "134_context_type_Belief_states",
      "135_context_type_Event_duration",
      "136_glue_mnli_entailment",
      "137_glue_mnli_neutral",
      "138_glue_mnli_contradiction",
      "139_news_class_Politics",
      "140_news_class_Technology",
      "141_news_class_Entertainment",
      "142_cancer_cat_Thyroid_Cancer",
      "143_cancer_cat_Lung_Cancer",
      "144_cancer_cat_Colon_Cancer",
      "145_disease_class_digestive system diseases",
      "146_disease_class_cardiovascular diseases",
      "147_disease_class_nervous system diseases",
      "151_it_tick_HR Support",
      "152_it_tick_Hardware",
      "153_it_tick_Administrative rights",
      "154_athlete_sport_football",
      "155_athlete_sport_basketball",
      "156_athlete_sport_baseball",
      "157_amazon_5star",
      "158_code_C",
      "159_code_Python",
      "160_code_HTML",
      "161_agnews_0",
      "162_agnews_1",
      "163_agnews_2",
      "21_headline_istrump",
      "22_headline_isobama",
      "23_headline_ischina",
      "24_headline_isiran",
      "26_headline_isfrontpage",
      "36_sciq_tf",
      "44_phys_tf",
      "47_reasoning_tf",
      "48_cm_correct",
      "49_cm_isshort",
      "50_deon_isvalid",
      "51_just_is",
      "54_cs_tf",
      "5_hist_fig_ismale",
      "65_high-school",
      "66_living-room",
      "68_credit-card",
      "69_blood-pressure",
      "6_hist_fig_isamerican",
      "70_prime-factors",
      "71_social-media",
      "72_gene-expression",
      "73_control-group",
      "74_magnetic-field",
      "75_cell-lines",
      "77_second-derivative",
      "78_north-america",
      "79_human-rights",
      "7_hist_fig_ispolitician",
      "80_side-effects",
      "81_public-health",
      "82_federal-government",
      "84_clinical-trials",
      "85_mental-health",
      "87_glue_cola",
      "89_glue_mrpc",
      "90_glue_qnli",
      "91_glue_qqp",
      "92_glue_sst2",
      "94_ai_gen"
    ],
    "ood": [
      "117_us_state_FL",
      "118_us_state_CA",
      "119_us_state_TX",
      "120_us_timezone_Chicago",
      "121_us_timezone_New_York",
      "122_us_timezone_Los_Angeles",
      "123_world_country_United_Kingdom",
      "124_world_country_United_States",
      "125_world_country_Italy",
      "130_temp_cat_Frequency",
      "131_temp_cat_Typical Time",
      "132_temp_cat_Event Ordering",
      "148_twt_emotion_worry",
      "149_twt_emotion_happiness",
      "150_twt_emotion_sadness",
      "41_truthqa_tf",
      "42_temp_sense",
      "52_virtue_is",
      "56_wikidatasex_or_gender",
      "57_wikidatais_alive",
      "58_wikidatapolitical_party",
      "59_wikidata_occupation_isjournalist",
      "60_wikidata_occupation_isathlete",
      "61_wikidata_occupation_isactor",
      "62_wikidata_occupation_ispolitician",
      "63_wikidata_occupation_issinger",
      "64_wikidata_occupation_isresearcher",
      "67_social-security",
      "76_trial-court",
      "83_third-party",
      "95_toxic_is",
      "96_spam_is"
    ],
    "ood_domains": [
      "social-security",
      "spam",
      "temp",
      "third-party",
      "toxic",
      "trial-court",
      "truthqa",
      "twt",
      "us",
      "virtue",
      "wikidata",
      "wikidatais",
      "wikidatapolitical",
      "wikidatasex",
      "world"
    ],
    "inferred_total_count": 113
  },
  "id_result": {
    "label": "id",
    "returncode": 0,
    "dataset_count": 81,
    "datasets": [
      "100_news_fake",
      "105_click_bait",
      "106_hate_hate",
      "107_hate_offensive",
      "110_aimade_humangpt3",
      "113_movie_sent",
      "114_nyc_borough_Manhattan",
      "115_nyc_borough_Brooklyn",
      "116_nyc_borough_Bronx",
      "126_art_type_book",
      "127_art_type_song",
      "128_art_type_movie",
      "129_arith_mc_A",
      "133_context_type_Causality",
      "134_context_type_Belief_states",
      "135_context_type_Event_duration",
      "136_glue_mnli_entailment",
      "137_glue_mnli_neutral",
      "138_glue_mnli_contradiction",
      "139_news_class_Politics",
      "140_news_class_Technology",
      "141_news_class_Entertainment",
      "142_cancer_cat_Thyroid_Cancer",
      "143_cancer_cat_Lung_Cancer",
      "144_cancer_cat_Colon_Cancer",
      "145_disease_class_digestive system diseases",
      "146_disease_class_cardiovascular diseases",
      "147_disease_class_nervous system diseases",
      "151_it_tick_HR Support",
      "152_it_tick_Hardware",
      "153_it_tick_Administrative rights",
      "154_athlete_sport_football",
      "155_athlete_sport_basketball",
      "156_athlete_sport_baseball",
      "157_amazon_5star",
      "158_code_C",
      "159_code_Python",
      "160_code_HTML",
      "161_agnews_0",
      "162_agnews_1",
      "163_agnews_2",
      "21_headline_istrump",
      "22_headline_isobama",
      "23_headline_ischina",
      "24_headline_isiran",
      "26_headline_isfrontpage",
      "36_sciq_tf",
      "44_phys_tf",
      "47_reasoning_tf",
      "48_cm_correct",
      "49_cm_isshort",
      "50_deon_isvalid",
      "51_just_is",
      "54_cs_tf",
      "5_hist_fig_ismale",
      "65_high-school",
      "66_living-room",
      "68_credit-card",
      "69_blood-pressure",
      "6_hist_fig_isamerican",
      "70_prime-factors",
      "71_social-media",
      "72_gene-expression",
      "73_control-group",
      "74_magnetic-field",
      "75_cell-lines",
      "77_second-derivative",
      "78_north-america",
      "79_human-rights",
      "7_hist_fig_ispolitician",
      "80_side-effects",
      "81_public-health",
      "82_federal-government",
      "84_clinical-trials",
      "85_mental-health",
      "87_glue_cola",
      "89_glue_mrpc",
      "90_glue_qnli",
      "91_glue_qqp",
      "92_glue_sst2",
      "94_ai_gen"
    ],
    "output_dir": "results/experiments/phase4e_ood_stress_b200/run_20260216T014105Z/id",
    "log_path": "results/experiments/phase4e_ood_stress_b200/run_20260216T014105Z/logs/id.log",
    "summary_path": "results/experiments/phase4e_ood_stress_b200/run_20260216T014105Z/id/husai_custom_sae_summary.json",
    "best_minus_llm_auc": -0.03539346350148398,
    "summary": {
      "timestamp_utc": "2026-02-16T01:46:13+00:00",
      "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed123/sae_final.pt --sae-release husai_cycle5_ood_id --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --reg-type l1 --setting normal --ks 1,2,5 --dataset-names '100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,126_art_type_book,127_art_type_song,128_art_type_movie,129_arith_mc_A,133_context_type_Causality,134_context_type_Belief_states,135_context_type_Event_duration,136_glue_mnli_entailment,137_glue_mnli_neutral,138_glue_mnli_contradiction,139_news_class_Politics,140_news_class_Technology,141_news_class_Entertainment,142_cancer_cat_Thyroid_Cancer,143_cancer_cat_Lung_Cancer,144_cancer_cat_Colon_Cancer,145_disease_class_digestive system diseases,146_disease_class_cardiovascular diseases,147_disease_class_nervous system diseases,151_it_tick_HR Support,152_it_tick_Hardware,153_it_tick_Administrative rights,154_athlete_sport_football,155_athlete_sport_basketball,156_athlete_sport_baseball,157_amazon_5star,158_code_C,159_code_Python,160_code_HTML,161_agnews_0,162_agnews_1,163_agnews_2,21_headline_istrump,22_headline_isobama,23_headline_ischina,24_headline_isiran,26_headline_isfrontpage,36_sciq_tf,44_phys_tf,47_reasoning_tf,48_cm_correct,49_cm_isshort,50_deon_isvalid,51_just_is,54_cs_tf,5_hist_fig_ismale,65_high-school,66_living-room,68_credit-card,69_blood-pressure,6_hist_fig_isamerican,70_prime-factors,71_social-media,72_gene-expression,73_control-group,74_magnetic-field,75_cell-lines,77_second-derivative,78_north-america,79_human-rights,7_hist_fig_ispolitician,80_side-effects,81_public-health,82_federal-government,84_clinical-trials,85_mental-health,87_glue_cola,89_glue_mrpc,90_glue_qnli,91_glue_qqp,92_glue_sst2,94_ai_gen' --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_ood_cycle5 --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_ood_stress_b200/run_20260216T014105Z/id --architecture topk --force-rerun",
      "config": {
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed123/sae_final.pt",
        "architecture_override": "topk",
        "sae_release": "husai_cycle5_ood_id",
        "model_name": "pythia-70m-deduped",
        "hook_layer": 0,
        "hook_name": "blocks.0.hook_resid_pre",
        "reg_type": "l1",
        "setting": "normal",
        "ks": [
          1,
          2,
          5
        ],
        "dataset_names": [
          "100_news_fake",
          "105_click_bait",
          "106_hate_hate",
          "107_hate_offensive",
          "110_aimade_humangpt3",
          "113_movie_sent",
          "114_nyc_borough_Manhattan",
          "115_nyc_borough_Brooklyn",
          "116_nyc_borough_Bronx",
          "126_art_type_book",
          "127_art_type_song",
          "128_art_type_movie",
          "129_arith_mc_A",
          "133_context_type_Causality",
          "134_context_type_Belief_states",
          "135_context_type_Event_duration",
          "136_glue_mnli_entailment",
          "137_glue_mnli_neutral",
          "138_glue_mnli_contradiction",
          "139_news_class_Politics",
          "140_news_class_Technology",
          "141_news_class_Entertainment",
          "142_cancer_cat_Thyroid_Cancer",
          "143_cancer_cat_Lung_Cancer",
          "144_cancer_cat_Colon_Cancer",
          "145_disease_class_digestive system diseases",
          "146_disease_class_cardiovascular diseases",
          "147_disease_class_nervous system diseases",
          "151_it_tick_HR Support",
          "152_it_tick_Hardware",
          "153_it_tick_Administrative rights",
          "154_athlete_sport_football",
          "155_athlete_sport_basketball",
          "156_athlete_sport_baseball",
          "157_amazon_5star",
          "158_code_C",
          "159_code_Python",
          "160_code_HTML",
          "161_agnews_0",
          "162_agnews_1",
          "163_agnews_2",
          "21_headline_istrump",
          "22_headline_isobama",
          "23_headline_ischina",
          "24_headline_isiran",
          "26_headline_isfrontpage",
          "36_sciq_tf",
          "44_phys_tf",
          "47_reasoning_tf",
          "48_cm_correct",
          "49_cm_isshort",
          "50_deon_isvalid",
          "51_just_is",
          "54_cs_tf",
          "5_hist_fig_ismale",
          "65_high-school",
          "66_living-room",
          "68_credit-card",
          "69_blood-pressure",
          "6_hist_fig_isamerican",
          "70_prime-factors",
          "71_social-media",
          "72_gene-expression",
          "73_control-group",
          "74_magnetic-field",
          "75_cell-lines",
          "77_second-derivative",
          "78_north-america",
          "79_human-rights",
          "7_hist_fig_ispolitician",
          "80_side-effects",
          "81_public-health",
          "82_federal-government",
          "84_clinical-trials",
          "85_mental-health",
          "87_glue_cola",
          "89_glue_mrpc",
          "90_glue_qnli",
          "91_glue_qqp",
          "92_glue_sst2",
          "94_ai_gen"
        ],
        "dataset_names_inferred_from_cache": false,
        "dataset_count": 81,
        "binarize": false,
        "device": "cuda",
        "dtype": "float32",
        "results_path": "/tmp/husai_saebench_probe_results_ood_cycle5",
        "model_cache_path": "/tmp/sae_bench_model_cache",
        "force_rerun": true
      },
      "config_hash": "8d88586853a5b403d4598754ec8a990cd1a617741e0585f4d9316d171bda1b93",
      "sae_meta": {
        "architecture": "topk",
        "eval_architecture": "topk",
        "d_model": 512,
        "d_sae": 1024,
        "k": 32,
        "dead_features_repaired": 0,
        "decoder_norm_max_deviation": 1.1920928955078125e-07
      },
      "summary": {
        "result_key": "husai_cycle5_ood_id_custom_sae",
        "llm_metrics": {
          "llm_test_accuracy": 0.6484440093938268,
          "llm_test_auc": 0.6685808578683783,
          "llm_test_f1": 0.6109485111283821
        },
        "sae_metrics_by_k": [
          {
            "k": 1,
            "test_accuracy": 0.6053691042570056,
            "test_auc": 0.6045385459907784,
            "test_f1": 0.5423790703166912
          },
          {
            "k": 2,
            "test_accuracy": 0.6137819045236594,
            "test_auc": 0.6232484669214633,
            "test_f1": 0.5575033611013406
          },
          {
            "k": 5,
            "test_accuracy": 0.6215719204424064,
            "test_auc": 0.6331873943668943,
            "test_f1": 0.5703781676540344
          }
        ],
        "best_by_auc": {
          "k": 5,
          "test_accuracy": 0.6215719204424064,
          "test_auc": 0.6331873943668943,
          "test_f1": 0.5703781676540344
        },
        "best_minus_llm_auc": -0.03539346350148398
      }
    }
  },
  "ood_result": {
    "label": "ood",
    "returncode": 0,
    "dataset_count": 32,
    "datasets": [
      "117_us_state_FL",
      "118_us_state_CA",
      "119_us_state_TX",
      "120_us_timezone_Chicago",
      "121_us_timezone_New_York",
      "122_us_timezone_Los_Angeles",
      "123_world_country_United_Kingdom",
      "124_world_country_United_States",
      "125_world_country_Italy",
      "130_temp_cat_Frequency",
      "131_temp_cat_Typical Time",
      "132_temp_cat_Event Ordering",
      "148_twt_emotion_worry",
      "149_twt_emotion_happiness",
      "150_twt_emotion_sadness",
      "41_truthqa_tf",
      "42_temp_sense",
      "52_virtue_is",
      "56_wikidatasex_or_gender",
      "57_wikidatais_alive",
      "58_wikidatapolitical_party",
      "59_wikidata_occupation_isjournalist",
      "60_wikidata_occupation_isathlete",
      "61_wikidata_occupation_isactor",
      "62_wikidata_occupation_ispolitician",
      "63_wikidata_occupation_issinger",
      "64_wikidata_occupation_isresearcher",
      "67_social-security",
      "76_trial-court",
      "83_third-party",
      "95_toxic_is",
      "96_spam_is"
    ],
    "output_dir": "results/experiments/phase4e_ood_stress_b200/run_20260216T014105Z/ood",
    "log_path": "results/experiments/phase4e_ood_stress_b200/run_20260216T014105Z/logs/ood.log",
    "summary_path": "results/experiments/phase4e_ood_stress_b200/run_20260216T014105Z/ood/husai_custom_sae_summary.json",
    "best_minus_llm_auc": -0.05638802005550925,
    "summary": {
      "timestamp_utc": "2026-02-16T01:48:18+00:00",
      "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed123/sae_final.pt --sae-release husai_cycle5_ood_ood --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --reg-type l1 --setting normal --ks 1,2,5 --dataset-names '117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom,124_world_country_United_States,125_world_country_Italy,130_temp_cat_Frequency,131_temp_cat_Typical Time,132_temp_cat_Event Ordering,148_twt_emotion_worry,149_twt_emotion_happiness,150_twt_emotion_sadness,41_truthqa_tf,42_temp_sense,52_virtue_is,56_wikidatasex_or_gender,57_wikidatais_alive,58_wikidatapolitical_party,59_wikidata_occupation_isjournalist,60_wikidata_occupation_isathlete,61_wikidata_occupation_isactor,62_wikidata_occupation_ispolitician,63_wikidata_occupation_issinger,64_wikidata_occupation_isresearcher,67_social-security,76_trial-court,83_third-party,95_toxic_is,96_spam_is' --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_ood_cycle5 --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_ood_stress_b200/run_20260216T014105Z/ood --architecture topk --force-rerun",
      "config": {
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed123/sae_final.pt",
        "architecture_override": "topk",
        "sae_release": "husai_cycle5_ood_ood",
        "model_name": "pythia-70m-deduped",
        "hook_layer": 0,
        "hook_name": "blocks.0.hook_resid_pre",
        "reg_type": "l1",
        "setting": "normal",
        "ks": [
          1,
          2,
          5
        ],
        "dataset_names": [
          "117_us_state_FL",
          "118_us_state_CA",
          "119_us_state_TX",
          "120_us_timezone_Chicago",
          "121_us_timezone_New_York",
          "122_us_timezone_Los_Angeles",
          "123_world_country_United_Kingdom",
          "124_world_country_United_States",
          "125_world_country_Italy",
          "130_temp_cat_Frequency",
          "131_temp_cat_Typical Time",
          "132_temp_cat_Event Ordering",
          "148_twt_emotion_worry",
          "149_twt_emotion_happiness",
          "150_twt_emotion_sadness",
          "41_truthqa_tf",
          "42_temp_sense",
          "52_virtue_is",
          "56_wikidatasex_or_gender",
          "57_wikidatais_alive",
          "58_wikidatapolitical_party",
          "59_wikidata_occupation_isjournalist",
          "60_wikidata_occupation_isathlete",
          "61_wikidata_occupation_isactor",
          "62_wikidata_occupation_ispolitician",
          "63_wikidata_occupation_issinger",
          "64_wikidata_occupation_isresearcher",
          "67_social-security",
          "76_trial-court",
          "83_third-party",
          "95_toxic_is",
          "96_spam_is"
        ],
        "dataset_names_inferred_from_cache": false,
        "dataset_count": 32,
        "binarize": false,
        "device": "cuda",
        "dtype": "float32",
        "results_path": "/tmp/husai_saebench_probe_results_ood_cycle5",
        "model_cache_path": "/tmp/sae_bench_model_cache",
        "force_rerun": true
      },
      "config_hash": "b2ee237184265c1e203503a949f1701a3581e811c3abf55a9b3b1f10abdcb5e7",
      "sae_meta": {
        "architecture": "topk",
        "eval_architecture": "topk",
        "d_model": 512,
        "d_sae": 1024,
        "k": 32,
        "dead_features_repaired": 0,
        "decoder_norm_max_deviation": 1.1920928955078125e-07
      },
      "summary": {
        "result_key": "husai_cycle5_ood_ood_custom_sae",
        "llm_metrics": {
          "llm_test_accuracy": 0.6495039546512676,
          "llm_test_auc": 0.6744019001622025,
          "llm_test_f1": 0.617120103509151
        },
        "sae_metrics_by_k": [
          {
            "k": 1,
            "test_accuracy": 0.5762271722856888,
            "test_auc": 0.5821456308334928,
            "test_f1": 0.5184625726711584
          },
          {
            "k": 2,
            "test_accuracy": 0.5857524016264964,
            "test_auc": 0.6007948956008077,
            "test_f1": 0.5407326407026446
          },
          {
            "k": 5,
            "test_accuracy": 0.595496384050081,
            "test_auc": 0.6180138801066932,
            "test_f1": 0.5667658911664134
          }
        ],
        "best_by_auc": {
          "k": 5,
          "test_accuracy": 0.595496384050081,
          "test_auc": 0.6180138801066932,
          "test_f1": 0.5667658911664134
        },
        "best_minus_llm_auc": -0.05638802005550925
      }
    }
  },
  "ood_drop": 0.020994556554025268,
  "relative_drop": 0.5931760974210678,
  "drop": 0.020994556554025268,
  "metrics": {
    "ood_drop": 0.020994556554025268,
    "relative_drop": 0.5931760974210678,
    "id_best_minus_llm_auc": -0.03539346350148398,
    "ood_best_minus_llm_auc": -0.05638802005550925
  }
}
