{
  "timestamp_utc": "2026-02-14T21:24:18+00:00",
  "command": "python scripts/experiments/run_architecture_frontier_external.py --activation-cache-dir /tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped --activation-glob *_blocks.0.hook_resid_pre.pt --architectures topk,relu,batchtopk,jumprelu --seeds 42,123,456,789,1011 --d-sae 1024 --k 32 --epochs 6 --batch-size 4096 --learning-rate 0.001 --device cuda --run-saebench --run-cebench --cebench-repo /workspace/CE-Bench --cebench-max-rows 200 --cebench-matched-baseline-summary docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json --saebench-results-path /tmp/husai_saebench_probe_results_frontier_multiseed --saebench-model-cache-path /tmp/sae_bench_model_cache --saebench-dataset-limit 8 --cebench-artifacts-path /tmp/ce_bench_artifacts_frontier_multiseed --output-dir results/experiments/phase4b_architecture_frontier_external_multiseed",
  "config": {
    "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
    "activation_glob": "*_blocks.0.hook_resid_pre.pt",
    "max_files": 80,
    "max_rows_per_file": 2048,
    "max_total_rows": 150000,
    "architectures": [
      "topk",
      "relu",
      "batchtopk",
      "jumprelu"
    ],
    "seeds": [
      42,
      123,
      456,
      789,
      1011
    ],
    "d_sae": 1024,
    "k": 32,
    "epochs": 6,
    "batch_size": 4096,
    "learning_rate": 0.001,
    "device": "cuda",
    "dtype": "float32",
    "model_name": "pythia-70m-deduped",
    "hook_layer": 0,
    "hook_name": "blocks.0.hook_resid_pre",
    "run_saebench": true,
    "run_cebench": true,
    "cebench_repo": "/workspace/CE-Bench",
    "cebench_max_rows": 200,
    "cebench_matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
    "saebench_datasets": [],
    "saebench_dataset_limit": 8,
    "relu_l1_coef": 0.001,
    "jumprelu_l0_coef": 0.001,
    "saebench_results_path": "/tmp/husai_saebench_probe_results_frontier_multiseed",
    "saebench_model_cache_path": "/tmp/sae_bench_model_cache",
    "cebench_artifacts_path": "/tmp/ce_bench_artifacts_frontier_multiseed",
    "data_meta": {
      "num_files_discovered": 113,
      "num_files_used": 80,
      "total_rows": 144042,
      "d_model": 512
    },
    "source_files": [
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/113_movie_sent_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/114_nyc_borough_Manhattan_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/115_nyc_borough_Brooklyn_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/116_nyc_borough_Bronx_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/117_us_state_FL_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/118_us_state_CA_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/119_us_state_TX_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/120_us_timezone_Chicago_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/121_us_timezone_New_York_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/122_us_timezone_Los_Angeles_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/123_world_country_United_Kingdom_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/124_world_country_United_States_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/125_world_country_Italy_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/126_art_type_book_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/127_art_type_song_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/128_art_type_movie_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/129_arith_mc_A_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/130_temp_cat_Frequency_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/131_temp_cat_Typical Time_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/132_temp_cat_Event Ordering_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/133_context_type_Causality_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/134_context_type_Belief_states_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/135_context_type_Event_duration_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/136_glue_mnli_entailment_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/137_glue_mnli_neutral_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/138_glue_mnli_contradiction_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/139_news_class_Politics_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/140_news_class_Technology_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/141_news_class_Entertainment_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/142_cancer_cat_Thyroid_Cancer_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/143_cancer_cat_Lung_Cancer_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/144_cancer_cat_Colon_Cancer_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/145_disease_class_digestive system diseases_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/146_disease_class_cardiovascular diseases_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/147_disease_class_nervous system diseases_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/148_twt_emotion_worry_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/149_twt_emotion_happiness_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/150_twt_emotion_sadness_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/151_it_tick_HR Support_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/152_it_tick_Hardware_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/153_it_tick_Administrative rights_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/154_athlete_sport_football_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/155_athlete_sport_basketball_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/156_athlete_sport_baseball_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/157_amazon_5star_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/158_code_C_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/159_code_Python_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/160_code_HTML_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/161_agnews_0_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/162_agnews_1_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/163_agnews_2_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/21_headline_istrump_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/22_headline_isobama_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/23_headline_ischina_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/24_headline_isiran_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/26_headline_isfrontpage_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/36_sciq_tf_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/41_truthqa_tf_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/42_temp_sense_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/44_phys_tf_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/47_reasoning_tf_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/48_cm_correct_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/49_cm_isshort_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/50_deon_isvalid_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/51_just_is_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/52_virtue_is_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/54_cs_tf_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/56_wikidatasex_or_gender_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/57_wikidatais_alive_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/58_wikidatapolitical_party_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/59_wikidata_occupation_isjournalist_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/5_hist_fig_ismale_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/60_wikidata_occupation_isathlete_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/61_wikidata_occupation_isactor_blocks.0.hook_resid_pre.pt",
      "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/62_wikidata_occupation_ispolitician_blocks.0.hook_resid_pre.pt"
    ],
    "dataset_names_count": 8,
    "run_id": "run_20260214T202538Z"
  },
  "records": [
    {
      "architecture": "topk",
      "seed": 42,
      "checkpoint": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed42/sae_final.pt",
      "train_metrics": {
        "mse": 0.00021257683692965657,
        "explained_variance": 0.7437508538331046,
        "l0": 32.0
      },
      "saebench": {
        "timestamp_utc": "2026-02-14T20:26:34+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed42/sae_final.pt --architecture topk --sae-release husai_topk_seed42 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_frontier_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed42/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed42/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_topk_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 8,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_frontier_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "8075e20255aeef5bbad671938c3af63a257fc7f12a12b164355f2ed82e2a66bd",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_topk_seed42_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6822175045257715,
            "llm_test_auc": 0.7056251859979396,
            "llm_test_f1": 0.6610129056743866
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.6271609534284811,
              "test_auc": 0.6408415308237494,
              "test_f1": 0.5566122432707787
            },
            {
              "k": 2,
              "test_accuracy": 0.6328997265514833,
              "test_auc": 0.6457346338902826,
              "test_f1": 0.5834130762778423
            },
            {
              "k": 5,
              "test_accuracy": 0.6510487353012745,
              "test_auc": 0.6638175720848825,
              "test_f1": 0.5967402870093411
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6510487353012745,
            "test_auc": 0.6638175720848825,
            "test_f1": 0.5967402870093411
          },
          "best_minus_llm_auc": -0.041807613913057096
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T20:28:48+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed42/sae_final.pt --architecture topk --sae-release husai_topk_seed42 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed42/cebench --artifacts-path /tmp/ce_bench_artifacts_frontier_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "27652a73c18ba92e433875bd2554b744a3cd48915234a223f265a336f6147ed9",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed42/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "0340244b549dc6ed2c80a1c9135b15009ad5d24cf481ac6a38d91fb97e718bea",
          "sae_release": "husai_topk_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed42/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_frontier_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed42/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed42/cebench/interpretability_eval/husai_topk_seed42/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed42/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 7.901997175216675,
          "independent_score_mean_max": 9.000797727108,
          "interpretability_score_mean_max": 7.585395152568817,
          "sae_release": "husai_topk_seed42",
          "sae_id": "custom_sae",
          "date": "2026-02-14 20:28:48",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 7.901997175216675,
          "independent_score_mean_max": 9.000797727108,
          "interpretability_score_mean_max": 7.585395152568817
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -42.60931025505066,
          "independent_score_mean_max": -41.99846862077713,
          "interpretability_score_mean_max": -40.36621643304825
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed42/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed42/cebench/cebench_metrics_summary.md"
        }
      },
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "architecture": "topk",
      "seed": 123,
      "checkpoint": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed123/sae_final.pt",
      "train_metrics": {
        "mse": 0.00021285809634719044,
        "explained_variance": 0.7434116009878537,
        "l0": 32.0
      },
      "saebench": {
        "timestamp_utc": "2026-02-14T20:29:17+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed123/sae_final.pt --architecture topk --sae-release husai_topk_seed123 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_frontier_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed123/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed123/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_topk_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 8,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_frontier_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "7b3091c324bbdf2c6eebbe5aa2c559828cd332d41c3ef0830458e89c82cc1538",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_topk_seed123_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6822175045257715,
            "llm_test_auc": 0.7056251859979396,
            "llm_test_f1": 0.6610129056743866
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.6478221981379245,
              "test_auc": 0.6520058801040122,
              "test_f1": 0.6046307692067175
            },
            {
              "k": 2,
              "test_accuracy": 0.6493429648631313,
              "test_auc": 0.6670291045512722,
              "test_f1": 0.6196622252933282
            },
            {
              "k": 5,
              "test_accuracy": 0.6551315287768572,
              "test_auc": 0.6730207654270954,
              "test_f1": 0.6304452690247184
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6551315287768572,
            "test_auc": 0.6730207654270954,
            "test_f1": 0.6304452690247184
          },
          "best_minus_llm_auc": -0.03260442057084412
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T20:31:29+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed123/sae_final.pt --architecture topk --sae-release husai_topk_seed123 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed123/cebench --artifacts-path /tmp/ce_bench_artifacts_frontier_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "39c672e9a8b5841551763f1d8c659c4d0904df0a05bbaaa9abd5f1d0b72f2b01",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed123/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "8e148e6f1fee12947ca48ea88a005d53579994ec833d181433fd57602d8924bd",
          "sae_release": "husai_topk_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed123/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_frontier_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed123/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed123/cebench/interpretability_eval/husai_topk_seed123/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed123/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 7.986529376506805,
          "independent_score_mean_max": 9.738143472671508,
          "interpretability_score_mean_max": 8.041117386817932,
          "sae_release": "husai_topk_seed123",
          "sae_id": "custom_sae",
          "date": "2026-02-14 20:31:29",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 7.986529376506805,
          "independent_score_mean_max": 9.738143472671508,
          "interpretability_score_mean_max": 8.041117386817932
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -42.52477805376053,
          "independent_score_mean_max": -41.26112287521362,
          "interpretability_score_mean_max": -39.910494198799135
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed123/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed123/cebench/cebench_metrics_summary.md"
        }
      },
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "architecture": "topk",
      "seed": 456,
      "checkpoint": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed456/sae_final.pt",
      "train_metrics": {
        "mse": 0.00021121800818946213,
        "explained_variance": 0.745388634657804,
        "l0": 32.0
      },
      "saebench": {
        "timestamp_utc": "2026-02-14T20:32:01+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed456/sae_final.pt --architecture topk --sae-release husai_topk_seed456 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_frontier_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed456/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed456/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_topk_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 8,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_frontier_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "a17d01a6adacd8ef0e00b1f9b172cb9464c479f5ac9f67830b735056c5b19580",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_topk_seed456_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6822175045257715,
            "llm_test_auc": 0.7056251859979396,
            "llm_test_f1": 0.6610129056743866
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.6276346136913846,
              "test_auc": 0.6277578882840271,
              "test_f1": 0.552958633274341
            },
            {
              "k": 2,
              "test_accuracy": 0.6422162639045216,
              "test_auc": 0.6520880855258522,
              "test_f1": 0.5762563338042316
            },
            {
              "k": 5,
              "test_accuracy": 0.6484874055461128,
              "test_auc": 0.662882394706047,
              "test_f1": 0.6181250915009898
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6484874055461128,
            "test_auc": 0.662882394706047,
            "test_f1": 0.6181250915009898
          },
          "best_minus_llm_auc": -0.04274279129189251
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T20:34:12+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed456/sae_final.pt --architecture topk --sae-release husai_topk_seed456 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed456/cebench --artifacts-path /tmp/ce_bench_artifacts_frontier_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "c1e41fc80399e1313dcf7619e462ce46613e0135d51029c419ccd7d921e412fe",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed456/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "d5156a269497bb0179b6e41491ceec3f52790d828fd87f47122d965f791a090c",
          "sae_release": "husai_topk_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed456/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_frontier_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed456/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed456/cebench/interpretability_eval/husai_topk_seed456/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed456/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 8.062998487949372,
          "independent_score_mean_max": 8.923474085330962,
          "interpretability_score_mean_max": 7.767499964237214,
          "sae_release": "husai_topk_seed456",
          "sae_id": "custom_sae",
          "date": "2026-02-14 20:34:12",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 8.062998487949372,
          "independent_score_mean_max": 8.923474085330962,
          "interpretability_score_mean_max": 7.767499964237214
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -42.448308942317965,
          "independent_score_mean_max": -42.075792262554174,
          "interpretability_score_mean_max": -40.184111621379856
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed456/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed456/cebench/cebench_metrics_summary.md"
        }
      },
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "architecture": "topk",
      "seed": 789,
      "checkpoint": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed789/sae_final.pt",
      "train_metrics": {
        "mse": 0.00020986865274608135,
        "explained_variance": 0.7470152239977429,
        "l0": 32.0
      },
      "saebench": {
        "timestamp_utc": "2026-02-14T20:34:46+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed789/sae_final.pt --architecture topk --sae-release husai_topk_seed789 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_frontier_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed789/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed789/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_topk_seed789",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 8,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_frontier_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "c1b1fd6a5890ea594d4dcf59fcb5fb2e78bd4d9857034fc1b570cedef6642dd9",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_topk_seed789_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6822175045257715,
            "llm_test_auc": 0.7056251859979396,
            "llm_test_f1": 0.6610129056743866
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.6407048745123547,
              "test_auc": 0.6427253014644462,
              "test_f1": 0.5830863755035735
            },
            {
              "k": 2,
              "test_accuracy": 0.6456621413940148,
              "test_auc": 0.6520693258074282,
              "test_f1": 0.5955562331253554
            },
            {
              "k": 5,
              "test_accuracy": 0.6552743516104982,
              "test_auc": 0.6650759678808793,
              "test_f1": 0.6273719868292078
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6552743516104982,
            "test_auc": 0.6650759678808793,
            "test_f1": 0.6273719868292078
          },
          "best_minus_llm_auc": -0.04054921811706025
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T20:37:00+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed789/sae_final.pt --architecture topk --sae-release husai_topk_seed789 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed789/cebench --artifacts-path /tmp/ce_bench_artifacts_frontier_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "b1562473d701c455f4d8d76d4d77a4868beafbc819ec8a311d61925c7d2c74e4",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed789/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "4beaab8f0054b95f10f8b98e6f08dcaec2d92c7e7447c969e76d5f9880cabe53",
          "sae_release": "husai_topk_seed789",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed789/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_frontier_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed789/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed789/cebench/interpretability_eval/husai_topk_seed789/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed789/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 8.210204319953919,
          "independent_score_mean_max": 9.095562295913696,
          "interpretability_score_mean_max": 7.90459540605545,
          "sae_release": "husai_topk_seed789",
          "sae_id": "custom_sae",
          "date": "2026-02-14 20:37:00",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 8.210204319953919,
          "independent_score_mean_max": 9.095562295913696,
          "interpretability_score_mean_max": 7.90459540605545
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -42.30110311031342,
          "independent_score_mean_max": -41.90370405197144,
          "interpretability_score_mean_max": -40.047016179561616
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed789/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed789/cebench/cebench_metrics_summary.md"
        }
      },
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "architecture": "topk",
      "seed": 1011,
      "checkpoint": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed1011/sae_final.pt",
      "train_metrics": {
        "mse": 0.00021080599981360137,
        "explained_variance": 0.7458855330728723,
        "l0": 32.0
      },
      "saebench": {
        "timestamp_utc": "2026-02-14T20:37:32+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed1011/sae_final.pt --architecture topk --sae-release husai_topk_seed1011 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_frontier_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed1011/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed1011/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_topk_seed1011",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 8,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_frontier_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "a60828890bfc06e4957c6f8a2ddfd37026ed1773e597176e7a9569064f6068c4",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_topk_seed1011_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6822175045257715,
            "llm_test_auc": 0.7056251859979396,
            "llm_test_f1": 0.6610129056743866
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.6327106102240581,
              "test_auc": 0.6414850213093468,
              "test_f1": 0.5797805971086343
            },
            {
              "k": 2,
              "test_accuracy": 0.6455359609739372,
              "test_auc": 0.653254218696688,
              "test_f1": 0.6066734460710564
            },
            {
              "k": 5,
              "test_accuracy": 0.6412785831316536,
              "test_auc": 0.6603652285623202,
              "test_f1": 0.6139079133299121
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6412785831316536,
            "test_auc": 0.6603652285623202,
            "test_f1": 0.6139079133299121
          },
          "best_minus_llm_auc": -0.04525995743561939
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T20:39:45+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed1011/sae_final.pt --architecture topk --sae-release husai_topk_seed1011 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed1011/cebench --artifacts-path /tmp/ce_bench_artifacts_frontier_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "b139012bcab1d7e57966cfa129b8dadad35b344a25ec6f7d290822f648947c08",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/topk_seed1011/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "25aaed4305f03de24254e777a19cba1ade00f790e57dabea83fcea99d0002d7d",
          "sae_release": "husai_topk_seed1011",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed1011/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_frontier_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed1011/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed1011/cebench/interpretability_eval/husai_topk_seed1011/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed1011/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 7.7187209320068355,
          "independent_score_mean_max": 8.2837304520607,
          "interpretability_score_mean_max": 7.3352331233024595,
          "sae_release": "husai_topk_seed1011",
          "sae_id": "custom_sae",
          "date": "2026-02-14 20:39:45",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 7.7187209320068355,
          "independent_score_mean_max": 8.2837304520607,
          "interpretability_score_mean_max": 7.3352331233024595
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -42.792586498260505,
          "independent_score_mean_max": -42.71553589582443,
          "interpretability_score_mean_max": -40.61637846231461
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed1011/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/topk_seed1011/cebench/cebench_metrics_summary.md"
        }
      },
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "architecture": "relu",
      "seed": 42,
      "checkpoint": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed42/sae_final.pt",
      "train_metrics": {
        "mse": 3.863715846819105e-06,
        "explained_variance": 0.9953425196668417,
        "l0": 654.0625
      },
      "saebench": {
        "timestamp_utc": "2026-02-14T20:40:17+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed42/sae_final.pt --architecture relu --sae-release husai_relu_seed42 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_frontier_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed42/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed42/sae_final.pt",
          "architecture_override": "relu",
          "sae_release": "husai_relu_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 8,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_frontier_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "8855fd4c783e3b28f864c565f0ac2bd3451e6cff353dd8a716ec3eccbb008d22",
        "sae_meta": {
          "architecture": "relu",
          "d_model": 512,
          "d_sae": 1024,
          "k": null
        },
        "summary": {
          "result_key": "husai_relu_seed42_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6822175045257715,
            "llm_test_auc": 0.7056251859979396,
            "llm_test_f1": 0.6610129056743866
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.6388064523049178,
              "test_auc": 0.637551937767731,
              "test_f1": 0.5974885448867371
            },
            {
              "k": 2,
              "test_accuracy": 0.6572728762544484,
              "test_auc": 0.6696793806599756,
              "test_f1": 0.6384237879418574
            },
            {
              "k": 5,
              "test_accuracy": 0.6638735036927684,
              "test_auc": 0.688367505246788,
              "test_f1": 0.6439479106296234
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6638735036927684,
            "test_auc": 0.688367505246788,
            "test_f1": 0.6439479106296234
          },
          "best_minus_llm_auc": -0.017257680751151527
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T20:42:31+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed42/sae_final.pt --architecture relu --sae-release husai_relu_seed42 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed42/cebench --artifacts-path /tmp/ce_bench_artifacts_frontier_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "444db117d314a994252853c3b4996b0ef61407e7cbd7d2c35f1d8231c3d55cc9",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed42/sae_final.pt",
          "architecture_override": "relu",
          "checkpoint_sha256": "fe206d2ac6464912bb1f855c8249cf7434b6034e7a6d9cb4146a2181fd327a17",
          "sae_release": "husai_relu_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed42/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_frontier_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "relu",
          "d_model": 512,
          "d_sae": 1024,
          "k": null
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed42/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed42/cebench/interpretability_eval/husai_relu_seed42/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed42/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 4.484524784088134,
          "independent_score_mean_max": 4.622185326814652,
          "interpretability_score_mean_max": 4.228144862651825,
          "sae_release": "husai_relu_seed42",
          "sae_id": "custom_sae",
          "date": "2026-02-14 20:42:31",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 4.484524784088134,
          "independent_score_mean_max": 4.622185326814652,
          "interpretability_score_mean_max": 4.228144862651825
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -46.0267826461792,
          "independent_score_mean_max": -46.37708102107048,
          "interpretability_score_mean_max": -43.72346672296524
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed42/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed42/cebench/cebench_metrics_summary.md"
        }
      },
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "architecture": "relu",
      "seed": 123,
      "checkpoint": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed123/sae_final.pt",
      "train_metrics": {
        "mse": 3.5489608762873104e-06,
        "explained_variance": 0.9957223457556417,
        "l0": 653.2134399414062
      },
      "saebench": {
        "timestamp_utc": "2026-02-14T20:43:04+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed123/sae_final.pt --architecture relu --sae-release husai_relu_seed123 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_frontier_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed123/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed123/sae_final.pt",
          "architecture_override": "relu",
          "sae_release": "husai_relu_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 8,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_frontier_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "07943658c4039af93a762d7ddf0769ba924d31f46c94eec68c1d6b04b887c0ed",
        "sae_meta": {
          "architecture": "relu",
          "d_model": 512,
          "d_sae": 1024,
          "k": null
        },
        "summary": {
          "result_key": "husai_relu_seed123_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6822175045257715,
            "llm_test_auc": 0.7056251859979396,
            "llm_test_f1": 0.6610129056743866
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.6462662735143123,
              "test_auc": 0.6552507613769242,
              "test_f1": 0.6048949029989908
            },
            {
              "k": 2,
              "test_accuracy": 0.6545119254139108,
              "test_auc": 0.6631871509666695,
              "test_f1": 0.6362778811607007
            },
            {
              "k": 5,
              "test_accuracy": 0.6614434696132173,
              "test_auc": 0.6828590504134303,
              "test_f1": 0.6433415931910846
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6614434696132173,
            "test_auc": 0.6828590504134303,
            "test_f1": 0.6433415931910846
          },
          "best_minus_llm_auc": -0.02276613558450924
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T20:45:20+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed123/sae_final.pt --architecture relu --sae-release husai_relu_seed123 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed123/cebench --artifacts-path /tmp/ce_bench_artifacts_frontier_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "8305887a9c8f3eed727244ad5bb2a665dffc3354e528e2ea150ffed6fc2b2780",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed123/sae_final.pt",
          "architecture_override": "relu",
          "checkpoint_sha256": "00606524d8e95fae9055ee53d6c43734c081e2690fd53e8dc4f4a08a39c2c3be",
          "sae_release": "husai_relu_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed123/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_frontier_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "relu",
          "d_model": 512,
          "d_sae": 1024,
          "k": null
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed123/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed123/cebench/interpretability_eval/husai_relu_seed123/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed123/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 4.468147946596146,
          "independent_score_mean_max": 4.635419731140137,
          "interpretability_score_mean_max": 4.2479898595809935,
          "sae_release": "husai_relu_seed123",
          "sae_id": "custom_sae",
          "date": "2026-02-14 20:45:19",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 4.468147946596146,
          "independent_score_mean_max": 4.635419731140137,
          "interpretability_score_mean_max": 4.2479898595809935
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -46.04315948367119,
          "independent_score_mean_max": -46.36384661674499,
          "interpretability_score_mean_max": -43.703621726036076
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed123/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed123/cebench/cebench_metrics_summary.md"
        }
      },
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "architecture": "relu",
      "seed": 456,
      "checkpoint": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed456/sae_final.pt",
      "train_metrics": {
        "mse": 3.661714345071232e-06,
        "explained_variance": 0.9955860125749312,
        "l0": 649.6810302734375
      },
      "saebench": {
        "timestamp_utc": "2026-02-14T20:45:51+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed456/sae_final.pt --architecture relu --sae-release husai_relu_seed456 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_frontier_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed456/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed456/sae_final.pt",
          "architecture_override": "relu",
          "sae_release": "husai_relu_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 8,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_frontier_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "301e6168dceb158308085c7c8f8875a4a109a2886f638fef8c3e600fd45f4acb",
        "sae_meta": {
          "architecture": "relu",
          "d_model": 512,
          "d_sae": 1024,
          "k": null
        },
        "summary": {
          "result_key": "husai_relu_seed456_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6822175045257715,
            "llm_test_auc": 0.7056251859979396,
            "llm_test_f1": 0.6610129056743866
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.6315397849860271,
              "test_auc": 0.6634881208151926,
              "test_f1": 0.611814686396781
            },
            {
              "k": 2,
              "test_accuracy": 0.6332753574602811,
              "test_auc": 0.665084333373009,
              "test_f1": 0.5939832558080906
            },
            {
              "k": 5,
              "test_accuracy": 0.6460724612000578,
              "test_auc": 0.6754173847070203,
              "test_f1": 0.6269019398080252
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6460724612000578,
            "test_auc": 0.6754173847070203,
            "test_f1": 0.6269019398080252
          },
          "best_minus_llm_auc": -0.030207801290919267
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T20:48:05+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed456/sae_final.pt --architecture relu --sae-release husai_relu_seed456 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed456/cebench --artifacts-path /tmp/ce_bench_artifacts_frontier_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "98f238726ae0c8108adf341da75a1f2cf2f4a9dbb69d0c70f36f5ded5463cbed",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed456/sae_final.pt",
          "architecture_override": "relu",
          "checkpoint_sha256": "d6a72c3395924ce2e9c9eb261a560a7332a4e39a98df43082d66470fd4cff07b",
          "sae_release": "husai_relu_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed456/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_frontier_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "relu",
          "d_model": 512,
          "d_sae": 1024,
          "k": null
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed456/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed456/cebench/interpretability_eval/husai_relu_seed456/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed456/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 4.4638720703125,
          "independent_score_mean_max": 4.659163082838059,
          "interpretability_score_mean_max": 4.265124114751816,
          "sae_release": "husai_relu_seed456",
          "sae_id": "custom_sae",
          "date": "2026-02-14 20:48:05",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 4.4638720703125,
          "independent_score_mean_max": 4.659163082838059,
          "interpretability_score_mean_max": 4.265124114751816
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -46.04743535995484,
          "independent_score_mean_max": -46.34010326504708,
          "interpretability_score_mean_max": -43.68648747086525
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed456/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed456/cebench/cebench_metrics_summary.md"
        }
      },
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "architecture": "relu",
      "seed": 789,
      "checkpoint": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed789/sae_final.pt",
      "train_metrics": {
        "mse": 3.6666758660430787e-06,
        "explained_variance": 0.9955806481612426,
        "l0": 653.2283935546875
      },
      "saebench": {
        "timestamp_utc": "2026-02-14T20:48:36+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed789/sae_final.pt --architecture relu --sae-release husai_relu_seed789 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_frontier_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed789/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed789/sae_final.pt",
          "architecture_override": "relu",
          "sae_release": "husai_relu_seed789",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 8,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_frontier_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "26eb829bbda29f54ab5fd1b7f21347f7b4a32e44c41dfb77e524b4cdf66165f6",
        "sae_meta": {
          "architecture": "relu",
          "d_model": 512,
          "d_sae": 1024,
          "k": null
        },
        "summary": {
          "result_key": "husai_relu_seed789_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6822175045257715,
            "llm_test_auc": 0.7056251859979396,
            "llm_test_f1": 0.6610129056743866
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.6369780064773853,
              "test_auc": 0.6477663092441424,
              "test_f1": 0.5966404275698041
            },
            {
              "k": 2,
              "test_accuracy": 0.647211388282952,
              "test_auc": 0.6639006695065678,
              "test_f1": 0.6290855141391986
            },
            {
              "k": 5,
              "test_accuracy": 0.658567709088363,
              "test_auc": 0.6771562140525657,
              "test_f1": 0.6400316351904715
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.658567709088363,
            "test_auc": 0.6771562140525657,
            "test_f1": 0.6400316351904715
          },
          "best_minus_llm_auc": -0.0284689719453739
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T20:50:52+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed789/sae_final.pt --architecture relu --sae-release husai_relu_seed789 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed789/cebench --artifacts-path /tmp/ce_bench_artifacts_frontier_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "724a1ddd87471531305811cf53942774ce1e038cd0fcfb8ec1c9e654846db1b9",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed789/sae_final.pt",
          "architecture_override": "relu",
          "checkpoint_sha256": "b1eb76b890aca977a8b5c547e880ac6902f61ae98e2829aefeae8e6858b783fe",
          "sae_release": "husai_relu_seed789",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed789/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_frontier_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "relu",
          "d_model": 512,
          "d_sae": 1024,
          "k": null
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed789/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed789/cebench/interpretability_eval/husai_relu_seed789/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed789/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 4.534199960231781,
          "independent_score_mean_max": 4.552861764430999,
          "interpretability_score_mean_max": 4.286626415252686,
          "sae_release": "husai_relu_seed789",
          "sae_id": "custom_sae",
          "date": "2026-02-14 20:50:52",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 4.534199960231781,
          "independent_score_mean_max": 4.552861764430999,
          "interpretability_score_mean_max": 4.286626415252686
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -45.977107470035556,
          "independent_score_mean_max": -46.44640458345413,
          "interpretability_score_mean_max": -43.66498517036438
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed789/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed789/cebench/cebench_metrics_summary.md"
        }
      },
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "architecture": "relu",
      "seed": 1011,
      "checkpoint": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed1011/sae_final.pt",
      "train_metrics": {
        "mse": 3.734717438419466e-06,
        "explained_variance": 0.9954985385354921,
        "l0": 658.1140747070312
      },
      "saebench": {
        "timestamp_utc": "2026-02-14T20:51:24+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed1011/sae_final.pt --architecture relu --sae-release husai_relu_seed1011 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_frontier_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed1011/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed1011/sae_final.pt",
          "architecture_override": "relu",
          "sae_release": "husai_relu_seed1011",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 8,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_frontier_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "325f7db459cdc97d54589fcc69cb330e167348d8457669427250b8d093b5464b",
        "sae_meta": {
          "architecture": "relu",
          "d_model": 512,
          "d_sae": 1024,
          "k": null
        },
        "summary": {
          "result_key": "husai_relu_seed1011_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6822175045257715,
            "llm_test_auc": 0.7056251859979396,
            "llm_test_f1": 0.6610129056743866
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.6292184277355668,
              "test_auc": 0.6436017671009618,
              "test_f1": 0.6093911998251292
            },
            {
              "k": 2,
              "test_accuracy": 0.6450635554790197,
              "test_auc": 0.6689289651332146,
              "test_f1": 0.6268017386123423
            },
            {
              "k": 5,
              "test_accuracy": 0.660651782312651,
              "test_auc": 0.6808696554404337,
              "test_f1": 0.6425465715832335
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.660651782312651,
            "test_auc": 0.6808696554404337,
            "test_f1": 0.6425465715832335
          },
          "best_minus_llm_auc": -0.0247555305575059
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T20:53:40+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed1011/sae_final.pt --architecture relu --sae-release husai_relu_seed1011 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed1011/cebench --artifacts-path /tmp/ce_bench_artifacts_frontier_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "50318859077cf362a1cd26f6310e5d2ca04fdbeb5596965eaf3236137d6a02dc",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed1011/sae_final.pt",
          "architecture_override": "relu",
          "checkpoint_sha256": "8fe3a9006213f8c492f5addb2c0fd3ed8581dafdb00bb360eeba941764b923a4",
          "sae_release": "husai_relu_seed1011",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed1011/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_frontier_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "relu",
          "d_model": 512,
          "d_sae": 1024,
          "k": null
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed1011/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed1011/cebench/interpretability_eval/husai_relu_seed1011/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed1011/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 4.378119590282441,
          "independent_score_mean_max": 4.599649174213409,
          "interpretability_score_mean_max": 4.260546675920486,
          "sae_release": "husai_relu_seed1011",
          "sae_id": "custom_sae",
          "date": "2026-02-14 20:53:40",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 4.378119590282441,
          "independent_score_mean_max": 4.599649174213409,
          "interpretability_score_mean_max": 4.260546675920486
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -46.1331878399849,
          "independent_score_mean_max": -46.39961717367172,
          "interpretability_score_mean_max": -43.69106490969658
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed1011/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/relu_seed1011/cebench/cebench_metrics_summary.md"
        }
      },
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "architecture": "batchtopk",
      "seed": 42,
      "checkpoint": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/batchtopk_seed42/sae_final.pt",
      "train_metrics": {
        "mse": 0.00026667225756682456,
        "explained_variance": 0.6785504213576471,
        "l0": 79.89453887939453
      },
      "saebench": {
        "timestamp_utc": "2026-02-14T20:54:12+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/batchtopk_seed42/sae_final.pt --architecture batchtopk --sae-release husai_batchtopk_seed42 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_frontier_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed42/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/batchtopk_seed42/sae_final.pt",
          "architecture_override": "batchtopk",
          "sae_release": "husai_batchtopk_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 8,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_frontier_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "5ce5ae11e0016fbbee58e4e6f77afeaa382604e91e10ddb74f571b6bcff9aac9",
        "sae_meta": {
          "architecture": "batchtopk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_batchtopk_seed42_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6822175045257715,
            "llm_test_auc": 0.7056251859979396,
            "llm_test_f1": 0.6610129056743866
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.6241997694925336,
              "test_auc": 0.6408079631745889,
              "test_f1": 0.5868766191329755
            },
            {
              "k": 2,
              "test_accuracy": 0.6246730297256962,
              "test_auc": 0.6407326041690029,
              "test_f1": 0.5713708422807529
            },
            {
              "k": 5,
              "test_accuracy": 0.6483575954723662,
              "test_auc": 0.6600292441574206,
              "test_f1": 0.6029917904803057
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6483575954723662,
            "test_auc": 0.6600292441574206,
            "test_f1": 0.6029917904803057
          },
          "best_minus_llm_auc": -0.04559594184051896
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T20:56:27+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/batchtopk_seed42/sae_final.pt --architecture batchtopk --sae-release husai_batchtopk_seed42 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed42/cebench --artifacts-path /tmp/ce_bench_artifacts_frontier_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "f54719ea4408223fe96fb97405566612d3308f09e10824fdc6a87b9c590f067b",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/batchtopk_seed42/sae_final.pt",
          "architecture_override": "batchtopk",
          "checkpoint_sha256": "2a2ee3c2ac2e74bb9388a1267e3af5341d3ab3f43ed9d5656fba3e972a68b2e1",
          "sae_release": "husai_batchtopk_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed42/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_frontier_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "batchtopk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed42/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed42/cebench/interpretability_eval/husai_batchtopk_seed42/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed42/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 6.804405152797699,
          "independent_score_mean_max": 7.664323370456696,
          "interpretability_score_mean_max": 6.4668281817436215,
          "sae_release": "husai_batchtopk_seed42",
          "sae_id": "custom_sae",
          "date": "2026-02-14 20:56:27",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 6.804405152797699,
          "independent_score_mean_max": 7.664323370456696,
          "interpretability_score_mean_max": 6.4668281817436215
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -43.70690227746964,
          "independent_score_mean_max": -43.33494297742844,
          "interpretability_score_mean_max": -41.48478340387344
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed42/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed42/cebench/cebench_metrics_summary.md"
        }
      },
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "architecture": "batchtopk",
      "seed": 123,
      "checkpoint": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/batchtopk_seed123/sae_final.pt",
      "train_metrics": {
        "mse": 0.00027287116972729564,
        "explained_variance": 0.6710735272128845,
        "l0": 82.41203308105469
      },
      "saebench": {
        "timestamp_utc": "2026-02-14T20:57:00+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/batchtopk_seed123/sae_final.pt --architecture batchtopk --sae-release husai_batchtopk_seed123 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_frontier_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed123/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/batchtopk_seed123/sae_final.pt",
          "architecture_override": "batchtopk",
          "sae_release": "husai_batchtopk_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 8,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_frontier_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "c81eedc8a73f6d759cba2a37ef27e7923ff98133ed70895766ef2de5318a4c30",
        "sae_meta": {
          "architecture": "batchtopk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_batchtopk_seed123_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6822175045257715,
            "llm_test_auc": 0.7056251859979396,
            "llm_test_f1": 0.6610129056743866
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.636548707850021,
              "test_auc": 0.6563388148791649,
              "test_f1": 0.587987242519258
            },
            {
              "k": 2,
              "test_accuracy": 0.6499615203199675,
              "test_auc": 0.6664069034781085,
              "test_f1": 0.6249218471987469
            },
            {
              "k": 5,
              "test_accuracy": 0.6503786589913099,
              "test_auc": 0.6669909009535485,
              "test_f1": 0.6264925498585109
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6503786589913099,
            "test_auc": 0.6669909009535485,
            "test_f1": 0.6264925498585109
          },
          "best_minus_llm_auc": -0.03863428504439104
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T20:59:25+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/batchtopk_seed123/sae_final.pt --architecture batchtopk --sae-release husai_batchtopk_seed123 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed123/cebench --artifacts-path /tmp/ce_bench_artifacts_frontier_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "10a386d87a8f70bf349cee2b25c184d2c2b9530132884a57a19799cdcc8eed90",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/batchtopk_seed123/sae_final.pt",
          "architecture_override": "batchtopk",
          "checkpoint_sha256": "d7e5ab092c3cc0c55a2e42cfe5b498dc98f42799f97f6139c0decca09e1124f1",
          "sae_release": "husai_batchtopk_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed123/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_frontier_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "batchtopk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed123/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed123/cebench/interpretability_eval/husai_batchtopk_seed123/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed123/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 6.732831707000733,
          "independent_score_mean_max": 8.09984831571579,
          "interpretability_score_mean_max": 6.74163304567337,
          "sae_release": "husai_batchtopk_seed123",
          "sae_id": "custom_sae",
          "date": "2026-02-14 20:59:25",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 6.732831707000733,
          "independent_score_mean_max": 8.09984831571579,
          "interpretability_score_mean_max": 6.74163304567337
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -43.7784757232666,
          "independent_score_mean_max": -42.89941803216934,
          "interpretability_score_mean_max": -41.2099785399437
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed123/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed123/cebench/cebench_metrics_summary.md"
        }
      },
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "architecture": "batchtopk",
      "seed": 456,
      "checkpoint": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/batchtopk_seed456/sae_final.pt",
      "train_metrics": {
        "mse": 0.0002638226142153144,
        "explained_variance": 0.6820005210530773,
        "l0": 81.59379577636719
      },
      "saebench": {
        "timestamp_utc": "2026-02-14T21:00:02+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/batchtopk_seed456/sae_final.pt --architecture batchtopk --sae-release husai_batchtopk_seed456 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_frontier_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed456/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/batchtopk_seed456/sae_final.pt",
          "architecture_override": "batchtopk",
          "sae_release": "husai_batchtopk_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 8,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_frontier_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "04fedd5b1f3c92519e591ca5f238238f5da014f68ffd3bafa3b45cd8cb1f4b2a",
        "sae_meta": {
          "architecture": "batchtopk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_batchtopk_seed456_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6822175045257715,
            "llm_test_auc": 0.7056251859979396,
            "llm_test_f1": 0.6610129056743866
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.6335910905827763,
              "test_auc": 0.6388980960979364,
              "test_f1": 0.5818288692152778
            },
            {
              "k": 2,
              "test_accuracy": 0.6464212385429181,
              "test_auc": 0.6541005764399199,
              "test_f1": 0.5933939204441195
            },
            {
              "k": 5,
              "test_accuracy": 0.6492325010404698,
              "test_auc": 0.6605722547648677,
              "test_f1": 0.6246584247331888
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6492325010404698,
            "test_auc": 0.6605722547648677,
            "test_f1": 0.6246584247331888
          },
          "best_minus_llm_auc": -0.04505293123307186
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T21:02:43+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/batchtopk_seed456/sae_final.pt --architecture batchtopk --sae-release husai_batchtopk_seed456 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed456/cebench --artifacts-path /tmp/ce_bench_artifacts_frontier_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "ee944dd86c3c7141f0fa2b9878137ee754852571f0d053862b89c784c7cc3199",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/batchtopk_seed456/sae_final.pt",
          "architecture_override": "batchtopk",
          "checkpoint_sha256": "e505c7a7ee2a9a4c418646959b33eee6639b3beae9a68f11b0c9dafbb8d51b79",
          "sae_release": "husai_batchtopk_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed456/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_frontier_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "batchtopk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed456/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed456/cebench/interpretability_eval/husai_batchtopk_seed456/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed456/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 6.719196875095367,
          "independent_score_mean_max": 7.3243869471549985,
          "interpretability_score_mean_max": 6.493619351387024,
          "sae_release": "husai_batchtopk_seed456",
          "sae_id": "custom_sae",
          "date": "2026-02-14 21:02:43",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 6.719196875095367,
          "independent_score_mean_max": 7.3243869471549985,
          "interpretability_score_mean_max": 6.493619351387024
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -43.79211055517197,
          "independent_score_mean_max": -43.674879400730134,
          "interpretability_score_mean_max": -41.45799223423004
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed456/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed456/cebench/cebench_metrics_summary.md"
        }
      },
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "architecture": "batchtopk",
      "seed": 789,
      "checkpoint": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/batchtopk_seed789/sae_final.pt",
      "train_metrics": {
        "mse": 0.0002648289082571864,
        "explained_variance": 0.6807637742794701,
        "l0": 80.90961456298828
      },
      "saebench": {
        "timestamp_utc": "2026-02-14T21:03:20+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/batchtopk_seed789/sae_final.pt --architecture batchtopk --sae-release husai_batchtopk_seed789 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_frontier_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed789/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/batchtopk_seed789/sae_final.pt",
          "architecture_override": "batchtopk",
          "sae_release": "husai_batchtopk_seed789",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 8,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_frontier_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "9baeeec6e07dcaedee9174cc01b4dd6dc955bb26352e9af890ede70ca8db485a",
        "sae_meta": {
          "architecture": "batchtopk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_batchtopk_seed789_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6822175045257715,
            "llm_test_auc": 0.7056251859979396,
            "llm_test_f1": 0.6610129056743866
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.6388947207627294,
              "test_auc": 0.6469539480989793,
              "test_f1": 0.5978265371361543
            },
            {
              "k": 2,
              "test_accuracy": 0.6429403483639147,
              "test_auc": 0.6526621828374604,
              "test_f1": 0.613144049517589
            },
            {
              "k": 5,
              "test_accuracy": 0.6506738889994275,
              "test_auc": 0.6578196838759898,
              "test_f1": 0.6244502658953411
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6506738889994275,
            "test_auc": 0.6578196838759898,
            "test_f1": 0.6244502658953411
          },
          "best_minus_llm_auc": -0.04780550212194978
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T21:06:01+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/batchtopk_seed789/sae_final.pt --architecture batchtopk --sae-release husai_batchtopk_seed789 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed789/cebench --artifacts-path /tmp/ce_bench_artifacts_frontier_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "5ee7eda40b99f6b08a29b91b3027270492c96aa36286e4059e018324c8a0cfc4",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/batchtopk_seed789/sae_final.pt",
          "architecture_override": "batchtopk",
          "checkpoint_sha256": "e1ab6d1fa63316b2369246f4765b1eae1654cc595363e6774a0ab411d64d6a76",
          "sae_release": "husai_batchtopk_seed789",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed789/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_frontier_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "batchtopk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed789/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed789/cebench/interpretability_eval/husai_batchtopk_seed789/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed789/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 6.960163791179657,
          "independent_score_mean_max": 7.620185921192169,
          "interpretability_score_mean_max": 6.650502767562866,
          "sae_release": "husai_batchtopk_seed789",
          "sae_id": "custom_sae",
          "date": "2026-02-14 21:06:01",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 6.960163791179657,
          "independent_score_mean_max": 7.620185921192169,
          "interpretability_score_mean_max": 6.650502767562866
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -43.551143639087684,
          "independent_score_mean_max": -43.379080426692965,
          "interpretability_score_mean_max": -41.3011088180542
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed789/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed789/cebench/cebench_metrics_summary.md"
        }
      },
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "architecture": "batchtopk",
      "seed": 1011,
      "checkpoint": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/batchtopk_seed1011/sae_final.pt",
      "train_metrics": {
        "mse": 0.00026934020570479333,
        "explained_variance": 0.6753263615879616,
        "l0": 78.57807922363281
      },
      "saebench": {
        "timestamp_utc": "2026-02-14T21:06:34+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/batchtopk_seed1011/sae_final.pt --architecture batchtopk --sae-release husai_batchtopk_seed1011 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_frontier_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed1011/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/batchtopk_seed1011/sae_final.pt",
          "architecture_override": "batchtopk",
          "sae_release": "husai_batchtopk_seed1011",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 8,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_frontier_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "a1a63f02ededf361a03f771c14c1aa1090883c0d384a416171fc029dc335fa3e",
        "sae_meta": {
          "architecture": "batchtopk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_batchtopk_seed1011_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6822175045257715,
            "llm_test_auc": 0.7056251859979396,
            "llm_test_f1": 0.6610129056743866
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.6312920180674229,
              "test_auc": 0.6401708084367025,
              "test_f1": 0.5766867248748241
            },
            {
              "k": 2,
              "test_accuracy": 0.6463435586533962,
              "test_auc": 0.6569996285504704,
              "test_f1": 0.6126890155710202
            },
            {
              "k": 5,
              "test_accuracy": 0.6519349308067341,
              "test_auc": 0.6659328589797535,
              "test_f1": 0.6213762644101292
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6519349308067341,
            "test_auc": 0.6659328589797535,
            "test_f1": 0.6213762644101292
          },
          "best_minus_llm_auc": -0.039692327018186035
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T21:08:49+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/batchtopk_seed1011/sae_final.pt --architecture batchtopk --sae-release husai_batchtopk_seed1011 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed1011/cebench --artifacts-path /tmp/ce_bench_artifacts_frontier_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "3c5021a578182fbbee17ba76a294667cbf8bd43bb27ef51258556dc19239308d",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/batchtopk_seed1011/sae_final.pt",
          "architecture_override": "batchtopk",
          "checkpoint_sha256": "988fbf5a362966480aac3e22a9d4f3bb93f1cf02bbb6f3a7800e1955a2bce17c",
          "sae_release": "husai_batchtopk_seed1011",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed1011/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_frontier_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "batchtopk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed1011/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed1011/cebench/interpretability_eval/husai_batchtopk_seed1011/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed1011/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 6.622306761741638,
          "independent_score_mean_max": 7.108637142181396,
          "interpretability_score_mean_max": 6.335611803531647,
          "sae_release": "husai_batchtopk_seed1011",
          "sae_id": "custom_sae",
          "date": "2026-02-14 21:08:49",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 6.622306761741638,
          "independent_score_mean_max": 7.108637142181396,
          "interpretability_score_mean_max": 6.335611803531647
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -43.889000668525696,
          "independent_score_mean_max": -43.89062920570374,
          "interpretability_score_mean_max": -41.61599978208542
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed1011/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/batchtopk_seed1011/cebench/cebench_metrics_summary.md"
        }
      },
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "architecture": "jumprelu",
      "seed": 42,
      "checkpoint": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/jumprelu_seed42/sae_final.pt",
      "train_metrics": {
        "mse": 3.885285877913702e-06,
        "explained_variance": 0.9953188298546644,
        "l0": 974.9674072265625
      },
      "saebench": {
        "timestamp_utc": "2026-02-14T21:09:26+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/jumprelu_seed42/sae_final.pt --architecture jumprelu --sae-release husai_jumprelu_seed42 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_frontier_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed42/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/jumprelu_seed42/sae_final.pt",
          "architecture_override": "jumprelu",
          "sae_release": "husai_jumprelu_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 8,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_frontier_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "d118cd09bffe7e7095278f528480acdfe3b8a497d166b1da96008adeeb9c01fc",
        "sae_meta": {
          "architecture": "jumprelu",
          "d_model": 512,
          "d_sae": 1024,
          "k": null
        },
        "summary": {
          "result_key": "husai_jumprelu_seed42_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6822175045257715,
            "llm_test_auc": 0.7056251859979396,
            "llm_test_f1": 0.6610129056743866
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.6526212119953946,
              "test_auc": 0.662551885228811,
              "test_f1": 0.6166819854759481
            },
            {
              "k": 2,
              "test_accuracy": 0.659245147036593,
              "test_auc": 0.6697764585897433,
              "test_f1": 0.6404011191751582
            },
            {
              "k": 5,
              "test_accuracy": 0.6625109917390476,
              "test_auc": 0.6741640133042202,
              "test_f1": 0.6438175872635046
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6625109917390476,
            "test_auc": 0.6741640133042202,
            "test_f1": 0.6438175872635046
          },
          "best_minus_llm_auc": -0.0314611726937194
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T21:11:53+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/jumprelu_seed42/sae_final.pt --architecture jumprelu --sae-release husai_jumprelu_seed42 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed42/cebench --artifacts-path /tmp/ce_bench_artifacts_frontier_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "188ddd553d21df9ef7d8aa95cf6503d4576ab8d87ddf37dca1ffc17cbdf47a1a",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/jumprelu_seed42/sae_final.pt",
          "architecture_override": "jumprelu",
          "checkpoint_sha256": "7d8c81a22578b73ac36c02d122256c648abe876601b158d195581e9afef80f2c",
          "sae_release": "husai_jumprelu_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed42/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_frontier_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "jumprelu",
          "d_model": 512,
          "d_sae": 1024,
          "k": null
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed42/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed42/cebench/interpretability_eval/husai_jumprelu_seed42/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed42/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 4.5816484069824215,
          "independent_score_mean_max": 4.6479918885231015,
          "interpretability_score_mean_max": 4.350986768007278,
          "sae_release": "husai_jumprelu_seed42",
          "sae_id": "custom_sae",
          "date": "2026-02-14 21:11:53",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 4.5816484069824215,
          "independent_score_mean_max": 4.6479918885231015,
          "interpretability_score_mean_max": 4.350986768007278
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -45.92965902328491,
          "independent_score_mean_max": -46.35127445936203,
          "interpretability_score_mean_max": -43.600624817609784
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed42/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed42/cebench/cebench_metrics_summary.md"
        }
      },
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "architecture": "jumprelu",
      "seed": 123,
      "checkpoint": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/jumprelu_seed123/sae_final.pt",
      "train_metrics": {
        "mse": 3.561417088349117e-06,
        "explained_variance": 0.9957073184355258,
        "l0": 975.0562744140625
      },
      "saebench": {
        "timestamp_utc": "2026-02-14T21:12:29+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/jumprelu_seed123/sae_final.pt --architecture jumprelu --sae-release husai_jumprelu_seed123 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_frontier_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed123/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/jumprelu_seed123/sae_final.pt",
          "architecture_override": "jumprelu",
          "sae_release": "husai_jumprelu_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 8,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_frontier_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "46d7e6d2f37a0e0ae45db3839ebf4d3dc21442771f9dba9eb0dca153e01e739e",
        "sae_meta": {
          "architecture": "jumprelu",
          "d_model": 512,
          "d_sae": 1024,
          "k": null
        },
        "summary": {
          "result_key": "husai_jumprelu_seed123_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6822175045257715,
            "llm_test_auc": 0.7056251859979396,
            "llm_test_f1": 0.6610129056743866
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.644387573814001,
              "test_auc": 0.6521589907218441,
              "test_f1": 0.6054976173961173
            },
            {
              "k": 2,
              "test_accuracy": 0.6514849808030062,
              "test_auc": 0.6621105713689509,
              "test_f1": 0.6335840499605155
            },
            {
              "k": 5,
              "test_accuracy": 0.6573059659793375,
              "test_auc": 0.6842103734956496,
              "test_f1": 0.6353317328058807
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6573059659793375,
            "test_auc": 0.6842103734956496,
            "test_f1": 0.6353317328058807
          },
          "best_minus_llm_auc": -0.021414812502289937
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T21:14:57+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/jumprelu_seed123/sae_final.pt --architecture jumprelu --sae-release husai_jumprelu_seed123 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed123/cebench --artifacts-path /tmp/ce_bench_artifacts_frontier_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "7f077ca8228539eb0d3d0fa0437d8755b3ff3307440170aeb1b313767175ada2",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/jumprelu_seed123/sae_final.pt",
          "architecture_override": "jumprelu",
          "checkpoint_sha256": "0146e41a147af2aa40710e92bf07a9916293dd7b8e2b5c1cdd0b4b3d4881f6c9",
          "sae_release": "husai_jumprelu_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed123/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_frontier_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "jumprelu",
          "d_model": 512,
          "d_sae": 1024,
          "k": null
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed123/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed123/cebench/interpretability_eval/husai_jumprelu_seed123/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed123/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 4.4674470782279965,
          "independent_score_mean_max": 4.6784037852287295,
          "interpretability_score_mean_max": 4.318877098560333,
          "sae_release": "husai_jumprelu_seed123",
          "sae_id": "custom_sae",
          "date": "2026-02-14 21:14:57",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 4.4674470782279965,
          "independent_score_mean_max": 4.6784037852287295,
          "interpretability_score_mean_max": 4.318877098560333
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -46.04386035203934,
          "independent_score_mean_max": -46.3208625626564,
          "interpretability_score_mean_max": -43.63273448705673
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed123/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed123/cebench/cebench_metrics_summary.md"
        }
      },
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "architecture": "jumprelu",
      "seed": 456,
      "checkpoint": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/jumprelu_seed456/sae_final.pt",
      "train_metrics": {
        "mse": 2.527550577724469e-06,
        "explained_variance": 0.9969552797171284,
        "l0": 975.3536376953125
      },
      "saebench": {
        "timestamp_utc": "2026-02-14T21:15:28+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/jumprelu_seed456/sae_final.pt --architecture jumprelu --sae-release husai_jumprelu_seed456 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_frontier_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed456/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/jumprelu_seed456/sae_final.pt",
          "architecture_override": "jumprelu",
          "sae_release": "husai_jumprelu_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 8,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_frontier_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "93074893ec8769be75b4847daf61ba2ced0480896ce9d85a68cc7a3719c31823",
        "sae_meta": {
          "architecture": "jumprelu",
          "d_model": 512,
          "d_sae": 1024,
          "k": null
        },
        "summary": {
          "result_key": "husai_jumprelu_seed456_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6822175045257715,
            "llm_test_auc": 0.7056251859979396,
            "llm_test_f1": 0.6610129056743866
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.6360716391815188,
              "test_auc": 0.6632650939046573,
              "test_f1": 0.5972341655344968
            },
            {
              "k": 2,
              "test_accuracy": 0.6472862382714811,
              "test_auc": 0.674375939183433,
              "test_f1": 0.6088630396515138
            },
            {
              "k": 5,
              "test_accuracy": 0.6404319369992832,
              "test_auc": 0.6733299810020346,
              "test_f1": 0.6226303791992384
            }
          ],
          "best_by_auc": {
            "k": 2,
            "test_accuracy": 0.6472862382714811,
            "test_auc": 0.674375939183433,
            "test_f1": 0.6088630396515138
          },
          "best_minus_llm_auc": -0.03124924681450658
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T21:17:53+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/jumprelu_seed456/sae_final.pt --architecture jumprelu --sae-release husai_jumprelu_seed456 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed456/cebench --artifacts-path /tmp/ce_bench_artifacts_frontier_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "832963c10a5e5df7639abd7c8f5c36b76d301bc3b351736042ef8959064e5716",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/jumprelu_seed456/sae_final.pt",
          "architecture_override": "jumprelu",
          "checkpoint_sha256": "19266c80bce53b2e6842689be92860822c9f5b88ad121acb9264b0722d7ed3ed",
          "sae_release": "husai_jumprelu_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed456/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_frontier_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "jumprelu",
          "d_model": 512,
          "d_sae": 1024,
          "k": null
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed456/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed456/cebench/interpretability_eval/husai_jumprelu_seed456/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed456/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 4.528254990577698,
          "independent_score_mean_max": 4.687939922809601,
          "interpretability_score_mean_max": 4.368884451389313,
          "sae_release": "husai_jumprelu_seed456",
          "sae_id": "custom_sae",
          "date": "2026-02-14 21:17:53",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 4.528254990577698,
          "independent_score_mean_max": 4.687939922809601,
          "interpretability_score_mean_max": 4.368884451389313
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -45.98305243968964,
          "independent_score_mean_max": -46.311326425075535,
          "interpretability_score_mean_max": -43.58272713422775
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed456/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed456/cebench/cebench_metrics_summary.md"
        }
      },
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "architecture": "jumprelu",
      "seed": 789,
      "checkpoint": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/jumprelu_seed789/sae_final.pt",
      "train_metrics": {
        "mse": 3.858925083477516e-06,
        "explained_variance": 0.9953530089429435,
        "l0": 974.0872192382812
      },
      "saebench": {
        "timestamp_utc": "2026-02-14T21:18:29+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/jumprelu_seed789/sae_final.pt --architecture jumprelu --sae-release husai_jumprelu_seed789 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_frontier_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed789/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/jumprelu_seed789/sae_final.pt",
          "architecture_override": "jumprelu",
          "sae_release": "husai_jumprelu_seed789",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 8,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_frontier_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "378bd51dd9630b284dbbad550fa835e2fb3d116fe182b7cba944b2bd9154b0af",
        "sae_meta": {
          "architecture": "jumprelu",
          "d_model": 512,
          "d_sae": 1024,
          "k": null
        },
        "summary": {
          "result_key": "husai_jumprelu_seed789_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6822175045257715,
            "llm_test_auc": 0.7056251859979396,
            "llm_test_f1": 0.6610129056743866
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.6501764068674195,
              "test_auc": 0.667486497719835,
              "test_f1": 0.6315628420074028
            },
            {
              "k": 2,
              "test_accuracy": 0.6570750651094649,
              "test_auc": 0.6759701298237771,
              "test_f1": 0.639049147924728
            },
            {
              "k": 5,
              "test_accuracy": 0.6601376882194165,
              "test_auc": 0.6801459699716302,
              "test_f1": 0.6404274823384338
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6601376882194165,
            "test_auc": 0.6801459699716302,
            "test_f1": 0.6404274823384338
          },
          "best_minus_llm_auc": -0.025479216026309315
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T21:21:00+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/jumprelu_seed789/sae_final.pt --architecture jumprelu --sae-release husai_jumprelu_seed789 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed789/cebench --artifacts-path /tmp/ce_bench_artifacts_frontier_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "e7971991a31d64e806e947e9a4180efd3aa8bae5dd00e4d3522f02b9e0704605",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/jumprelu_seed789/sae_final.pt",
          "architecture_override": "jumprelu",
          "checkpoint_sha256": "bf90e4772b84fc09fc5dd0a7ed111df9ef0a0cd084ce4e60f450b3c9535c9d80",
          "sae_release": "husai_jumprelu_seed789",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed789/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_frontier_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "jumprelu",
          "d_model": 512,
          "d_sae": 1024,
          "k": null
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed789/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed789/cebench/interpretability_eval/husai_jumprelu_seed789/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed789/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 4.627220405340195,
          "independent_score_mean_max": 4.588466300964355,
          "interpretability_score_mean_max": 4.472902365922928,
          "sae_release": "husai_jumprelu_seed789",
          "sae_id": "custom_sae",
          "date": "2026-02-14 21:21:00",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 4.627220405340195,
          "independent_score_mean_max": 4.588466300964355,
          "interpretability_score_mean_max": 4.472902365922928
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -45.88408702492714,
          "independent_score_mean_max": -46.41080004692078,
          "interpretability_score_mean_max": -43.47870921969414
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed789/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed789/cebench/cebench_metrics_summary.md"
        }
      },
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "architecture": "jumprelu",
      "seed": 1011,
      "checkpoint": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/jumprelu_seed1011/sae_final.pt",
      "train_metrics": {
        "mse": 3.0460275866062148e-06,
        "explained_variance": 0.9963296411293415,
        "l0": 980.3933715820312
      },
      "saebench": {
        "timestamp_utc": "2026-02-14T21:21:38+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/jumprelu_seed1011/sae_final.pt --architecture jumprelu --sae-release husai_jumprelu_seed1011 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_frontier_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed1011/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/jumprelu_seed1011/sae_final.pt",
          "architecture_override": "jumprelu",
          "sae_release": "husai_jumprelu_seed1011",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 8,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_frontier_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "b90138550befbe5e0d9768fc8a3859d50df05f3f29282178a7d64a0a6b263a7b",
        "sae_meta": {
          "architecture": "jumprelu",
          "d_model": 512,
          "d_sae": 1024,
          "k": null
        },
        "summary": {
          "result_key": "husai_jumprelu_seed1011_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6822175045257715,
            "llm_test_auc": 0.7056251859979396,
            "llm_test_f1": 0.6610129056743866
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.6270342936880954,
              "test_auc": 0.6506732701737754,
              "test_f1": 0.5868957806238322
            },
            {
              "k": 2,
              "test_accuracy": 0.6352925974767315,
              "test_auc": 0.6533497813921095,
              "test_f1": 0.6168411028037384
            },
            {
              "k": 5,
              "test_accuracy": 0.650094084936491,
              "test_auc": 0.662345458061901,
              "test_f1": 0.6301867784944964
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.650094084936491,
            "test_auc": 0.662345458061901,
            "test_f1": 0.6301867784944964
          },
          "best_minus_llm_auc": -0.043279727936038515
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T21:24:16+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/jumprelu_seed1011/sae_final.pt --architecture jumprelu --sae-release husai_jumprelu_seed1011 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed1011/cebench --artifacts-path /tmp/ce_bench_artifacts_frontier_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "745b3fbc0a9b468cbf3edd7e82f2cb0fd3318d54935b9af9fdeca48c6c140af6",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/jumprelu_seed1011/sae_final.pt",
          "architecture_override": "jumprelu",
          "checkpoint_sha256": "768506d8625868e95248ddee9bf58270ddf69a06ad53c2872cc1eb75f82b91b8",
          "sae_release": "husai_jumprelu_seed1011",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed1011/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_frontier_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "jumprelu",
          "d_model": 512,
          "d_sae": 1024,
          "k": null
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed1011/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed1011/cebench/interpretability_eval/husai_jumprelu_seed1011/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed1011/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 4.511317571401596,
          "independent_score_mean_max": 4.708419317007065,
          "interpretability_score_mean_max": 4.383361794948578,
          "sae_release": "husai_jumprelu_seed1011",
          "sae_id": "custom_sae",
          "date": "2026-02-14 21:24:16",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 4.511317571401596,
          "independent_score_mean_max": 4.708419317007065,
          "interpretability_score_mean_max": 4.383361794948578
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -45.99998985886574,
          "independent_score_mean_max": -46.29084703087807,
          "interpretability_score_mean_max": -43.56824979066849
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed1011/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/external_eval/jumprelu_seed1011/cebench/cebench_metrics_summary.md"
        }
      },
      "saebench_returncode": 0,
      "cebench_returncode": 0
    }
  ],
  "aggregate": {
    "topk": {
      "train_mse": {
        "mean": 0.00021146551880519838,
        "std": 1.2470473327172792e-06,
        "min": 0.00020986865274608135,
        "max": 0.00021285809634719044,
        "n": 5
      },
      "train_ev": {
        "mean": 0.7450903693098755,
        "std": 0.001503236341090101,
        "min": 0.7434116009878537,
        "max": 0.7470152239977429,
        "n": 5
      },
      "train_l0": {
        "mean": 32.0,
        "std": 0.0,
        "min": 32.0,
        "max": 32.0,
        "n": 5
      },
      "saebench_best_auc": {
        "mean": 0.6650323857322449,
        "std": 0.004787919673827799,
        "min": 0.6603652285623202,
        "max": 0.6730207654270954,
        "n": 5
      },
      "saebench_best_minus_llm_auc": {
        "mean": -0.04059280026569467,
        "std": 0.004787919673827799,
        "min": -0.04525995743561939,
        "max": -0.03260442057084412,
        "n": 5
      },
      "cebench_contrastive_max": {
        "mean": 7.976090058326721,
        "std": 0.1831667031088131,
        "min": 7.7187209320068355,
        "max": 8.210204319953919,
        "n": 5
      },
      "cebench_independent_max": {
        "mean": 9.008341606616975,
        "std": 0.517817280498809,
        "min": 8.2837304520607,
        "max": 9.738143472671508,
        "n": 5
      },
      "cebench_interpretability_max": {
        "mean": 7.726768206596374,
        "std": 0.2763073731865415,
        "min": 7.3352331233024595,
        "max": 8.041117386817932,
        "n": 5
      },
      "cebench_interp_delta_vs_baseline": {
        "mean": -40.22484337902069,
        "std": 0.2763073731865414,
        "min": -40.61637846231461,
        "max": -39.910494198799135,
        "n": 5
      }
    },
    "relu": {
      "train_mse": {
        "mean": 3.6951568745280385e-06,
        "std": 1.154123107359228e-07,
        "min": 3.5489608762873104e-06,
        "max": 3.863715846819105e-06,
        "n": 5
      },
      "train_ev": {
        "mean": 0.9955460129388298,
        "std": 0.00013924453536972025,
        "min": 0.9953425196668417,
        "max": 0.9957223457556417,
        "n": 5
      },
      "train_l0": {
        "mean": 653.6598876953125,
        "std": 3.009098194526942,
        "min": 649.6810302734375,
        "max": 658.1140747070312,
        "n": 5
      },
      "saebench_best_auc": {
        "mean": 0.6809339619720476,
        "std": 0.005090964312356711,
        "min": 0.6754173847070203,
        "max": 0.688367505246788,
        "n": 5
      },
      "saebench_best_minus_llm_auc": {
        "mean": -0.024691224025891967,
        "std": 0.005090964312356711,
        "min": -0.030207801290919267,
        "max": -0.017257680751151527,
        "n": 5
      },
      "cebench_contrastive_max": {
        "mean": 4.465772870302201,
        "std": 0.05640534645113746,
        "min": 4.378119590282441,
        "max": 4.534199960231781,
        "n": 5
      },
      "cebench_independent_max": {
        "mean": 4.613855815887452,
        "std": 0.04033991389607773,
        "min": 4.552861764430999,
        "max": 4.659163082838059,
        "n": 5
      },
      "cebench_interpretability_max": {
        "mean": 4.257686385631561,
        "std": 0.021608740054005857,
        "min": 4.228144862651825,
        "max": 4.286626415252686,
        "n": 5
      },
      "cebench_interp_delta_vs_baseline": {
        "mean": -43.69392519998551,
        "std": 0.021608740054005766,
        "min": -43.72346672296524,
        "max": -43.66498517036438,
        "n": 5
      }
    },
    "batchtopk": {
      "train_mse": {
        "mean": 0.00026750703109428285,
        "std": 3.659871204617414e-06,
        "min": 0.0002638226142153144,
        "max": 0.00027287116972729564,
        "n": 5
      },
      "train_ev": {
        "mean": 0.6775429210982081,
        "std": 0.0044165977474633095,
        "min": 0.6710735272128845,
        "max": 0.6820005210530773,
        "n": 5
      },
      "train_l0": {
        "mean": 80.6776123046875,
        "std": 1.49354241477878,
        "min": 78.57807922363281,
        "max": 82.41203308105469,
        "n": 5
      },
      "saebench_best_auc": {
        "mean": 0.662268988546316,
        "std": 0.003981584980779549,
        "min": 0.6578196838759898,
        "max": 0.6669909009535485,
        "n": 5
      },
      "saebench_best_minus_llm_auc": {
        "mean": -0.043356197451623536,
        "std": 0.003981584980779549,
        "min": -0.04780550212194978,
        "max": -0.03863428504439104,
        "n": 5
      },
      "cebench_contrastive_max": {
        "mean": 6.767780857563018,
        "std": 0.1255956718809126,
        "min": 6.622306761741638,
        "max": 6.960163791179657,
        "n": 5
      },
      "cebench_independent_max": {
        "mean": 7.56347633934021,
        "std": 0.37587345946722767,
        "min": 7.108637142181396,
        "max": 8.09984831571579,
        "n": 5
      },
      "cebench_interpretability_max": {
        "mean": 6.537639029979705,
        "std": 0.15978007561776875,
        "min": 6.335611803531647,
        "max": 6.74163304567337,
        "n": 5
      },
      "cebench_interp_delta_vs_baseline": {
        "mean": -41.413972555637365,
        "std": 0.15978007561776653,
        "min": -41.61599978208542,
        "max": -41.2099785399437,
        "n": 5
      }
    },
    "jumprelu": {
      "train_mse": {
        "mean": 3.375841242814204e-06,
        "std": 5.821761606436967e-07,
        "min": 2.527550577724469e-06,
        "max": 3.885285877913702e-06,
        "n": 5
      },
      "train_ev": {
        "mean": 0.9959328156159207,
        "std": 0.0007012322364560376,
        "min": 0.9953188298546644,
        "max": 0.9969552797171284,
        "n": 5
      },
      "train_l0": {
        "mean": 975.97158203125,
        "std": 2.5164983728233916,
        "min": 974.0872192382812,
        "max": 980.3933715820312,
        "n": 5
      },
      "saebench_best_auc": {
        "mean": 0.6750483508033668,
        "std": 0.008254181594785073,
        "min": 0.662345458061901,
        "max": 0.6842103734956496,
        "n": 5
      },
      "saebench_best_minus_llm_auc": {
        "mean": -0.03057683519457275,
        "std": 0.008254181594785073,
        "min": -0.043279727936038515,
        "max": -0.021414812502289937,
        "n": 5
      },
      "cebench_contrastive_max": {
        "mean": 4.543177690505982,
        "std": 0.06228172633567813,
        "min": 4.4674470782279965,
        "max": 4.627220405340195,
        "n": 5
      },
      "cebench_independent_max": {
        "mean": 4.66224424290657,
        "std": 0.04663649473488473,
        "min": 4.588466300964355,
        "max": 4.708419317007065,
        "n": 5
      },
      "cebench_interpretability_max": {
        "mean": 4.379002495765685,
        "std": 0.05774623000216079,
        "min": 4.318877098560333,
        "max": 4.472902365922928,
        "n": 5
      },
      "cebench_interp_delta_vs_baseline": {
        "mean": -43.57260908985138,
        "std": 0.05774623000215929,
        "min": -43.63273448705673,
        "max": -43.47870921969414,
        "n": 5
      }
    }
  }
}
