{
  "timestamp_utc": "2026-02-14T22:42:34+00:00",
  "command": "python scripts/experiments/run_external_metric_scaling_study.py --activation-cache-dir /tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped --activation-glob-template *_blocks.{layer}.hook_resid_pre.pt --hook-name-template blocks.{layer}.hook_resid_pre --token-budgets 10000,30000 --hook-layers 0,1 --d-sae-values 1024,2048 --seeds 42,123,456 --k 32 --epochs 6 --batch-size 4096 --learning-rate 0.001 --max-files 16 --max-rows-per-file 2048 --run-saebench --run-cebench --cebench-repo /workspace/CE-Bench --cebench-max-rows 200 --cebench-matched-baseline-summary docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json --saebench-datasets 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom --saebench-results-path /tmp/husai_saebench_probe_results_scaling_multiseed --saebench-model-cache-path /tmp/sae_bench_model_cache --cebench-artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --device cuda --dtype float32 --output-dir results/experiments/phase4e_external_scaling_study_multiseed",
  "config": {
    "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
    "activation_glob_template": "*_blocks.{layer}.hook_resid_pre.pt",
    "hook_name_template": "blocks.{layer}.hook_resid_pre",
    "token_budgets": [
      10000,
      30000
    ],
    "hook_layers": [
      0,
      1
    ],
    "d_sae_values": [
      1024,
      2048
    ],
    "seeds": [
      42,
      123,
      456
    ],
    "k": 32,
    "epochs": 6,
    "batch_size": 4096,
    "learning_rate": 0.001,
    "max_files": 16,
    "max_rows_per_file": 2048,
    "model_name": "pythia-70m-deduped",
    "device": "cuda",
    "dtype": "float32",
    "run_saebench": true,
    "run_cebench": true,
    "cebench_repo": "/workspace/CE-Bench",
    "cebench_max_rows": 200,
    "cebench_matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
    "saebench_datasets": [
      "100_news_fake",
      "105_click_bait",
      "106_hate_hate",
      "107_hate_offensive",
      "110_aimade_humangpt3",
      "113_movie_sent",
      "114_nyc_borough_Manhattan",
      "115_nyc_borough_Brooklyn",
      "116_nyc_borough_Bronx",
      "117_us_state_FL",
      "118_us_state_CA",
      "119_us_state_TX",
      "120_us_timezone_Chicago",
      "121_us_timezone_New_York",
      "122_us_timezone_Los_Angeles",
      "123_world_country_United_Kingdom"
    ],
    "saebench_dataset_limit": 0,
    "run_id": "run_20260214T212435Z"
  },
  "records": [
    {
      "condition_id": "tok10000_layer0_dsae1024_seed42",
      "token_budget": 10000,
      "hook_layer": 0,
      "hook_name": "blocks.0.hook_resid_pre",
      "d_sae": 1024,
      "seed": 42,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-14T21:24:48+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.0.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 10000,
          "d_sae": 1024,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 42,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae1024_seed42/sae_final.pt",
        "data": {
          "num_files_discovered": 113,
          "num_files_used": 5,
          "total_rows": 10000,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.0.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 0.023924529552459717,
          "mse": 0.0006875979597680271,
          "aux": 0.023236930991212528,
          "l0": 32.0,
          "explained_variance": 0.9982019997501268
        }
      },
      "activation_file_count": 113,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-14T21:26:11+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae1024_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer0_dsae1024_seed42 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae1024_seed42/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae1024_seed42/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok10000_layer0_dsae1024_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "af5619aaf9459d188e33617c51c5434cfde25d4f3fdc093a237cefac6b0e98a0",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok10000_layer0_dsae1024_seed42_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6605360072070458,
            "llm_test_auc": 0.6928164663947145,
            "llm_test_f1": 0.6491238078533221
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5903987347162004,
              "test_auc": 0.5939589336382974,
              "test_f1": 0.5281342705186512
            },
            {
              "k": 2,
              "test_accuracy": 0.5981931483839118,
              "test_auc": 0.6082231163019333,
              "test_f1": 0.5518259796231364
            },
            {
              "k": 5,
              "test_accuracy": 0.6072726025379925,
              "test_auc": 0.6277230783557071,
              "test_f1": 0.5786097562703737
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6072726025379925,
            "test_auc": 0.6277230783557071,
            "test_f1": 0.5786097562703737
          },
          "best_minus_llm_auc": -0.06509338803900744
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T21:28:49+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae1024_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer0_dsae1024_seed42 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae1024_seed42/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "2388f1656fa7ecd3f6fb430c02a8bac6917edbf62220d3649fdf803de6f022fc",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae1024_seed42/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "0418ea95802bdfb167214ed18391fd6530dae850271ab763aaa99ec6c29e1310",
          "sae_release": "husai_scaling_tok10000_layer0_dsae1024_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae1024_seed42/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae1024_seed42/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae1024_seed42/cebench/interpretability_eval/husai_scaling_tok10000_layer0_dsae1024_seed42/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae1024_seed42/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 6.1035255479812625,
          "independent_score_mean_max": 6.32729287147522,
          "interpretability_score_mean_max": 6.1836693835258485,
          "sae_release": "husai_scaling_tok10000_layer0_dsae1024_seed42",
          "sae_id": "custom_sae",
          "date": "2026-02-14 21:28:49",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 6.1035255479812625,
          "independent_score_mean_max": 6.32729287147522,
          "interpretability_score_mean_max": 6.1836693835258485
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -44.40778188228607,
          "independent_score_mean_max": -44.67197347640991,
          "interpretability_score_mean_max": -41.76794220209122
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae1024_seed42/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae1024_seed42/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae1024_seed42/sae_final.pt",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok10000_layer0_dsae1024_seed123",
      "token_budget": 10000,
      "hook_layer": 0,
      "hook_name": "blocks.0.hook_resid_pre",
      "d_sae": 1024,
      "seed": 123,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-14T21:29:04+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.0.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 10000,
          "d_sae": 1024,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 123,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae1024_seed123/sae_final.pt",
        "data": {
          "num_files_discovered": 113,
          "num_files_used": 5,
          "total_rows": 10000,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.0.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 0.02379264123737812,
          "mse": 0.0006858942215330899,
          "aux": 0.02310674637556076,
          "l0": 32.0,
          "explained_variance": 0.9982084311369147
        }
      },
      "activation_file_count": 113,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-14T21:29:41+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae1024_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer0_dsae1024_seed123 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae1024_seed123/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae1024_seed123/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok10000_layer0_dsae1024_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "6850065e6df66cdf0d14b8b7e0bcfe15eb7c8cb5f8b0a031ec680f94b16b0e67",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok10000_layer0_dsae1024_seed123_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6605360072070458,
            "llm_test_auc": 0.6928164663947145,
            "llm_test_f1": 0.6491238078533221
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5861750484053766,
              "test_auc": 0.5892655881173839,
              "test_f1": 0.513946961631109
            },
            {
              "k": 2,
              "test_accuracy": 0.5905527849526783,
              "test_auc": 0.6046519431434139,
              "test_f1": 0.5296517280716009
            },
            {
              "k": 5,
              "test_accuracy": 0.6014362414474675,
              "test_auc": 0.6209133024126676,
              "test_f1": 0.5594497759454683
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6014362414474675,
            "test_auc": 0.6209133024126676,
            "test_f1": 0.5594497759454683
          },
          "best_minus_llm_auc": -0.07190316398204688
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T21:32:00+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae1024_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer0_dsae1024_seed123 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae1024_seed123/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "8864c1d96cb5bb2fd93bb51e9ec6d1bf3b68ec5dd43e6fc92e444d125b70272d",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae1024_seed123/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "c333ad6f3d948b9a0f4ec4788ef72bba592795291636a69dcb33b89e78a040f2",
          "sae_release": "husai_scaling_tok10000_layer0_dsae1024_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae1024_seed123/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae1024_seed123/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae1024_seed123/cebench/interpretability_eval/husai_scaling_tok10000_layer0_dsae1024_seed123/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae1024_seed123/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 5.634961463212967,
          "independent_score_mean_max": 6.197543411254883,
          "interpretability_score_mean_max": 6.018432220220566,
          "sae_release": "husai_scaling_tok10000_layer0_dsae1024_seed123",
          "sae_id": "custom_sae",
          "date": "2026-02-14 21:32:00",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 5.634961463212967,
          "independent_score_mean_max": 6.197543411254883,
          "interpretability_score_mean_max": 6.018432220220566
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -44.87634596705437,
          "independent_score_mean_max": -44.80172293663025,
          "interpretability_score_mean_max": -41.9331793653965
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae1024_seed123/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae1024_seed123/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae1024_seed123/sae_final.pt",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok10000_layer0_dsae1024_seed456",
      "token_budget": 10000,
      "hook_layer": 0,
      "hook_name": "blocks.0.hook_resid_pre",
      "d_sae": 1024,
      "seed": 456,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-14T21:32:12+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.0.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 10000,
          "d_sae": 1024,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 456,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae1024_seed456/sae_final.pt",
        "data": {
          "num_files_discovered": 113,
          "num_files_used": 5,
          "total_rows": 10000,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.0.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 0.02399079129099846,
          "mse": 0.0007003333303146064,
          "aux": 0.023290457824865978,
          "l0": 32.0,
          "explained_variance": 0.9981626392627304
        }
      },
      "activation_file_count": 113,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-14T21:32:47+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae1024_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer0_dsae1024_seed456 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae1024_seed456/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae1024_seed456/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok10000_layer0_dsae1024_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "cb806bec095786f374cd089526ed738260cf7535285ef38555c1e27e172ff39f",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok10000_layer0_dsae1024_seed456_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6605360072070458,
            "llm_test_auc": 0.6928164663947145,
            "llm_test_f1": 0.6491238078533221
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5878874773197063,
              "test_auc": 0.5917173748304826,
              "test_f1": 0.5249293113536437
            },
            {
              "k": 2,
              "test_accuracy": 0.5980171604787029,
              "test_auc": 0.6064672182679284,
              "test_f1": 0.5498499317223879
            },
            {
              "k": 5,
              "test_accuracy": 0.5989926897646873,
              "test_auc": 0.6194876406793117,
              "test_f1": 0.5651406367341564
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.5989926897646873,
            "test_auc": 0.6194876406793117,
            "test_f1": 0.5651406367341564
          },
          "best_minus_llm_auc": -0.07332882571540278
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T21:35:14+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae1024_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer0_dsae1024_seed456 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae1024_seed456/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "47d56fc194eb44f442ae4c3793a0784807f6656f5bc48b4c652bdfe7cf5155ed",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae1024_seed456/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "e361f9f3e9e06c0cb329a402be50577cd2b4da425b145a645c5127c6db3bde22",
          "sae_release": "husai_scaling_tok10000_layer0_dsae1024_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae1024_seed456/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae1024_seed456/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae1024_seed456/cebench/interpretability_eval/husai_scaling_tok10000_layer0_dsae1024_seed456/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae1024_seed456/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 5.777439216375351,
          "independent_score_mean_max": 6.15539378285408,
          "interpretability_score_mean_max": 6.017686002254486,
          "sae_release": "husai_scaling_tok10000_layer0_dsae1024_seed456",
          "sae_id": "custom_sae",
          "date": "2026-02-14 21:35:14",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 5.777439216375351,
          "independent_score_mean_max": 6.15539378285408,
          "interpretability_score_mean_max": 6.017686002254486
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -44.73386821389199,
          "independent_score_mean_max": -44.84387256503105,
          "interpretability_score_mean_max": -41.93392558336258
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae1024_seed456/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae1024_seed456/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae1024_seed456/sae_final.pt",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok10000_layer0_dsae2048_seed42",
      "token_budget": 10000,
      "hook_layer": 0,
      "hook_name": "blocks.0.hook_resid_pre",
      "d_sae": 2048,
      "seed": 42,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-14T21:35:26+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.0.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 10000,
          "d_sae": 2048,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 42,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae2048_seed42/sae_final.pt",
        "data": {
          "num_files_discovered": 113,
          "num_files_used": 5,
          "total_rows": 10000,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.0.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 0.027739140515526135,
          "mse": 0.0007266085982943574,
          "aux": 0.027012531956036884,
          "l0": 32.0,
          "explained_variance": 0.998099990811878
        }
      },
      "activation_file_count": 113,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-14T21:36:01+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae2048_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer0_dsae2048_seed42 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae2048_seed42/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae2048_seed42/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok10000_layer0_dsae2048_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "de8b05f26c2d109464ef785413411dacda8f495906f3959129c6c2d34c993548",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok10000_layer0_dsae2048_seed42_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6605360072070458,
            "llm_test_auc": 0.6928164663947145,
            "llm_test_f1": 0.6491238078533221
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5888758791882616,
              "test_auc": 0.5873558965248387,
              "test_f1": 0.5043111853444856
            },
            {
              "k": 2,
              "test_accuracy": 0.5971572239701463,
              "test_auc": 0.6052092540111926,
              "test_f1": 0.5391802561581029
            },
            {
              "k": 5,
              "test_accuracy": 0.6015673758237768,
              "test_auc": 0.6193987212915215,
              "test_f1": 0.5555359738829213
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6015673758237768,
            "test_auc": 0.6193987212915215,
            "test_f1": 0.5555359738829213
          },
          "best_minus_llm_auc": -0.07341774510319299
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T21:38:26+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae2048_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer0_dsae2048_seed42 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae2048_seed42/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "f4d919376fcb3ab6b5c19ca670be1946409ac6d0b1eeae7f37463e3a2addd1c0",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae2048_seed42/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "12c2711c0d51c19d5f53f37ab0ad1a7575284e8f0a7940e5453f6f28bbd95f5d",
          "sae_release": "husai_scaling_tok10000_layer0_dsae2048_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae2048_seed42/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae2048_seed42/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae2048_seed42/cebench/interpretability_eval/husai_scaling_tok10000_layer0_dsae2048_seed42/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae2048_seed42/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 7.1261831831932065,
          "independent_score_mean_max": 7.748877003192901,
          "interpretability_score_mean_max": 7.5051356482505795,
          "sae_release": "husai_scaling_tok10000_layer0_dsae2048_seed42",
          "sae_id": "custom_sae",
          "date": "2026-02-14 21:38:26",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 7.1261831831932065,
          "independent_score_mean_max": 7.748877003192901,
          "interpretability_score_mean_max": 7.5051356482505795
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -43.38512424707413,
          "independent_score_mean_max": -43.25038934469223,
          "interpretability_score_mean_max": -40.446475937366486
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae2048_seed42/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae2048_seed42/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae2048_seed42/sae_final.pt",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok10000_layer0_dsae2048_seed123",
      "token_budget": 10000,
      "hook_layer": 0,
      "hook_name": "blocks.0.hook_resid_pre",
      "d_sae": 2048,
      "seed": 123,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-14T21:38:38+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.0.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 10000,
          "d_sae": 2048,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 123,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae2048_seed123/sae_final.pt",
        "data": {
          "num_files_discovered": 113,
          "num_files_used": 5,
          "total_rows": 10000,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.0.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 0.0276201485345761,
          "mse": 0.0007260841278669735,
          "aux": 0.026894064620137215,
          "l0": 32.0,
          "explained_variance": 0.9981034543306702
        }
      },
      "activation_file_count": 113,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-14T21:39:14+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae2048_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer0_dsae2048_seed123 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae2048_seed123/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae2048_seed123/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok10000_layer0_dsae2048_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "6a3668c82c122419d2e96a9426c735cf781c9947eac67b7f498287f424c4ee9b",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok10000_layer0_dsae2048_seed123_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6605360072070458,
            "llm_test_auc": 0.6928164663947145,
            "llm_test_f1": 0.6491238078533221
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5806877655941578,
              "test_auc": 0.5809584186787633,
              "test_f1": 0.5013294789747487
            },
            {
              "k": 2,
              "test_accuracy": 0.5850785074879666,
              "test_auc": 0.5872511693861652,
              "test_f1": 0.5222540727705147
            },
            {
              "k": 5,
              "test_accuracy": 0.5903430683233557,
              "test_auc": 0.5991072952056484,
              "test_f1": 0.5366964612789497
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.5903430683233557,
            "test_auc": 0.5991072952056484,
            "test_f1": 0.5366964612789497
          },
          "best_minus_llm_auc": -0.09370917118906608
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T21:41:41+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae2048_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer0_dsae2048_seed123 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae2048_seed123/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "4918e289afde43108865526ab7b5ab5aa0048396c9fb5fe4e202e7fe88be0655",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae2048_seed123/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "07a72e2b5668b29f0d4a64d6eff32ab3bb1af626bec0889a9424b3c5897bdb98",
          "sae_release": "husai_scaling_tok10000_layer0_dsae2048_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae2048_seed123/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae2048_seed123/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae2048_seed123/cebench/interpretability_eval/husai_scaling_tok10000_layer0_dsae2048_seed123/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae2048_seed123/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 6.752451677322387,
          "independent_score_mean_max": 7.693973977565765,
          "interpretability_score_mean_max": 7.224240756034851,
          "sae_release": "husai_scaling_tok10000_layer0_dsae2048_seed123",
          "sae_id": "custom_sae",
          "date": "2026-02-14 21:41:40",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 6.752451677322387,
          "independent_score_mean_max": 7.693973977565765,
          "interpretability_score_mean_max": 7.224240756034851
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -43.75885575294495,
          "independent_score_mean_max": -43.305292370319364,
          "interpretability_score_mean_max": -40.72737082958221
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae2048_seed123/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae2048_seed123/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae2048_seed123/sae_final.pt",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok10000_layer0_dsae2048_seed456",
      "token_budget": 10000,
      "hook_layer": 0,
      "hook_name": "blocks.0.hook_resid_pre",
      "d_sae": 2048,
      "seed": 456,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-14T21:41:52+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.0.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 10000,
          "d_sae": 2048,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 456,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae2048_seed456/sae_final.pt",
        "data": {
          "num_files_discovered": 113,
          "num_files_used": 5,
          "total_rows": 10000,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.0.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 0.027181211858987808,
          "mse": 0.0007088804850354791,
          "aux": 0.02647233133514722,
          "l0": 32.0,
          "explained_variance": 0.998140215359969
        }
      },
      "activation_file_count": 113,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-14T21:42:28+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae2048_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer0_dsae2048_seed456 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae2048_seed456/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae2048_seed456/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok10000_layer0_dsae2048_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "aed9c931e34170afd98eca4e5f3abf6e3689c574dfcc5c90724db292a8b85300",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok10000_layer0_dsae2048_seed456_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6605360072070458,
            "llm_test_auc": 0.6928164663947145,
            "llm_test_f1": 0.6491238078533221
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5891564651852224,
              "test_auc": 0.590753704554468,
              "test_f1": 0.5100593515597587
            },
            {
              "k": 2,
              "test_accuracy": 0.5957378159584937,
              "test_auc": 0.6075970467954673,
              "test_f1": 0.5343365281893284
            },
            {
              "k": 5,
              "test_accuracy": 0.6004439759006254,
              "test_auc": 0.6123442020359172,
              "test_f1": 0.5529446377195382
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6004439759006254,
            "test_auc": 0.6123442020359172,
            "test_f1": 0.5529446377195382
          },
          "best_minus_llm_auc": -0.08047226435879729
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T21:44:50+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae2048_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer0_dsae2048_seed456 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae2048_seed456/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "ad1d092a245f347f4e53c22c0334047480baaab2d53bb281b0754d84793e0bf4",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae2048_seed456/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "02d2646491628f601bc3658c9a050e3309ccc8f8d97f86aa0817c2ea30876ff8",
          "sae_release": "husai_scaling_tok10000_layer0_dsae2048_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae2048_seed456/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae2048_seed456/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae2048_seed456/cebench/interpretability_eval/husai_scaling_tok10000_layer0_dsae2048_seed456/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae2048_seed456/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 6.836932709217072,
          "independent_score_mean_max": 7.629131286144257,
          "interpretability_score_mean_max": 7.289512159824372,
          "sae_release": "husai_scaling_tok10000_layer0_dsae2048_seed456",
          "sae_id": "custom_sae",
          "date": "2026-02-14 21:44:50",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 6.836932709217072,
          "independent_score_mean_max": 7.629131286144257,
          "interpretability_score_mean_max": 7.289512159824372
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -43.67437472105026,
          "independent_score_mean_max": -43.370135061740875,
          "interpretability_score_mean_max": -40.6620994257927
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae2048_seed456/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer0_dsae2048_seed456/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer0_dsae2048_seed456/sae_final.pt",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok10000_layer1_dsae1024_seed42",
      "token_budget": 10000,
      "hook_layer": 1,
      "hook_name": "blocks.1.hook_resid_pre",
      "d_sae": 1024,
      "seed": 42,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-14T21:45:03+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.1.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 10000,
          "d_sae": 1024,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 42,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae1024_seed42/sae_final.pt",
        "data": {
          "num_files_discovered": 16,
          "num_files_used": 5,
          "total_rows": 10000,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.1.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 30.693647384643555,
          "mse": 0.09719767173131307,
          "aux": 30.59644953409831,
          "l0": 32.0,
          "explained_variance": 0.9973477099700362
        }
      },
      "activation_file_count": 16,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-14T21:47:15+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae1024_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer1_dsae1024_seed42 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae1024_seed42/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae1024_seed42/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok10000_layer1_dsae1024_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "67facd425bd149ff861cbf0ba86510e8b16a0071a57f457c736ef4504006f760",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok10000_layer1_dsae1024_seed42_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6892726671847929,
            "llm_test_auc": 0.7249531199453926,
            "llm_test_f1": 0.6888365744296719
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5951484569633723,
              "test_auc": 0.6059859502291934,
              "test_f1": 0.5580986101623011
            },
            {
              "k": 2,
              "test_accuracy": 0.6055067132435785,
              "test_auc": 0.620196184691548,
              "test_f1": 0.5783452602303576
            },
            {
              "k": 5,
              "test_accuracy": 0.607846635889937,
              "test_auc": 0.6296849225596066,
              "test_f1": 0.5893324753455207
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.607846635889937,
            "test_auc": 0.6296849225596066,
            "test_f1": 0.5893324753455207
          },
          "best_minus_llm_auc": -0.09526819738578596
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T21:49:29+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae1024_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer1_dsae1024_seed42 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae1024_seed42/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "dc4f952df721f254c257045a4e153bb934b9725cd13611ee5de2d4c7c33eae9a",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae1024_seed42/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "5880a7d2cbae1e0ddd0784c24383a3bf5f40dbdd0406aaed97f2e3cc1d124f8c",
          "sae_release": "husai_scaling_tok10000_layer1_dsae1024_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae1024_seed42/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae1024_seed42/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae1024_seed42/cebench/interpretability_eval/husai_scaling_tok10000_layer1_dsae1024_seed42/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae1024_seed42/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 8.353993542194367,
          "independent_score_mean_max": 8.643186836242675,
          "interpretability_score_mean_max": 7.965026619434357,
          "sae_release": "husai_scaling_tok10000_layer1_dsae1024_seed42",
          "sae_id": "custom_sae",
          "date": "2026-02-14 21:49:28",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 8.353993542194367,
          "independent_score_mean_max": 8.643186836242675,
          "interpretability_score_mean_max": 7.965026619434357
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -42.15731388807297,
          "independent_score_mean_max": -42.356079511642456,
          "interpretability_score_mean_max": -39.98658496618271
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae1024_seed42/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae1024_seed42/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae1024_seed42/sae_final.pt",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok10000_layer1_dsae1024_seed123",
      "token_budget": 10000,
      "hook_layer": 1,
      "hook_name": "blocks.1.hook_resid_pre",
      "d_sae": 1024,
      "seed": 123,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-14T21:49:40+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.1.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 10000,
          "d_sae": 1024,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 123,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae1024_seed123/sae_final.pt",
        "data": {
          "num_files_discovered": 16,
          "num_files_used": 5,
          "total_rows": 10000,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.1.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 30.879554748535156,
          "mse": 0.09864334017038345,
          "aux": 30.780911127726238,
          "l0": 32.0,
          "explained_variance": 0.9973074189200457
        }
      },
      "activation_file_count": 16,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-14T21:50:14+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae1024_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer1_dsae1024_seed123 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae1024_seed123/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae1024_seed123/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok10000_layer1_dsae1024_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "1e8cf96cdaf817561ebdc01b987663efccbae236bb7a3578682731d572b9a5cc",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok10000_layer1_dsae1024_seed123_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6892726671847929,
            "llm_test_auc": 0.7249531199453926,
            "llm_test_f1": 0.6888365744296719
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.58912400007913,
              "test_auc": 0.5980836971051798,
              "test_f1": 0.5322119687237977
            },
            {
              "k": 2,
              "test_accuracy": 0.592629991646939,
              "test_auc": 0.6172858153345406,
              "test_f1": 0.5495387362386535
            },
            {
              "k": 5,
              "test_accuracy": 0.6152279140156325,
              "test_auc": 0.6340163961038806,
              "test_f1": 0.5912966971027499
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6152279140156325,
            "test_auc": 0.6340163961038806,
            "test_f1": 0.5912966971027499
          },
          "best_minus_llm_auc": -0.09093672384151197
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T21:52:27+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae1024_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer1_dsae1024_seed123 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae1024_seed123/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "58e06705af0f9a546b456bd96ace28b0db74ad283a385994ef38c9cc1a6d7dc0",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae1024_seed123/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "e3c279b792c7e6f82189e235d126012d91a7c803d42e49fc5585a69b8f9f2344",
          "sae_release": "husai_scaling_tok10000_layer1_dsae1024_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae1024_seed123/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae1024_seed123/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae1024_seed123/cebench/interpretability_eval/husai_scaling_tok10000_layer1_dsae1024_seed123/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae1024_seed123/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 8.256409327983857,
          "independent_score_mean_max": 8.831921746730805,
          "interpretability_score_mean_max": 8.1289102602005,
          "sae_release": "husai_scaling_tok10000_layer1_dsae1024_seed123",
          "sae_id": "custom_sae",
          "date": "2026-02-14 21:52:26",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 8.256409327983857,
          "independent_score_mean_max": 8.831921746730805,
          "interpretability_score_mean_max": 8.1289102602005
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -42.25489810228348,
          "independent_score_mean_max": -42.167344601154326,
          "interpretability_score_mean_max": -39.82270132541657
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae1024_seed123/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae1024_seed123/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae1024_seed123/sae_final.pt",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok10000_layer1_dsae1024_seed456",
      "token_budget": 10000,
      "hook_layer": 1,
      "hook_name": "blocks.1.hook_resid_pre",
      "d_sae": 1024,
      "seed": 456,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-14T21:52:38+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.1.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 10000,
          "d_sae": 1024,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 456,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae1024_seed456/sae_final.pt",
        "data": {
          "num_files_discovered": 16,
          "num_files_used": 5,
          "total_rows": 10000,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.1.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 31.743549982706707,
          "mse": 0.10233704497416814,
          "aux": 31.641213099161785,
          "l0": 32.0,
          "explained_variance": 0.9972004854643253
        }
      },
      "activation_file_count": 16,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-14T21:53:09+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae1024_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer1_dsae1024_seed456 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae1024_seed456/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae1024_seed456/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok10000_layer1_dsae1024_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "79698c0af5425b3e4d227237e113cdff1c7dcd0e1737d4dc2046de91620b577b",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok10000_layer1_dsae1024_seed456_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6892726671847929,
            "llm_test_auc": 0.7249531199453926,
            "llm_test_f1": 0.6888365744296719
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5909786984521694,
              "test_auc": 0.6039557749546629,
              "test_f1": 0.5690719537936866
            },
            {
              "k": 2,
              "test_accuracy": 0.5995531866311412,
              "test_auc": 0.6192446250291722,
              "test_f1": 0.5825595439128594
            },
            {
              "k": 5,
              "test_accuracy": 0.6099313688595435,
              "test_auc": 0.6332066155116611,
              "test_f1": 0.6074349670166826
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6099313688595435,
            "test_auc": 0.6332066155116611,
            "test_f1": 0.6074349670166826
          },
          "best_minus_llm_auc": -0.09174650443373145
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T21:55:30+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae1024_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer1_dsae1024_seed456 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae1024_seed456/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "7689b7ec8138d33457c50e30af42848a1f50f73a37ee2a917267da443fac823c",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae1024_seed456/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "9280fd108d90c413834131c5c8d60652306f2ee3a6021f12d85bdb2303644ae1",
          "sae_release": "husai_scaling_tok10000_layer1_dsae1024_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae1024_seed456/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae1024_seed456/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae1024_seed456/cebench/interpretability_eval/husai_scaling_tok10000_layer1_dsae1024_seed456/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae1024_seed456/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 8.209339504241944,
          "independent_score_mean_max": 8.85705245733261,
          "interpretability_score_mean_max": 8.029204256534577,
          "sae_release": "husai_scaling_tok10000_layer1_dsae1024_seed456",
          "sae_id": "custom_sae",
          "date": "2026-02-14 21:55:30",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 8.209339504241944,
          "independent_score_mean_max": 8.85705245733261,
          "interpretability_score_mean_max": 8.029204256534577
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -42.30196792602539,
          "independent_score_mean_max": -42.142213890552526,
          "interpretability_score_mean_max": -39.92240732908249
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae1024_seed456/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae1024_seed456/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae1024_seed456/sae_final.pt",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok10000_layer1_dsae2048_seed42",
      "token_budget": 10000,
      "hook_layer": 1,
      "hook_name": "blocks.1.hook_resid_pre",
      "d_sae": 2048,
      "seed": 42,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-14T21:55:42+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.1.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 10000,
          "d_sae": 2048,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 42,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae2048_seed42/sae_final.pt",
        "data": {
          "num_files_discovered": 16,
          "num_files_used": 5,
          "total_rows": 10000,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.1.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 35.605055491129555,
          "mse": 0.09882016976674397,
          "aux": 35.50623575846354,
          "l0": 32.0,
          "explained_variance": 0.997303435911961
        }
      },
      "activation_file_count": 16,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-14T21:56:16+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae2048_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer1_dsae2048_seed42 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae2048_seed42/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae2048_seed42/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok10000_layer1_dsae2048_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "59c42490d030eab8e81a13cb21da86987c6322a6bdff776837715a92d2cbebba",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok10000_layer1_dsae2048_seed42_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6892726671847929,
            "llm_test_auc": 0.7249531199453926,
            "llm_test_f1": 0.6888365744296719
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5966319218257604,
              "test_auc": 0.6048622500167886,
              "test_f1": 0.5470320917495226
            },
            {
              "k": 2,
              "test_accuracy": 0.5998705692962161,
              "test_auc": 0.6143154286428657,
              "test_f1": 0.5671594830273673
            },
            {
              "k": 5,
              "test_accuracy": 0.6089285038463079,
              "test_auc": 0.6285164386917815,
              "test_f1": 0.5877860803494441
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6089285038463079,
            "test_auc": 0.6285164386917815,
            "test_f1": 0.5877860803494441
          },
          "best_minus_llm_auc": -0.09643668125361105
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T21:58:38+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae2048_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer1_dsae2048_seed42 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae2048_seed42/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "ad34c9a52ac48313f5462deea583ad8559eba2e889dc5229ad2a001207a3483f",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae2048_seed42/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "d64c59133df0a7365a4a48504dbbe312e28df770b3d798d821d459c43147c4cb",
          "sae_release": "husai_scaling_tok10000_layer1_dsae2048_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae2048_seed42/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae2048_seed42/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae2048_seed42/cebench/interpretability_eval/husai_scaling_tok10000_layer1_dsae2048_seed42/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae2048_seed42/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 10.061518630981446,
          "independent_score_mean_max": 11.641879398822784,
          "interpretability_score_mean_max": 10.38789572238922,
          "sae_release": "husai_scaling_tok10000_layer1_dsae2048_seed42",
          "sae_id": "custom_sae",
          "date": "2026-02-14 21:58:38",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 10.061518630981446,
          "independent_score_mean_max": 11.641879398822784,
          "interpretability_score_mean_max": 10.38789572238922
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -40.44978879928589,
          "independent_score_mean_max": -39.357386949062345,
          "interpretability_score_mean_max": -37.56371586322784
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae2048_seed42/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae2048_seed42/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae2048_seed42/sae_final.pt",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok10000_layer1_dsae2048_seed123",
      "token_budget": 10000,
      "hook_layer": 1,
      "hook_name": "blocks.1.hook_resid_pre",
      "d_sae": 2048,
      "seed": 123,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-14T21:58:50+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.1.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 10000,
          "d_sae": 2048,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 123,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae2048_seed123/sae_final.pt",
        "data": {
          "num_files_discovered": 16,
          "num_files_used": 5,
          "total_rows": 10000,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.1.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 34.886828104654946,
          "mse": 0.0953299676378568,
          "aux": 34.791497548421226,
          "l0": 32.0,
          "explained_variance": 0.9973978611554415
        }
      },
      "activation_file_count": 16,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-14T21:59:23+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae2048_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer1_dsae2048_seed123 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae2048_seed123/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae2048_seed123/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok10000_layer1_dsae2048_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "8eb1a584a378c9dd5f03b93f617efc07f27dc5d011cdac68a61f37c3b508a80c",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok10000_layer1_dsae2048_seed123_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6892726671847929,
            "llm_test_auc": 0.7249531199453926,
            "llm_test_f1": 0.6888365744296719
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5847308014386691,
              "test_auc": 0.592707811293502,
              "test_f1": 0.5294122725252431
            },
            {
              "k": 2,
              "test_accuracy": 0.5902602681203406,
              "test_auc": 0.6037567233740073,
              "test_f1": 0.5629963259030321
            },
            {
              "k": 5,
              "test_accuracy": 0.6029747907419506,
              "test_auc": 0.6203108683146042,
              "test_f1": 0.5799174292333348
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6029747907419506,
            "test_auc": 0.6203108683146042,
            "test_f1": 0.5799174292333348
          },
          "best_minus_llm_auc": -0.10464225163078833
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T22:01:49+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae2048_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer1_dsae2048_seed123 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae2048_seed123/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "24ea0cb7e9bf973638eb4a331bccab1180e289aa8ee99785f1084bc3c1acce48",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae2048_seed123/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "4cfcc0abd5088b90ac3672ee416b1a5411eb2762c011e2c7ee634175aab4ae54",
          "sae_release": "husai_scaling_tok10000_layer1_dsae2048_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae2048_seed123/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae2048_seed123/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae2048_seed123/cebench/interpretability_eval/husai_scaling_tok10000_layer1_dsae2048_seed123/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae2048_seed123/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 10.045469830036163,
          "independent_score_mean_max": 10.648979089260102,
          "interpretability_score_mean_max": 9.797955973148346,
          "sae_release": "husai_scaling_tok10000_layer1_dsae2048_seed123",
          "sae_id": "custom_sae",
          "date": "2026-02-14 22:01:49",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 10.045469830036163,
          "independent_score_mean_max": 10.648979089260102,
          "interpretability_score_mean_max": 9.797955973148346
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -40.465837600231175,
          "independent_score_mean_max": -40.35028725862503,
          "interpretability_score_mean_max": -38.15365561246872
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae2048_seed123/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae2048_seed123/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae2048_seed123/sae_final.pt",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok10000_layer1_dsae2048_seed456",
      "token_budget": 10000,
      "hook_layer": 1,
      "hook_name": "blocks.1.hook_resid_pre",
      "d_sae": 2048,
      "seed": 456,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-14T22:02:01+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.1.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 10000,
          "d_sae": 2048,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 456,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae2048_seed456/sae_final.pt",
        "data": {
          "num_files_discovered": 16,
          "num_files_used": 5,
          "total_rows": 10000,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.1.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 35.46282704671224,
          "mse": 0.09705839306116104,
          "aux": 35.36576843261719,
          "l0": 32.0,
          "explained_variance": 0.9973448873547938
        }
      },
      "activation_file_count": 16,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-14T22:02:36+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae2048_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer1_dsae2048_seed456 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae2048_seed456/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae2048_seed456/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok10000_layer1_dsae2048_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "9131e0242d05e0ae3d9ac1fc90f12c15c1b4c92975b484e0af0b108f7435774c",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok10000_layer1_dsae2048_seed456_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6892726671847929,
            "llm_test_auc": 0.7249531199453926,
            "llm_test_f1": 0.6888365744296719
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5854474761347989,
              "test_auc": 0.5918182894676515,
              "test_f1": 0.5339449255505163
            },
            {
              "k": 2,
              "test_accuracy": 0.5834740194584211,
              "test_auc": 0.5950048838587361,
              "test_f1": 0.5446701561640617
            },
            {
              "k": 5,
              "test_accuracy": 0.6024666847331104,
              "test_auc": 0.6264216979460538,
              "test_f1": 0.5681540561280853
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6024666847331104,
            "test_auc": 0.6264216979460538,
            "test_f1": 0.5681540561280853
          },
          "best_minus_llm_auc": -0.09853142199933873
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T22:05:01+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae2048_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer1_dsae2048_seed456 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae2048_seed456/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "b791e22d902e2e7f2c100a31cd20e2b5bc196a7e33a0e0237e3b799629e08317",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae2048_seed456/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "7d0311fc9294672aca94c60fed0ea24a7a7a828d195d69a634c900fb4bef9664",
          "sae_release": "husai_scaling_tok10000_layer1_dsae2048_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae2048_seed456/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae2048_seed456/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae2048_seed456/cebench/interpretability_eval/husai_scaling_tok10000_layer1_dsae2048_seed456/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae2048_seed456/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 9.792248265743256,
          "independent_score_mean_max": 11.173166918754578,
          "interpretability_score_mean_max": 9.79960574388504,
          "sae_release": "husai_scaling_tok10000_layer1_dsae2048_seed456",
          "sae_id": "custom_sae",
          "date": "2026-02-14 22:05:01",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 9.792248265743256,
          "independent_score_mean_max": 11.173166918754578,
          "interpretability_score_mean_max": 9.79960574388504
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -40.71905916452408,
          "independent_score_mean_max": -39.82609942913055,
          "interpretability_score_mean_max": -38.15200584173203
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae2048_seed456/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok10000_layer1_dsae2048_seed456/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok10000_layer1_dsae2048_seed456/sae_final.pt",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok30000_layer0_dsae1024_seed42",
      "token_budget": 30000,
      "hook_layer": 0,
      "hook_name": "blocks.0.hook_resid_pre",
      "d_sae": 1024,
      "seed": 42,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-14T22:05:14+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.0.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 30000,
          "d_sae": 1024,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 42,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae1024_seed42/sae_final.pt",
        "data": {
          "num_files_discovered": 113,
          "num_files_used": 16,
          "total_rows": 29432,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/113_movie_sent_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/114_nyc_borough_Manhattan_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/115_nyc_borough_Brooklyn_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/116_nyc_borough_Bronx_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/117_us_state_FL_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/118_us_state_CA_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/119_us_state_TX_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/120_us_timezone_Chicago_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/121_us_timezone_New_York_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/122_us_timezone_Los_Angeles_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/123_world_country_United_Kingdom_blocks.0.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 0.017084247898310423,
          "mse": 0.0005171554366825148,
          "aux": 0.016567092388868332,
          "l0": 32.0,
          "explained_variance": 0.9988613782775199
        }
      },
      "activation_file_count": 113,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-14T22:05:49+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae1024_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer0_dsae1024_seed42 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae1024_seed42/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae1024_seed42/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok30000_layer0_dsae1024_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "61101c0600df0124867202918f6b073aa2f75607384dcde493e5d2e7f2fab4da",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok30000_layer0_dsae1024_seed42_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6605360072070458,
            "llm_test_auc": 0.6928164663947145,
            "llm_test_f1": 0.6491238078533221
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5907864222559945,
              "test_auc": 0.5961540016688295,
              "test_f1": 0.5268683040363374
            },
            {
              "k": 2,
              "test_accuracy": 0.5891605257127436,
              "test_auc": 0.5991051916232417,
              "test_f1": 0.5214799265252326
            },
            {
              "k": 5,
              "test_accuracy": 0.5949139622866223,
              "test_auc": 0.6091209692709305,
              "test_f1": 0.5607896663058237
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.5949139622866223,
            "test_auc": 0.6091209692709305,
            "test_f1": 0.5607896663058237
          },
          "best_minus_llm_auc": -0.08369549712378399
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T22:08:10+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae1024_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer0_dsae1024_seed42 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae1024_seed42/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "517ae113225219162d0a81a73ff54520ce5bba3dd8002a8bba54b2116258f363",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae1024_seed42/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "0a680b634db1c845e6a2f360086f42fcbb965c203809f892214fd2d4dda8b059",
          "sae_release": "husai_scaling_tok30000_layer0_dsae1024_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae1024_seed42/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae1024_seed42/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae1024_seed42/cebench/interpretability_eval/husai_scaling_tok30000_layer0_dsae1024_seed42/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae1024_seed42/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 6.311198704242706,
          "independent_score_mean_max": 6.553330156803131,
          "interpretability_score_mean_max": 6.440171301364899,
          "sae_release": "husai_scaling_tok30000_layer0_dsae1024_seed42",
          "sae_id": "custom_sae",
          "date": "2026-02-14 22:08:10",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 6.311198704242706,
          "independent_score_mean_max": 6.553330156803131,
          "interpretability_score_mean_max": 6.440171301364899
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -44.20010872602463,
          "independent_score_mean_max": -44.445936191082,
          "interpretability_score_mean_max": -41.51144028425217
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae1024_seed42/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae1024_seed42/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae1024_seed42/sae_final.pt",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok30000_layer0_dsae1024_seed123",
      "token_budget": 30000,
      "hook_layer": 0,
      "hook_name": "blocks.0.hook_resid_pre",
      "d_sae": 1024,
      "seed": 123,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-14T22:08:23+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.0.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 30000,
          "d_sae": 1024,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 123,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae1024_seed123/sae_final.pt",
        "data": {
          "num_files_discovered": 113,
          "num_files_used": 16,
          "total_rows": 29432,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/113_movie_sent_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/114_nyc_borough_Manhattan_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/115_nyc_borough_Brooklyn_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/116_nyc_borough_Bronx_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/117_us_state_FL_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/118_us_state_CA_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/119_us_state_TX_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/120_us_timezone_Chicago_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/121_us_timezone_New_York_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/122_us_timezone_Los_Angeles_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/123_world_country_United_Kingdom_blocks.0.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 0.017122834688052535,
          "mse": 0.0005205777415540069,
          "aux": 0.01660225703381002,
          "l0": 32.0,
          "explained_variance": 0.9988539345273441
        }
      },
      "activation_file_count": 113,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-14T22:08:57+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae1024_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer0_dsae1024_seed123 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae1024_seed123/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae1024_seed123/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok30000_layer0_dsae1024_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "6f93557499523b22d880c72b32cdbea05eeb0ac5bb0e5787dd3725c3d4c45c3c",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok30000_layer0_dsae1024_seed123_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6605360072070458,
            "llm_test_auc": 0.6928164663947145,
            "llm_test_f1": 0.6491238078533221
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5791669073009936,
              "test_auc": 0.5910499058685158,
              "test_f1": 0.5253777385598273
            },
            {
              "k": 2,
              "test_accuracy": 0.5857482683149119,
              "test_auc": 0.6006771826979113,
              "test_f1": 0.5366302199078505
            },
            {
              "k": 5,
              "test_accuracy": 0.6013701368489877,
              "test_auc": 0.6203747097094297,
              "test_f1": 0.566973987944639
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6013701368489877,
            "test_auc": 0.6203747097094297,
            "test_f1": 0.566973987944639
          },
          "best_minus_llm_auc": -0.07244175668528485
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T22:11:19+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae1024_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer0_dsae1024_seed123 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae1024_seed123/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "debd966ec2e62bdab9b26356a8e4f7b6635b0628b531bf350fc491418c76d045",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae1024_seed123/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "4ce2bc82b7e4de2fe8d49d91cec633ee86873f3aa8b03f3bd0cc69866ad7bb78",
          "sae_release": "husai_scaling_tok30000_layer0_dsae1024_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae1024_seed123/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae1024_seed123/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae1024_seed123/cebench/interpretability_eval/husai_scaling_tok30000_layer0_dsae1024_seed123/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae1024_seed123/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 5.983096237182617,
          "independent_score_mean_max": 6.326351456642151,
          "interpretability_score_mean_max": 6.146210596561432,
          "sae_release": "husai_scaling_tok30000_layer0_dsae1024_seed123",
          "sae_id": "custom_sae",
          "date": "2026-02-14 22:11:19",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 5.983096237182617,
          "independent_score_mean_max": 6.326351456642151,
          "interpretability_score_mean_max": 6.146210596561432
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -44.52821119308472,
          "independent_score_mean_max": -44.67291489124298,
          "interpretability_score_mean_max": -41.805400989055634
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae1024_seed123/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae1024_seed123/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae1024_seed123/sae_final.pt",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok30000_layer0_dsae1024_seed456",
      "token_budget": 30000,
      "hook_layer": 0,
      "hook_name": "blocks.0.hook_resid_pre",
      "d_sae": 1024,
      "seed": 456,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-14T22:11:34+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.0.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 30000,
          "d_sae": 1024,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 456,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae1024_seed456/sae_final.pt",
        "data": {
          "num_files_discovered": 113,
          "num_files_used": 16,
          "total_rows": 29432,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/113_movie_sent_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/114_nyc_borough_Manhattan_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/115_nyc_borough_Brooklyn_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/116_nyc_borough_Bronx_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/117_us_state_FL_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/118_us_state_CA_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/119_us_state_TX_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/120_us_timezone_Chicago_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/121_us_timezone_New_York_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/122_us_timezone_Los_Angeles_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/123_world_country_United_Kingdom_blocks.0.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 0.016936915926635265,
          "mse": 0.0005123616574564949,
          "aux": 0.016424554167315364,
          "l0": 32.0,
          "explained_variance": 0.9988714647523498
        }
      },
      "activation_file_count": 113,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-14T22:12:11+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae1024_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer0_dsae1024_seed456 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae1024_seed456/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae1024_seed456/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok30000_layer0_dsae1024_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "2b5015490224e16cf0ee6c42a441fd8d989ae2eddc171b79e06b0a6652a5a04c",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok30000_layer0_dsae1024_seed456_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6605360072070458,
            "llm_test_auc": 0.6928164663947145,
            "llm_test_f1": 0.6491238078533221
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5873226835695563,
              "test_auc": 0.5922259225933229,
              "test_f1": 0.5078439165162539
            },
            {
              "k": 2,
              "test_accuracy": 0.5935018038151861,
              "test_auc": 0.6126340028405698,
              "test_f1": 0.52486183465231
            },
            {
              "k": 5,
              "test_accuracy": 0.6022962037010151,
              "test_auc": 0.617735259028003,
              "test_f1": 0.5521057841938213
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6022962037010151,
            "test_auc": 0.617735259028003,
            "test_f1": 0.5521057841938213
          },
          "best_minus_llm_auc": -0.07508120736671153
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T22:14:27+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae1024_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer0_dsae1024_seed456 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae1024_seed456/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "0d036e7db08e2abb03c39faf6fa81b627d8d898d22538537a43ae81b1759698f",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae1024_seed456/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "599ad4f876c18f336b57f5676df5b53de277080322d77d1fe5b26df8a2b8cb90",
          "sae_release": "husai_scaling_tok30000_layer0_dsae1024_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae1024_seed456/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae1024_seed456/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae1024_seed456/cebench/interpretability_eval/husai_scaling_tok30000_layer0_dsae1024_seed456/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae1024_seed456/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 5.862107490301132,
          "independent_score_mean_max": 6.040290415287018,
          "interpretability_score_mean_max": 6.07377854347229,
          "sae_release": "husai_scaling_tok30000_layer0_dsae1024_seed456",
          "sae_id": "custom_sae",
          "date": "2026-02-14 22:14:27",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 5.862107490301132,
          "independent_score_mean_max": 6.040290415287018,
          "interpretability_score_mean_max": 6.07377854347229
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -44.6491999399662,
          "independent_score_mean_max": -44.958975932598115,
          "interpretability_score_mean_max": -41.87783304214477
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae1024_seed456/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae1024_seed456/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae1024_seed456/sae_final.pt",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok30000_layer0_dsae2048_seed42",
      "token_budget": 30000,
      "hook_layer": 0,
      "hook_name": "blocks.0.hook_resid_pre",
      "d_sae": 2048,
      "seed": 42,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-14T22:14:40+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.0.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 30000,
          "d_sae": 2048,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 42,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae2048_seed42/sae_final.pt",
        "data": {
          "num_files_discovered": 113,
          "num_files_used": 16,
          "total_rows": 29432,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/113_movie_sent_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/114_nyc_borough_Manhattan_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/115_nyc_borough_Brooklyn_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/116_nyc_borough_Bronx_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/117_us_state_FL_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/118_us_state_CA_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/119_us_state_TX_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/120_us_timezone_Chicago_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/121_us_timezone_New_York_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/122_us_timezone_Los_Angeles_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/123_world_country_United_Kingdom_blocks.0.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 0.01843550056219101,
          "mse": 0.00048202962716459297,
          "aux": 0.01795347104780376,
          "l0": 32.0,
          "explained_variance": 0.9989387148129208
        }
      },
      "activation_file_count": 113,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-14T22:15:16+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae2048_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer0_dsae2048_seed42 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae2048_seed42/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae2048_seed42/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok30000_layer0_dsae2048_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "9fc5579bf792abcad626a40a2b0b04ebe7112099df37d73746e11dfc23080765",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok30000_layer0_dsae2048_seed42_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6605360072070458,
            "llm_test_auc": 0.6928164663947145,
            "llm_test_f1": 0.6491238078533221
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5917701302212305,
              "test_auc": 0.5965916268977953,
              "test_f1": 0.5062297606159099
            },
            {
              "k": 2,
              "test_accuracy": 0.5960193773385362,
              "test_auc": 0.607571299215313,
              "test_f1": 0.5239699492844394
            },
            {
              "k": 5,
              "test_accuracy": 0.6056306926034268,
              "test_auc": 0.6240643304453132,
              "test_f1": 0.5516114286478107
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6056306926034268,
            "test_auc": 0.6240643304453132,
            "test_f1": 0.5516114286478107
          },
          "best_minus_llm_auc": -0.06875213594940133
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T22:17:40+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae2048_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer0_dsae2048_seed42 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae2048_seed42/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "7719a327b598e5f0bcf48d7ae6489eeeae4c86e24f3f2b16954d98f4a07953ad",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae2048_seed42/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "14c5f86e7c8ff1edffada665973bdfb760f19ea3796db5ac499cbee6ae8b5def",
          "sae_release": "husai_scaling_tok30000_layer0_dsae2048_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae2048_seed42/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae2048_seed42/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae2048_seed42/cebench/interpretability_eval/husai_scaling_tok30000_layer0_dsae2048_seed42/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae2048_seed42/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 7.114756343364715,
          "independent_score_mean_max": 7.556517963409424,
          "interpretability_score_mean_max": 7.378561475276947,
          "sae_release": "husai_scaling_tok30000_layer0_dsae2048_seed42",
          "sae_id": "custom_sae",
          "date": "2026-02-14 22:17:40",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 7.114756343364715,
          "independent_score_mean_max": 7.556517963409424,
          "interpretability_score_mean_max": 7.378561475276947
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -43.39655108690262,
          "independent_score_mean_max": -43.442748384475706,
          "interpretability_score_mean_max": -40.57305011034012
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae2048_seed42/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae2048_seed42/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae2048_seed42/sae_final.pt",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok30000_layer0_dsae2048_seed123",
      "token_budget": 30000,
      "hook_layer": 0,
      "hook_name": "blocks.0.hook_resid_pre",
      "d_sae": 2048,
      "seed": 123,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-14T22:17:52+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.0.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 30000,
          "d_sae": 2048,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 123,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae2048_seed123/sae_final.pt",
        "data": {
          "num_files_discovered": 113,
          "num_files_used": 16,
          "total_rows": 29432,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/113_movie_sent_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/114_nyc_borough_Manhattan_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/115_nyc_borough_Brooklyn_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/116_nyc_borough_Bronx_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/117_us_state_FL_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/118_us_state_CA_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/119_us_state_TX_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/120_us_timezone_Chicago_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/121_us_timezone_New_York_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/122_us_timezone_Los_Angeles_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/123_world_country_United_Kingdom_blocks.0.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 0.018548463005572557,
          "mse": 0.0004816643413505517,
          "aux": 0.018066798569634557,
          "l0": 32.0,
          "explained_variance": 0.9989396033926008
        }
      },
      "activation_file_count": 113,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-14T22:18:27+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae2048_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer0_dsae2048_seed123 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae2048_seed123/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae2048_seed123/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok30000_layer0_dsae2048_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "b8fad3a3cfa14f5e2769b587f3938c16c0d917fdfbd6e50c32f401ad55329fb8",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok30000_layer0_dsae2048_seed123_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6605360072070458,
            "llm_test_auc": 0.6928164663947145,
            "llm_test_f1": 0.6491238078533221
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5853481078052883,
              "test_auc": 0.5971995705147919,
              "test_f1": 0.5091860191433084
            },
            {
              "k": 2,
              "test_accuracy": 0.5880445704635336,
              "test_auc": 0.6004987356799645,
              "test_f1": 0.5137818031907142
            },
            {
              "k": 5,
              "test_accuracy": 0.5985765586010483,
              "test_auc": 0.6122028089610242,
              "test_f1": 0.5500783069860061
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.5985765586010483,
            "test_auc": 0.6122028089610242,
            "test_f1": 0.5500783069860061
          },
          "best_minus_llm_auc": -0.08061365743369031
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T22:20:47+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae2048_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer0_dsae2048_seed123 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae2048_seed123/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "f15b33c79497f1c259ee1e3c692179eb3266396b56e1b766b1b71a535bbf609e",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae2048_seed123/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "cc0a6299dce8761413bd6957b24ef67c00b719e26303ce7749dadb5ead1dbdff",
          "sae_release": "husai_scaling_tok30000_layer0_dsae2048_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae2048_seed123/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae2048_seed123/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae2048_seed123/cebench/interpretability_eval/husai_scaling_tok30000_layer0_dsae2048_seed123/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae2048_seed123/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 7.12040402173996,
          "independent_score_mean_max": 7.925388143062592,
          "interpretability_score_mean_max": 7.48186362028122,
          "sae_release": "husai_scaling_tok30000_layer0_dsae2048_seed123",
          "sae_id": "custom_sae",
          "date": "2026-02-14 22:20:47",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 7.12040402173996,
          "independent_score_mean_max": 7.925388143062592,
          "interpretability_score_mean_max": 7.48186362028122
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -43.39090340852738,
          "independent_score_mean_max": -43.07387820482254,
          "interpretability_score_mean_max": -40.46974796533585
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae2048_seed123/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae2048_seed123/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae2048_seed123/sae_final.pt",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok30000_layer0_dsae2048_seed456",
      "token_budget": 30000,
      "hook_layer": 0,
      "hook_name": "blocks.0.hook_resid_pre",
      "d_sae": 2048,
      "seed": 456,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-14T22:21:00+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.0.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 30000,
          "d_sae": 2048,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 456,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae2048_seed456/sae_final.pt",
        "data": {
          "num_files_discovered": 113,
          "num_files_used": 16,
          "total_rows": 29432,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/113_movie_sent_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/114_nyc_borough_Manhattan_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/115_nyc_borough_Brooklyn_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/116_nyc_borough_Bronx_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/117_us_state_FL_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/118_us_state_CA_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/119_us_state_TX_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/120_us_timezone_Chicago_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/121_us_timezone_New_York_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/122_us_timezone_Los_Angeles_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/123_world_country_United_Kingdom_blocks.0.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 0.018263207282871008,
          "mse": 0.0004765472222061362,
          "aux": 0.017786660231649876,
          "l0": 32.0,
          "explained_variance": 0.9989503501489568
        }
      },
      "activation_file_count": 113,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-14T22:21:37+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae2048_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer0_dsae2048_seed456 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae2048_seed456/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae2048_seed456/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok30000_layer0_dsae2048_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "2b37a6a022888c3911ff709aa31577f6cc0b63a276692304a985d9132e2bef4b",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok30000_layer0_dsae2048_seed456_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6605360072070458,
            "llm_test_auc": 0.6928164663947145,
            "llm_test_f1": 0.6491238078533221
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5879425482834627,
              "test_auc": 0.5918266378854241,
              "test_f1": 0.5269138666245236
            },
            {
              "k": 2,
              "test_accuracy": 0.5944542386905887,
              "test_auc": 0.5990448347294757,
              "test_f1": 0.5367319066780736
            },
            {
              "k": 5,
              "test_accuracy": 0.5989988405356267,
              "test_auc": 0.6021996326386087,
              "test_f1": 0.5354098693741438
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.5989988405356267,
            "test_auc": 0.6021996326386087,
            "test_f1": 0.5354098693741438
          },
          "best_minus_llm_auc": -0.09061683375610585
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T22:24:02+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae2048_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer0_dsae2048_seed456 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae2048_seed456/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "cbc88f7370c8772ac4c21dd669c3a9b258166a22c502c535506a0bfa5f3bccbc",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae2048_seed456/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "f60113041bba4348003a2d2deaac5aa91deb470114a79ceadfc56f3e8593341e",
          "sae_release": "husai_scaling_tok30000_layer0_dsae2048_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae2048_seed456/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae2048_seed456/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae2048_seed456/cebench/interpretability_eval/husai_scaling_tok30000_layer0_dsae2048_seed456/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae2048_seed456/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 6.934480469226838,
          "independent_score_mean_max": 7.731433269977569,
          "interpretability_score_mean_max": 7.233709025382995,
          "sae_release": "husai_scaling_tok30000_layer0_dsae2048_seed456",
          "sae_id": "custom_sae",
          "date": "2026-02-14 22:24:02",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 6.934480469226838,
          "independent_score_mean_max": 7.731433269977569,
          "interpretability_score_mean_max": 7.233709025382995
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -43.576826961040496,
          "independent_score_mean_max": -43.26783307790756,
          "interpretability_score_mean_max": -40.71790256023407
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae2048_seed456/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer0_dsae2048_seed456/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer0_dsae2048_seed456/sae_final.pt",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok30000_layer1_dsae1024_seed42",
      "token_budget": 30000,
      "hook_layer": 1,
      "hook_name": "blocks.1.hook_resid_pre",
      "d_sae": 1024,
      "seed": 42,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-14T22:24:15+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.1.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 30000,
          "d_sae": 1024,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 42,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae1024_seed42/sae_final.pt",
        "data": {
          "num_files_discovered": 16,
          "num_files_used": 16,
          "total_rows": 29432,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/113_movie_sent_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/114_nyc_borough_Manhattan_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/115_nyc_borough_Brooklyn_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/116_nyc_borough_Bronx_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/117_us_state_FL_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/118_us_state_CA_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/119_us_state_TX_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/120_us_timezone_Chicago_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/121_us_timezone_New_York_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/122_us_timezone_Los_Angeles_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/123_world_country_United_Kingdom_blocks.1.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 23.26072907447815,
          "mse": 0.08206475153565407,
          "aux": 23.178664445877075,
          "l0": 32.0,
          "explained_variance": 0.998272660888211
        }
      },
      "activation_file_count": 16,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-14T22:24:47+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae1024_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer1_dsae1024_seed42 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae1024_seed42/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae1024_seed42/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok30000_layer1_dsae1024_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "7623849f5044f5be807f7ccfa0c0c74dc6b88a957241744688b4265a72bfb38f",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok30000_layer1_dsae1024_seed42_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6892726671847929,
            "llm_test_auc": 0.7249531199453926,
            "llm_test_f1": 0.6888365744296719
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5928992115289298,
              "test_auc": 0.6053345489546763,
              "test_f1": 0.5376086426432782
            },
            {
              "k": 2,
              "test_accuracy": 0.5962918122776254,
              "test_auc": 0.6166895951008803,
              "test_f1": 0.5609721113722823
            },
            {
              "k": 5,
              "test_accuracy": 0.6148064252879937,
              "test_auc": 0.6330983889271419,
              "test_f1": 0.5999645810815082
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6148064252879937,
            "test_auc": 0.6330983889271419,
            "test_f1": 0.5999645810815082
          },
          "best_minus_llm_auc": -0.0918547310182507
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T22:27:07+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae1024_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer1_dsae1024_seed42 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae1024_seed42/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "1dbc56b1cd63adbd833df25cdb318f72ba5994c7f36d71aef1a7b2a98e418671",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae1024_seed42/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "ae62f2465ca668a0a01904a1ed102a9729b92acf8355961559775a724ac4fd10",
          "sae_release": "husai_scaling_tok30000_layer1_dsae1024_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae1024_seed42/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae1024_seed42/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae1024_seed42/cebench/interpretability_eval/husai_scaling_tok30000_layer1_dsae1024_seed42/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae1024_seed42/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 8.475596950054168,
          "independent_score_mean_max": 8.995376138687133,
          "interpretability_score_mean_max": 8.10938639640808,
          "sae_release": "husai_scaling_tok30000_layer1_dsae1024_seed42",
          "sae_id": "custom_sae",
          "date": "2026-02-14 22:27:07",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 8.475596950054168,
          "independent_score_mean_max": 8.995376138687133,
          "interpretability_score_mean_max": 8.10938639640808
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -42.03571048021317,
          "independent_score_mean_max": -42.003890209198,
          "interpretability_score_mean_max": -39.84222518920899
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae1024_seed42/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae1024_seed42/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae1024_seed42/sae_final.pt",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok30000_layer1_dsae1024_seed123",
      "token_budget": 30000,
      "hook_layer": 1,
      "hook_name": "blocks.1.hook_resid_pre",
      "d_sae": 1024,
      "seed": 123,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-14T22:27:19+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.1.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 30000,
          "d_sae": 1024,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 123,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae1024_seed123/sae_final.pt",
        "data": {
          "num_files_discovered": 16,
          "num_files_used": 16,
          "total_rows": 29432,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/113_movie_sent_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/114_nyc_borough_Manhattan_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/115_nyc_borough_Brooklyn_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/116_nyc_borough_Bronx_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/117_us_state_FL_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/118_us_state_CA_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/119_us_state_TX_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/120_us_timezone_Chicago_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/121_us_timezone_New_York_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/122_us_timezone_Los_Angeles_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/123_world_country_United_Kingdom_blocks.1.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 22.864981174468994,
          "mse": 0.08093847054988146,
          "aux": 22.784042596817017,
          "l0": 32.0,
          "explained_variance": 0.9982959893654948
        }
      },
      "activation_file_count": 16,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-14T22:27:52+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae1024_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer1_dsae1024_seed123 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae1024_seed123/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae1024_seed123/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok30000_layer1_dsae1024_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "8249fa4260803bec6f0621b87cbb776dc71f03b6a59d72810e80a815be17f14b",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok30000_layer1_dsae1024_seed123_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6892726671847929,
            "llm_test_auc": 0.7249531199453926,
            "llm_test_f1": 0.6888365744296719
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5946997414596433,
              "test_auc": 0.6093530677280196,
              "test_f1": 0.5592797683104771
            },
            {
              "k": 2,
              "test_accuracy": 0.6005921492350107,
              "test_auc": 0.6202095282466353,
              "test_f1": 0.5707644410802721
            },
            {
              "k": 5,
              "test_accuracy": 0.6096729505054878,
              "test_auc": 0.6340908567941328,
              "test_f1": 0.5955278259324764
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6096729505054878,
            "test_auc": 0.6340908567941328,
            "test_f1": 0.5955278259324764
          },
          "best_minus_llm_auc": -0.09086226315125978
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T22:30:02+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae1024_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer1_dsae1024_seed123 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae1024_seed123/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "5818f1e387c102e93feb2548eb1984dd1b03f11da43146dc424d9dbfc8390bd4",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae1024_seed123/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "975388f4081462d2d24a26d7a4d598583a54f298b39916d435b7df6f6bdf9531",
          "sae_release": "husai_scaling_tok30000_layer1_dsae1024_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae1024_seed123/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae1024_seed123/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae1024_seed123/cebench/interpretability_eval/husai_scaling_tok30000_layer1_dsae1024_seed123/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae1024_seed123/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 8.789426472187042,
          "independent_score_mean_max": 9.052521500587464,
          "interpretability_score_mean_max": 8.48562894821167,
          "sae_release": "husai_scaling_tok30000_layer1_dsae1024_seed123",
          "sae_id": "custom_sae",
          "date": "2026-02-14 22:30:02",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 8.789426472187042,
          "independent_score_mean_max": 9.052521500587464,
          "interpretability_score_mean_max": 8.48562894821167
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -41.72188095808029,
          "independent_score_mean_max": -41.946744847297666,
          "interpretability_score_mean_max": -39.465982637405396
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae1024_seed123/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae1024_seed123/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae1024_seed123/sae_final.pt",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok30000_layer1_dsae1024_seed456",
      "token_budget": 30000,
      "hook_layer": 1,
      "hook_name": "blocks.1.hook_resid_pre",
      "d_sae": 1024,
      "seed": 456,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-14T22:30:15+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.1.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 30000,
          "d_sae": 1024,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 456,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae1024_seed456/sae_final.pt",
        "data": {
          "num_files_discovered": 16,
          "num_files_used": 16,
          "total_rows": 29432,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/113_movie_sent_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/114_nyc_borough_Manhattan_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/115_nyc_borough_Brooklyn_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/116_nyc_borough_Bronx_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/117_us_state_FL_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/118_us_state_CA_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/119_us_state_TX_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/120_us_timezone_Chicago_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/121_us_timezone_New_York_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/122_us_timezone_Los_Angeles_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/123_world_country_United_Kingdom_blocks.1.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 23.93584656715393,
          "mse": 0.08592038508504629,
          "aux": 23.849926233291626,
          "l0": 32.0,
          "explained_variance": 0.9981928888569956
        }
      },
      "activation_file_count": 16,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-14T22:30:48+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae1024_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer1_dsae1024_seed456 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae1024_seed456/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae1024_seed456/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok30000_layer1_dsae1024_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "d6b375592442a955ab277d4b16b06bdef705ce1d8f53ff8ab80fee1fd16b8cff",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok30000_layer1_dsae1024_seed456_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6892726671847929,
            "llm_test_auc": 0.7249531199453926,
            "llm_test_f1": 0.6888365744296719
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5978247908704022,
              "test_auc": 0.610535001610099,
              "test_f1": 0.5617475113271754
            },
            {
              "k": 2,
              "test_accuracy": 0.5988035916055388,
              "test_auc": 0.6227141520136907,
              "test_f1": 0.5611541955651869
            },
            {
              "k": 5,
              "test_accuracy": 0.6144484311123015,
              "test_auc": 0.6416989705768275,
              "test_f1": 0.6023428247402924
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6144484311123015,
            "test_auc": 0.6416989705768275,
            "test_f1": 0.6023428247402924
          },
          "best_minus_llm_auc": -0.08325414936856512
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T22:33:05+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae1024_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer1_dsae1024_seed456 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae1024_seed456/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "21e647024a8ce446652a1b3c63717c12eb661ba63600c75885f567cb8c35f9e3",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae1024_seed456/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "3e2f3848f0f8df5d27e5eaff76f5617eda98a4cf9b780d8fc7808edaae11f296",
          "sae_release": "husai_scaling_tok30000_layer1_dsae1024_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae1024_seed456/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae1024_seed456/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae1024_seed456/cebench/interpretability_eval/husai_scaling_tok30000_layer1_dsae1024_seed456/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae1024_seed456/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 8.613990087509155,
          "independent_score_mean_max": 9.176166086196899,
          "interpretability_score_mean_max": 8.4096173620224,
          "sae_release": "husai_scaling_tok30000_layer1_dsae1024_seed456",
          "sae_id": "custom_sae",
          "date": "2026-02-14 22:33:05",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 8.613990087509155,
          "independent_score_mean_max": 9.176166086196899,
          "interpretability_score_mean_max": 8.4096173620224
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -41.897317342758186,
          "independent_score_mean_max": -41.82310026168823,
          "interpretability_score_mean_max": -39.54199422359466
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae1024_seed456/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae1024_seed456/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae1024_seed456/sae_final.pt",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok30000_layer1_dsae2048_seed42",
      "token_budget": 30000,
      "hook_layer": 1,
      "hook_name": "blocks.1.hook_resid_pre",
      "d_sae": 2048,
      "seed": 42,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-14T22:33:18+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.1.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 30000,
          "d_sae": 2048,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 42,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae2048_seed42/sae_final.pt",
        "data": {
          "num_files_discovered": 16,
          "num_files_used": 16,
          "total_rows": 29432,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/113_movie_sent_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/114_nyc_borough_Manhattan_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/115_nyc_borough_Brooklyn_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/116_nyc_borough_Bronx_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/117_us_state_FL_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/118_us_state_CA_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/119_us_state_TX_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/120_us_timezone_Chicago_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/121_us_timezone_New_York_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/122_us_timezone_Los_Angeles_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/123_world_country_United_Kingdom_blocks.1.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 25.0125412940979,
          "mse": 0.0762296924367547,
          "aux": 24.93631148338318,
          "l0": 32.0,
          "explained_variance": 0.9983954800719961
        }
      },
      "activation_file_count": 16,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-14T22:33:51+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae2048_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer1_dsae2048_seed42 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae2048_seed42/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae2048_seed42/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok30000_layer1_dsae2048_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "d2668b42f401133bc61fd9d1a479fbea8c89fe70dbc617848df4004c0f710d01",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok30000_layer1_dsae2048_seed42_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6892726671847929,
            "llm_test_auc": 0.7249531199453926,
            "llm_test_f1": 0.6888365744296719
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5815366763566039,
              "test_auc": 0.5964915216637288,
              "test_f1": 0.5302673336345135
            },
            {
              "k": 2,
              "test_accuracy": 0.583767475799262,
              "test_auc": 0.602648481204297,
              "test_f1": 0.5386672207659978
            },
            {
              "k": 5,
              "test_accuracy": 0.606391767279692,
              "test_auc": 0.629366816923878,
              "test_f1": 0.5832864755774729
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.606391767279692,
            "test_auc": 0.629366816923878,
            "test_f1": 0.5832864755774729
          },
          "best_minus_llm_auc": -0.09558630302151461
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T22:36:13+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae2048_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer1_dsae2048_seed42 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae2048_seed42/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "6451cc17c25d6a5678a8d8e944a4cba49b469d796f4dd5bae9e847f177afc94c",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae2048_seed42/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "053b72fbd072223138be2a160e1c515eea1f9990f9e83dfb01ad35f85625d1ce",
          "sae_release": "husai_scaling_tok30000_layer1_dsae2048_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae2048_seed42/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae2048_seed42/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae2048_seed42/cebench/interpretability_eval/husai_scaling_tok30000_layer1_dsae2048_seed42/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae2048_seed42/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 10.57554366350174,
          "independent_score_mean_max": 11.559704933166504,
          "interpretability_score_mean_max": 10.552940266132355,
          "sae_release": "husai_scaling_tok30000_layer1_dsae2048_seed42",
          "sae_id": "custom_sae",
          "date": "2026-02-14 22:36:13",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 10.57554366350174,
          "independent_score_mean_max": 11.559704933166504,
          "interpretability_score_mean_max": 10.552940266132355
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -39.9357637667656,
          "independent_score_mean_max": -39.439561414718625,
          "interpretability_score_mean_max": -37.39867131948471
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae2048_seed42/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae2048_seed42/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae2048_seed42/sae_final.pt",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok30000_layer1_dsae2048_seed123",
      "token_budget": 30000,
      "hook_layer": 1,
      "hook_name": "blocks.1.hook_resid_pre",
      "d_sae": 2048,
      "seed": 123,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-14T22:36:27+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.1.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 30000,
          "d_sae": 2048,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 123,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae2048_seed123/sae_final.pt",
        "data": {
          "num_files_discovered": 16,
          "num_files_used": 16,
          "total_rows": 29432,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/113_movie_sent_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/114_nyc_borough_Manhattan_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/115_nyc_borough_Brooklyn_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/116_nyc_borough_Bronx_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/117_us_state_FL_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/118_us_state_CA_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/119_us_state_TX_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/120_us_timezone_Chicago_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/121_us_timezone_New_York_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/122_us_timezone_Los_Angeles_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/123_world_country_United_Kingdom_blocks.1.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 25.16940712928772,
          "mse": 0.0759917525574565,
          "aux": 25.09341549873352,
          "l0": 32.0,
          "explained_variance": 0.9984001334147674
        }
      },
      "activation_file_count": 16,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-14T22:37:00+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae2048_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer1_dsae2048_seed123 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae2048_seed123/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae2048_seed123/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok30000_layer1_dsae2048_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "07aa97e2a475e12391d27d39027ad48ba51c5c2e316914917859304204fba450",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok30000_layer1_dsae2048_seed123_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6892726671847929,
            "llm_test_auc": 0.7249531199453926,
            "llm_test_f1": 0.6888365744296719
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5903520764225163,
              "test_auc": 0.5998283166460755,
              "test_f1": 0.5483052332854206
            },
            {
              "k": 2,
              "test_accuracy": 0.5935353183574069,
              "test_auc": 0.6104453717338384,
              "test_f1": 0.5563485862383424
            },
            {
              "k": 5,
              "test_accuracy": 0.6021654189195628,
              "test_auc": 0.6302437785422621,
              "test_f1": 0.5833583091463332
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6021654189195628,
            "test_auc": 0.6302437785422621,
            "test_f1": 0.5833583091463332
          },
          "best_minus_llm_auc": -0.09470934140313048
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T22:39:23+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae2048_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer1_dsae2048_seed123 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae2048_seed123/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "fade2b0b1dd0bf191020f3de983e242336f343eb71b0ec32bce3faf033c43a11",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae2048_seed123/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "fd2e592b1559caf22f6ddbca3e1f5fd64237de130f659ea0e5bca6cf8cf7a56f",
          "sae_release": "husai_scaling_tok30000_layer1_dsae2048_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae2048_seed123/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae2048_seed123/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae2048_seed123/cebench/interpretability_eval/husai_scaling_tok30000_layer1_dsae2048_seed123/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae2048_seed123/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 10.810209999084472,
          "independent_score_mean_max": 11.617531554698944,
          "interpretability_score_mean_max": 10.472996156215668,
          "sae_release": "husai_scaling_tok30000_layer1_dsae2048_seed123",
          "sae_id": "custom_sae",
          "date": "2026-02-14 22:39:23",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 10.810209999084472,
          "independent_score_mean_max": 11.617531554698944,
          "interpretability_score_mean_max": 10.472996156215668
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -39.701097431182866,
          "independent_score_mean_max": -39.38173479318619,
          "interpretability_score_mean_max": -37.4786154294014
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae2048_seed123/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae2048_seed123/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae2048_seed123/sae_final.pt",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok30000_layer1_dsae2048_seed456",
      "token_budget": 30000,
      "hook_layer": 1,
      "hook_name": "blocks.1.hook_resid_pre",
      "d_sae": 2048,
      "seed": 456,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-14T22:39:36+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.1.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 30000,
          "d_sae": 2048,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 456,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae2048_seed456/sae_final.pt",
        "data": {
          "num_files_discovered": 16,
          "num_files_used": 16,
          "total_rows": 29432,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/113_movie_sent_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/114_nyc_borough_Manhattan_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/115_nyc_borough_Brooklyn_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/116_nyc_borough_Bronx_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/117_us_state_FL_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/118_us_state_CA_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/119_us_state_TX_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/120_us_timezone_Chicago_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/121_us_timezone_New_York_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/122_us_timezone_Los_Angeles_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/123_world_country_United_Kingdom_blocks.1.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 25.053161144256592,
          "mse": 0.0764489434659481,
          "aux": 24.976712226867676,
          "l0": 32.0,
          "explained_variance": 0.9983920959214571
        }
      },
      "activation_file_count": 16,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-14T22:40:10+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae2048_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer1_dsae2048_seed456 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae2048_seed456/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae2048_seed456/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok30000_layer1_dsae2048_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "e95086a07da7f5fa0f4716f372bef8cde0839203df0413fdc048ba3742cc47a4",
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok30000_layer1_dsae2048_seed456_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6892726671847929,
            "llm_test_auc": 0.7249531199453926,
            "llm_test_f1": 0.6888365744296719
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5925619807766261,
              "test_auc": 0.6056496069221368,
              "test_f1": 0.5459900533663105
            },
            {
              "k": 2,
              "test_accuracy": 0.5984210712552194,
              "test_auc": 0.6183308386928116,
              "test_f1": 0.5721518989023663
            },
            {
              "k": 5,
              "test_accuracy": 0.6068469104416733,
              "test_auc": 0.6308327485164374,
              "test_f1": 0.5781166001821632
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6068469104416733,
            "test_auc": 0.6308327485164374,
            "test_f1": 0.5781166001821632
          },
          "best_minus_llm_auc": -0.09412037142895513
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-14T22:42:32+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae2048_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer1_dsae2048_seed456 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae2048_seed456/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "61868e4b87fdbaaeeee975d5392e683bb6c7f3e53d4db97cd1038e98f39d3773",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae2048_seed456/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "340eff87801eb327d751b3bf9b49cdb9925b1531ebb342cd0263ef1fab8138a5",
          "sae_release": "husai_scaling_tok30000_layer1_dsae2048_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae2048_seed456/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae2048_seed456/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae2048_seed456/cebench/interpretability_eval/husai_scaling_tok30000_layer1_dsae2048_seed456/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae2048_seed456/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 9.5802614569664,
          "independent_score_mean_max": 10.785375874042511,
          "interpretability_score_mean_max": 9.53832999944687,
          "sae_release": "husai_scaling_tok30000_layer1_dsae2048_seed456",
          "sae_id": "custom_sae",
          "date": "2026-02-14 22:42:32",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 9.5802614569664,
          "independent_score_mean_max": 10.785375874042511,
          "interpretability_score_mean_max": 9.53832999944687
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -40.93104597330094,
          "independent_score_mean_max": -40.213890473842625,
          "interpretability_score_mean_max": -38.4132815861702
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae2048_seed456/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/external_eval/tok30000_layer1_dsae2048_seed456/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260214T212435Z/checkpoints/tok30000_layer1_dsae2048_seed456/sae_final.pt",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    }
  ],
  "aggregates": {
    "by_token_budget": {
      "10000": {
        "saebench_best_minus_llm_auc": {
          "mean": -0.08629052824435675,
          "std": 0.012793609002197916,
          "min": -0.10464225163078833,
          "max": -0.06509338803900744,
          "n": 12
        },
        "cebench_interpretability_max": {
          "mean": 7.862272895475228,
          "std": 1.4915234425128283,
          "min": 6.017686002254486,
          "max": 10.38789572238922,
          "n": 12
        },
        "cebench_interp_delta_vs_baseline": {
          "mean": -40.08933869014184,
          "std": 1.4915234425128288,
          "min": -41.93392558336258,
          "max": -37.56371586322784,
          "n": 12
        }
      },
      "30000": {
        "saebench_best_minus_llm_auc": {
          "mean": -0.08513235397555448,
          "std": 0.009275035438460156,
          "min": -0.09558630302151461,
          "max": -0.06875213594940133,
          "n": 12
        },
        "cebench_interpretability_max": {
          "mean": 8.026932807564735,
          "std": 1.544148448289346,
          "min": 6.07377854347229,
          "max": 10.552940266132355,
          "n": 12
        },
        "cebench_interp_delta_vs_baseline": {
          "mean": -39.92467877805234,
          "std": 1.5441484482893448,
          "min": -41.87783304214477,
          "max": -37.39867131948471,
          "n": 12
        }
      }
    },
    "by_hook_layer": {
      "0": {
        "saebench_best_minus_llm_auc": {
          "mean": -0.07742713722520761,
          "std": 0.008628240396075121,
          "min": -0.09370917118906608,
          "max": -0.06509338803900744,
          "n": 12
        },
        "cebench_interpretability_max": {
          "mean": 6.749414227704207,
          "std": 0.6439175697142968,
          "min": 6.017686002254486,
          "max": 7.5051356482505795,
          "n": 12
        },
        "cebench_interp_delta_vs_baseline": {
          "mean": -41.20219735791286,
          "std": 0.643917569714296,
          "min": -41.93392558336258,
          "max": -40.446475937366486,
          "n": 12
        }
      },
      "1": {
        "saebench_best_minus_llm_auc": {
          "mean": -0.0939957449947036,
          "std": 0.005123507500565954,
          "min": -0.10464225163078833,
          "max": -0.08325414936856512,
          "n": 12
        },
        "cebench_interpretability_max": {
          "mean": 9.139791475335757,
          "std": 1.045237315836624,
          "min": 7.965026619434357,
          "max": 10.552940266132355,
          "n": 12
        },
        "cebench_interp_delta_vs_baseline": {
          "mean": -38.81182011028131,
          "std": 1.0452373158366242,
          "min": -39.98658496618271,
          "max": -37.39867131948471,
          "n": 12
        }
      }
    },
    "by_d_sae": {
      "1024": {
        "saebench_best_minus_llm_auc": {
          "mean": -0.08212220067594521,
          "std": 0.010149011936029009,
          "min": -0.09526819738578596,
          "max": -0.06509338803900744,
          "n": 12
        },
        "cebench_interpretability_max": {
          "mean": 7.1673101575175915,
          "std": 1.0807816623762538,
          "min": 6.017686002254486,
          "max": 8.48562894821167,
          "n": 12
        },
        "cebench_interp_delta_vs_baseline": {
          "mean": -40.784301428099475,
          "std": 1.080781662376253,
          "min": -41.93392558336258,
          "max": -39.465982637405396,
          "n": 12
        }
      },
      "2048": {
        "saebench_best_minus_llm_auc": {
          "mean": -0.08930068154396602,
          "std": 0.01092356774416891,
          "min": -0.10464225163078833,
          "max": -0.06875213594940133,
          "n": 12
        },
        "cebench_interpretability_max": {
          "mean": 8.721895545522372,
          "std": 1.4620249543451833,
          "min": 7.224240756034851,
          "max": 10.552940266132355,
          "n": 12
        },
        "cebench_interp_delta_vs_baseline": {
          "mean": -39.2297160400947,
          "std": 1.4620249543451835,
          "min": -40.72737082958221,
          "max": -37.39867131948471,
          "n": 12
        }
      }
    }
  }
}
