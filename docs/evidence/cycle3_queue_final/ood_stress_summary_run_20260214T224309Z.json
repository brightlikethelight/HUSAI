{
  "run_metadata": {
    "timestamp_utc": "2026-02-14T22:50:28+00:00",
    "git_commit": "511c56c059a37db98375f00166e56ff9030bfab2",
    "command": "python scripts/experiments/run_ood_stress_eval.py --checkpoint results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed42/sae_final.pt --architecture relu --sae-release husai_cycle3_ood --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_ood_cycle3 --model-cache-path /tmp/sae_bench_model_cache --output-dir results/experiments/phase4e_ood_stress_b200 --force-rerun",
    "config_hash": "8db32fc750f5cee0f2814c3b9c1eeeabe141ae1e347c449af49f6c641344b348",
    "run_id": "run_20260214T224309Z"
  },
  "config": {
    "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed42/sae_final.pt",
    "architecture": "relu",
    "sae_release": "husai_cycle3_ood",
    "model_name": "pythia-70m-deduped",
    "hook_layer": 0,
    "hook_name": "blocks.0.hook_resid_pre",
    "reg_type": "l1",
    "setting": "normal",
    "ks": "1,2,5",
    "device": "cuda",
    "dtype": "float32",
    "results_path": "/tmp/husai_saebench_probe_results_ood_cycle3",
    "model_cache_path": "/tmp/sae_bench_model_cache",
    "output_dir": "/workspace/HUSAI/results/experiments/phase4e_ood_stress_b200",
    "id_dataset_count": 81,
    "ood_dataset_count": 32,
    "ood_domains": [
      "social-security",
      "spam",
      "temp",
      "third-party",
      "toxic",
      "trial-court",
      "truthqa",
      "twt",
      "us",
      "virtue",
      "wikidata",
      "wikidatais",
      "wikidatapolitical",
      "wikidatasex",
      "world"
    ]
  },
  "datasets": {
    "id": [
      "100_news_fake",
      "105_click_bait",
      "106_hate_hate",
      "107_hate_offensive",
      "110_aimade_humangpt3",
      "113_movie_sent",
      "114_nyc_borough_Manhattan",
      "115_nyc_borough_Brooklyn",
      "116_nyc_borough_Bronx",
      "126_art_type_book",
      "127_art_type_song",
      "128_art_type_movie",
      "129_arith_mc_A",
      "133_context_type_Causality",
      "134_context_type_Belief_states",
      "135_context_type_Event_duration",
      "136_glue_mnli_entailment",
      "137_glue_mnli_neutral",
      "138_glue_mnli_contradiction",
      "139_news_class_Politics",
      "140_news_class_Technology",
      "141_news_class_Entertainment",
      "142_cancer_cat_Thyroid_Cancer",
      "143_cancer_cat_Lung_Cancer",
      "144_cancer_cat_Colon_Cancer",
      "145_disease_class_digestive system diseases",
      "146_disease_class_cardiovascular diseases",
      "147_disease_class_nervous system diseases",
      "151_it_tick_HR Support",
      "152_it_tick_Hardware",
      "153_it_tick_Administrative rights",
      "154_athlete_sport_football",
      "155_athlete_sport_basketball",
      "156_athlete_sport_baseball",
      "157_amazon_5star",
      "158_code_C",
      "159_code_Python",
      "160_code_HTML",
      "161_agnews_0",
      "162_agnews_1",
      "163_agnews_2",
      "21_headline_istrump",
      "22_headline_isobama",
      "23_headline_ischina",
      "24_headline_isiran",
      "26_headline_isfrontpage",
      "36_sciq_tf",
      "44_phys_tf",
      "47_reasoning_tf",
      "48_cm_correct",
      "49_cm_isshort",
      "50_deon_isvalid",
      "51_just_is",
      "54_cs_tf",
      "5_hist_fig_ismale",
      "65_high-school",
      "66_living-room",
      "68_credit-card",
      "69_blood-pressure",
      "6_hist_fig_isamerican",
      "70_prime-factors",
      "71_social-media",
      "72_gene-expression",
      "73_control-group",
      "74_magnetic-field",
      "75_cell-lines",
      "77_second-derivative",
      "78_north-america",
      "79_human-rights",
      "7_hist_fig_ispolitician",
      "80_side-effects",
      "81_public-health",
      "82_federal-government",
      "84_clinical-trials",
      "85_mental-health",
      "87_glue_cola",
      "89_glue_mrpc",
      "90_glue_qnli",
      "91_glue_qqp",
      "92_glue_sst2",
      "94_ai_gen"
    ],
    "ood": [
      "117_us_state_FL",
      "118_us_state_CA",
      "119_us_state_TX",
      "120_us_timezone_Chicago",
      "121_us_timezone_New_York",
      "122_us_timezone_Los_Angeles",
      "123_world_country_United_Kingdom",
      "124_world_country_United_States",
      "125_world_country_Italy",
      "130_temp_cat_Frequency",
      "131_temp_cat_Typical Time",
      "132_temp_cat_Event Ordering",
      "148_twt_emotion_worry",
      "149_twt_emotion_happiness",
      "150_twt_emotion_sadness",
      "41_truthqa_tf",
      "42_temp_sense",
      "52_virtue_is",
      "56_wikidatasex_or_gender",
      "57_wikidatais_alive",
      "58_wikidatapolitical_party",
      "59_wikidata_occupation_isjournalist",
      "60_wikidata_occupation_isathlete",
      "61_wikidata_occupation_isactor",
      "62_wikidata_occupation_ispolitician",
      "63_wikidata_occupation_issinger",
      "64_wikidata_occupation_isresearcher",
      "67_social-security",
      "76_trial-court",
      "83_third-party",
      "95_toxic_is",
      "96_spam_is"
    ],
    "ood_domains": [
      "social-security",
      "spam",
      "temp",
      "third-party",
      "toxic",
      "trial-court",
      "truthqa",
      "twt",
      "us",
      "virtue",
      "wikidata",
      "wikidatais",
      "wikidatapolitical",
      "wikidatasex",
      "world"
    ],
    "inferred_total_count": 113
  },
  "id_result": {
    "label": "id",
    "returncode": 0,
    "dataset_count": 81,
    "datasets": [
      "100_news_fake",
      "105_click_bait",
      "106_hate_hate",
      "107_hate_offensive",
      "110_aimade_humangpt3",
      "113_movie_sent",
      "114_nyc_borough_Manhattan",
      "115_nyc_borough_Brooklyn",
      "116_nyc_borough_Bronx",
      "126_art_type_book",
      "127_art_type_song",
      "128_art_type_movie",
      "129_arith_mc_A",
      "133_context_type_Causality",
      "134_context_type_Belief_states",
      "135_context_type_Event_duration",
      "136_glue_mnli_entailment",
      "137_glue_mnli_neutral",
      "138_glue_mnli_contradiction",
      "139_news_class_Politics",
      "140_news_class_Technology",
      "141_news_class_Entertainment",
      "142_cancer_cat_Thyroid_Cancer",
      "143_cancer_cat_Lung_Cancer",
      "144_cancer_cat_Colon_Cancer",
      "145_disease_class_digestive system diseases",
      "146_disease_class_cardiovascular diseases",
      "147_disease_class_nervous system diseases",
      "151_it_tick_HR Support",
      "152_it_tick_Hardware",
      "153_it_tick_Administrative rights",
      "154_athlete_sport_football",
      "155_athlete_sport_basketball",
      "156_athlete_sport_baseball",
      "157_amazon_5star",
      "158_code_C",
      "159_code_Python",
      "160_code_HTML",
      "161_agnews_0",
      "162_agnews_1",
      "163_agnews_2",
      "21_headline_istrump",
      "22_headline_isobama",
      "23_headline_ischina",
      "24_headline_isiran",
      "26_headline_isfrontpage",
      "36_sciq_tf",
      "44_phys_tf",
      "47_reasoning_tf",
      "48_cm_correct",
      "49_cm_isshort",
      "50_deon_isvalid",
      "51_just_is",
      "54_cs_tf",
      "5_hist_fig_ismale",
      "65_high-school",
      "66_living-room",
      "68_credit-card",
      "69_blood-pressure",
      "6_hist_fig_isamerican",
      "70_prime-factors",
      "71_social-media",
      "72_gene-expression",
      "73_control-group",
      "74_magnetic-field",
      "75_cell-lines",
      "77_second-derivative",
      "78_north-america",
      "79_human-rights",
      "7_hist_fig_ispolitician",
      "80_side-effects",
      "81_public-health",
      "82_federal-government",
      "84_clinical-trials",
      "85_mental-health",
      "87_glue_cola",
      "89_glue_mrpc",
      "90_glue_qnli",
      "91_glue_qqp",
      "92_glue_sst2",
      "94_ai_gen"
    ],
    "output_dir": "results/experiments/phase4e_ood_stress_b200/run_20260214T224309Z/id",
    "log_path": "results/experiments/phase4e_ood_stress_b200/run_20260214T224309Z/logs/id.log",
    "summary_path": "results/experiments/phase4e_ood_stress_b200/run_20260214T224309Z/id/husai_custom_sae_summary.json",
    "best_minus_llm_auc": -0.028988677631220372,
    "summary": {
      "timestamp_utc": "2026-02-14T22:48:15+00:00",
      "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed42/sae_final.pt --sae-release husai_cycle3_ood_id --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --reg-type l1 --setting normal --ks 1,2,5 --dataset-names '100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,126_art_type_book,127_art_type_song,128_art_type_movie,129_arith_mc_A,133_context_type_Causality,134_context_type_Belief_states,135_context_type_Event_duration,136_glue_mnli_entailment,137_glue_mnli_neutral,138_glue_mnli_contradiction,139_news_class_Politics,140_news_class_Technology,141_news_class_Entertainment,142_cancer_cat_Thyroid_Cancer,143_cancer_cat_Lung_Cancer,144_cancer_cat_Colon_Cancer,145_disease_class_digestive system diseases,146_disease_class_cardiovascular diseases,147_disease_class_nervous system diseases,151_it_tick_HR Support,152_it_tick_Hardware,153_it_tick_Administrative rights,154_athlete_sport_football,155_athlete_sport_basketball,156_athlete_sport_baseball,157_amazon_5star,158_code_C,159_code_Python,160_code_HTML,161_agnews_0,162_agnews_1,163_agnews_2,21_headline_istrump,22_headline_isobama,23_headline_ischina,24_headline_isiran,26_headline_isfrontpage,36_sciq_tf,44_phys_tf,47_reasoning_tf,48_cm_correct,49_cm_isshort,50_deon_isvalid,51_just_is,54_cs_tf,5_hist_fig_ismale,65_high-school,66_living-room,68_credit-card,69_blood-pressure,6_hist_fig_isamerican,70_prime-factors,71_social-media,72_gene-expression,73_control-group,74_magnetic-field,75_cell-lines,77_second-derivative,78_north-america,79_human-rights,7_hist_fig_ispolitician,80_side-effects,81_public-health,82_federal-government,84_clinical-trials,85_mental-health,87_glue_cola,89_glue_mrpc,90_glue_qnli,91_glue_qqp,92_glue_sst2,94_ai_gen' --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_ood_cycle3 --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_ood_stress_b200/run_20260214T224309Z/id --architecture relu --force-rerun",
      "config": {
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed42/sae_final.pt",
        "architecture_override": "relu",
        "sae_release": "husai_cycle3_ood_id",
        "model_name": "pythia-70m-deduped",
        "hook_layer": 0,
        "hook_name": "blocks.0.hook_resid_pre",
        "reg_type": "l1",
        "setting": "normal",
        "ks": [
          1,
          2,
          5
        ],
        "dataset_names": [
          "100_news_fake",
          "105_click_bait",
          "106_hate_hate",
          "107_hate_offensive",
          "110_aimade_humangpt3",
          "113_movie_sent",
          "114_nyc_borough_Manhattan",
          "115_nyc_borough_Brooklyn",
          "116_nyc_borough_Bronx",
          "126_art_type_book",
          "127_art_type_song",
          "128_art_type_movie",
          "129_arith_mc_A",
          "133_context_type_Causality",
          "134_context_type_Belief_states",
          "135_context_type_Event_duration",
          "136_glue_mnli_entailment",
          "137_glue_mnli_neutral",
          "138_glue_mnli_contradiction",
          "139_news_class_Politics",
          "140_news_class_Technology",
          "141_news_class_Entertainment",
          "142_cancer_cat_Thyroid_Cancer",
          "143_cancer_cat_Lung_Cancer",
          "144_cancer_cat_Colon_Cancer",
          "145_disease_class_digestive system diseases",
          "146_disease_class_cardiovascular diseases",
          "147_disease_class_nervous system diseases",
          "151_it_tick_HR Support",
          "152_it_tick_Hardware",
          "153_it_tick_Administrative rights",
          "154_athlete_sport_football",
          "155_athlete_sport_basketball",
          "156_athlete_sport_baseball",
          "157_amazon_5star",
          "158_code_C",
          "159_code_Python",
          "160_code_HTML",
          "161_agnews_0",
          "162_agnews_1",
          "163_agnews_2",
          "21_headline_istrump",
          "22_headline_isobama",
          "23_headline_ischina",
          "24_headline_isiran",
          "26_headline_isfrontpage",
          "36_sciq_tf",
          "44_phys_tf",
          "47_reasoning_tf",
          "48_cm_correct",
          "49_cm_isshort",
          "50_deon_isvalid",
          "51_just_is",
          "54_cs_tf",
          "5_hist_fig_ismale",
          "65_high-school",
          "66_living-room",
          "68_credit-card",
          "69_blood-pressure",
          "6_hist_fig_isamerican",
          "70_prime-factors",
          "71_social-media",
          "72_gene-expression",
          "73_control-group",
          "74_magnetic-field",
          "75_cell-lines",
          "77_second-derivative",
          "78_north-america",
          "79_human-rights",
          "7_hist_fig_ispolitician",
          "80_side-effects",
          "81_public-health",
          "82_federal-government",
          "84_clinical-trials",
          "85_mental-health",
          "87_glue_cola",
          "89_glue_mrpc",
          "90_glue_qnli",
          "91_glue_qqp",
          "92_glue_sst2",
          "94_ai_gen"
        ],
        "dataset_names_inferred_from_cache": false,
        "dataset_count": 81,
        "binarize": false,
        "device": "cuda",
        "dtype": "float32",
        "results_path": "/tmp/husai_saebench_probe_results_ood_cycle3",
        "model_cache_path": "/tmp/sae_bench_model_cache",
        "force_rerun": true
      },
      "config_hash": "7d1bf3ce6e4720e0734d7d14b92ff6cbefd87f53e1fcb8cf82638d2b7b672ff9",
      "sae_meta": {
        "architecture": "relu",
        "d_model": 512,
        "d_sae": 1024,
        "k": null
      },
      "summary": {
        "result_key": "husai_cycle3_ood_id_custom_sae",
        "llm_metrics": {
          "llm_test_accuracy": 0.6484440093938268,
          "llm_test_auc": 0.6685808578683783,
          "llm_test_f1": 0.6109485111283821
        },
        "sae_metrics_by_k": [
          {
            "k": 1,
            "test_accuracy": 0.6031160897485629,
            "test_auc": 0.603248220225644,
            "test_f1": 0.5475309668112919
          },
          {
            "k": 2,
            "test_accuracy": 0.6164259233825695,
            "test_auc": 0.6279482841888266,
            "test_f1": 0.564124824556756
          },
          {
            "k": 5,
            "test_accuracy": 0.6253010814712335,
            "test_auc": 0.6395921802371579,
            "test_f1": 0.5777874923960404
          }
        ],
        "best_by_auc": {
          "k": 5,
          "test_accuracy": 0.6253010814712335,
          "test_auc": 0.6395921802371579,
          "test_f1": 0.5777874923960404
        },
        "best_minus_llm_auc": -0.028988677631220372
      }
    }
  },
  "ood_result": {
    "label": "ood",
    "returncode": 0,
    "dataset_count": 32,
    "datasets": [
      "117_us_state_FL",
      "118_us_state_CA",
      "119_us_state_TX",
      "120_us_timezone_Chicago",
      "121_us_timezone_New_York",
      "122_us_timezone_Los_Angeles",
      "123_world_country_United_Kingdom",
      "124_world_country_United_States",
      "125_world_country_Italy",
      "130_temp_cat_Frequency",
      "131_temp_cat_Typical Time",
      "132_temp_cat_Event Ordering",
      "148_twt_emotion_worry",
      "149_twt_emotion_happiness",
      "150_twt_emotion_sadness",
      "41_truthqa_tf",
      "42_temp_sense",
      "52_virtue_is",
      "56_wikidatasex_or_gender",
      "57_wikidatais_alive",
      "58_wikidatapolitical_party",
      "59_wikidata_occupation_isjournalist",
      "60_wikidata_occupation_isathlete",
      "61_wikidata_occupation_isactor",
      "62_wikidata_occupation_ispolitician",
      "63_wikidata_occupation_issinger",
      "64_wikidata_occupation_isresearcher",
      "67_social-security",
      "76_trial-court",
      "83_third-party",
      "95_toxic_is",
      "96_spam_is"
    ],
    "output_dir": "results/experiments/phase4e_ood_stress_b200/run_20260214T224309Z/ood",
    "log_path": "results/experiments/phase4e_ood_stress_b200/run_20260214T224309Z/logs/ood.log",
    "summary_path": "results/experiments/phase4e_ood_stress_b200/run_20260214T224309Z/ood/husai_custom_sae_summary.json",
    "best_minus_llm_auc": -0.0434427392464225,
    "summary": {
      "timestamp_utc": "2026-02-14T22:50:26+00:00",
      "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed42/sae_final.pt --sae-release husai_cycle3_ood_ood --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --reg-type l1 --setting normal --ks 1,2,5 --dataset-names '117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom,124_world_country_United_States,125_world_country_Italy,130_temp_cat_Frequency,131_temp_cat_Typical Time,132_temp_cat_Event Ordering,148_twt_emotion_worry,149_twt_emotion_happiness,150_twt_emotion_sadness,41_truthqa_tf,42_temp_sense,52_virtue_is,56_wikidatasex_or_gender,57_wikidatais_alive,58_wikidatapolitical_party,59_wikidata_occupation_isjournalist,60_wikidata_occupation_isathlete,61_wikidata_occupation_isactor,62_wikidata_occupation_ispolitician,63_wikidata_occupation_issinger,64_wikidata_occupation_isresearcher,67_social-security,76_trial-court,83_third-party,95_toxic_is,96_spam_is' --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_ood_cycle3 --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_ood_stress_b200/run_20260214T224309Z/ood --architecture relu --force-rerun",
      "config": {
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4b_architecture_frontier_external_multiseed/run_20260214T202538Z/checkpoints/relu_seed42/sae_final.pt",
        "architecture_override": "relu",
        "sae_release": "husai_cycle3_ood_ood",
        "model_name": "pythia-70m-deduped",
        "hook_layer": 0,
        "hook_name": "blocks.0.hook_resid_pre",
        "reg_type": "l1",
        "setting": "normal",
        "ks": [
          1,
          2,
          5
        ],
        "dataset_names": [
          "117_us_state_FL",
          "118_us_state_CA",
          "119_us_state_TX",
          "120_us_timezone_Chicago",
          "121_us_timezone_New_York",
          "122_us_timezone_Los_Angeles",
          "123_world_country_United_Kingdom",
          "124_world_country_United_States",
          "125_world_country_Italy",
          "130_temp_cat_Frequency",
          "131_temp_cat_Typical Time",
          "132_temp_cat_Event Ordering",
          "148_twt_emotion_worry",
          "149_twt_emotion_happiness",
          "150_twt_emotion_sadness",
          "41_truthqa_tf",
          "42_temp_sense",
          "52_virtue_is",
          "56_wikidatasex_or_gender",
          "57_wikidatais_alive",
          "58_wikidatapolitical_party",
          "59_wikidata_occupation_isjournalist",
          "60_wikidata_occupation_isathlete",
          "61_wikidata_occupation_isactor",
          "62_wikidata_occupation_ispolitician",
          "63_wikidata_occupation_issinger",
          "64_wikidata_occupation_isresearcher",
          "67_social-security",
          "76_trial-court",
          "83_third-party",
          "95_toxic_is",
          "96_spam_is"
        ],
        "dataset_names_inferred_from_cache": false,
        "dataset_count": 32,
        "binarize": false,
        "device": "cuda",
        "dtype": "float32",
        "results_path": "/tmp/husai_saebench_probe_results_ood_cycle3",
        "model_cache_path": "/tmp/sae_bench_model_cache",
        "force_rerun": true
      },
      "config_hash": "2de7583a3cd9b4f4388554fa8b2aa81fdeeb34084603bf810f2b75166b3e26bf",
      "sae_meta": {
        "architecture": "relu",
        "d_model": 512,
        "d_sae": 1024,
        "k": null
      },
      "summary": {
        "result_key": "husai_cycle3_ood_ood_custom_sae",
        "llm_metrics": {
          "llm_test_accuracy": 0.6495039546512676,
          "llm_test_auc": 0.6744019001622025,
          "llm_test_f1": 0.617120103509151
        },
        "sae_metrics_by_k": [
          {
            "k": 1,
            "test_accuracy": 0.5706404576526255,
            "test_auc": 0.5943051391028205,
            "test_f1": 0.547748519124227
          },
          {
            "k": 2,
            "test_accuracy": 0.5808590229382709,
            "test_auc": 0.609400948019669,
            "test_f1": 0.5563297591328136
          },
          {
            "k": 5,
            "test_accuracy": 0.6014360414544735,
            "test_auc": 0.63095916091578,
            "test_f1": 0.582236693411708
          }
        ],
        "best_by_auc": {
          "k": 5,
          "test_accuracy": 0.6014360414544735,
          "test_auc": 0.63095916091578,
          "test_f1": 0.582236693411708
        },
        "best_minus_llm_auc": -0.0434427392464225
      }
    }
  },
  "ood_drop": 0.01445406161520213,
  "relative_drop": 0.49861058855735185,
  "drop": 0.01445406161520213,
  "metrics": {
    "ood_drop": 0.01445406161520213,
    "relative_drop": 0.49861058855735185,
    "id_best_minus_llm_auc": -0.028988677631220372,
    "ood_best_minus_llm_auc": -0.0434427392464225
  }
}
