{
  "timestamp_utc": "2026-02-13T03:28:49+00:00",
  "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint results/saes/husai_pythia70m_topk_seed456/sae_final.pt --sae-release husai_pythia70m_topk_seed456 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --reg-type l1 --setting normal --ks 1,2,5 --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_seed456 --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_benchmark_official/run_20260213T032116Z/husai_custom_saebench --force-rerun",
  "config": {
    "checkpoint": "results/saes/husai_pythia70m_topk_seed456/sae_final.pt",
    "sae_release": "husai_pythia70m_topk_seed456",
    "model_name": "pythia-70m-deduped",
    "hook_layer": 0,
    "hook_name": "blocks.0.hook_resid_pre",
    "reg_type": "l1",
    "setting": "normal",
    "ks": [
      1,
      2,
      5
    ],
    "dataset_names": [
      "100_news_fake",
      "105_click_bait",
      "106_hate_hate",
      "107_hate_offensive",
      "110_aimade_humangpt3",
      "113_movie_sent",
      "114_nyc_borough_Manhattan",
      "115_nyc_borough_Brooklyn",
      "116_nyc_borough_Bronx",
      "117_us_state_FL",
      "118_us_state_CA",
      "119_us_state_TX",
      "120_us_timezone_Chicago",
      "121_us_timezone_New_York",
      "122_us_timezone_Los_Angeles",
      "123_world_country_United_Kingdom",
      "124_world_country_United_States",
      "125_world_country_Italy",
      "126_art_type_book",
      "127_art_type_song",
      "128_art_type_movie",
      "129_arith_mc_A",
      "130_temp_cat_Frequency",
      "131_temp_cat_Typical Time",
      "132_temp_cat_Event Ordering",
      "133_context_type_Causality",
      "134_context_type_Belief_states",
      "135_context_type_Event_duration",
      "136_glue_mnli_entailment",
      "137_glue_mnli_neutral",
      "138_glue_mnli_contradiction",
      "139_news_class_Politics",
      "140_news_class_Technology",
      "141_news_class_Entertainment",
      "142_cancer_cat_Thyroid_Cancer",
      "143_cancer_cat_Lung_Cancer",
      "144_cancer_cat_Colon_Cancer",
      "145_disease_class_digestive system diseases",
      "146_disease_class_cardiovascular diseases",
      "147_disease_class_nervous system diseases",
      "148_twt_emotion_worry",
      "149_twt_emotion_happiness",
      "150_twt_emotion_sadness",
      "151_it_tick_HR Support",
      "152_it_tick_Hardware",
      "153_it_tick_Administrative rights",
      "154_athlete_sport_football",
      "155_athlete_sport_basketball",
      "156_athlete_sport_baseball",
      "157_amazon_5star",
      "158_code_C",
      "159_code_Python",
      "160_code_HTML",
      "161_agnews_0",
      "162_agnews_1",
      "163_agnews_2",
      "21_headline_istrump",
      "22_headline_isobama",
      "23_headline_ischina",
      "24_headline_isiran",
      "26_headline_isfrontpage",
      "36_sciq_tf",
      "41_truthqa_tf",
      "42_temp_sense",
      "44_phys_tf",
      "47_reasoning_tf",
      "48_cm_correct",
      "49_cm_isshort",
      "50_deon_isvalid",
      "51_just_is",
      "52_virtue_is",
      "54_cs_tf",
      "56_wikidatasex_or_gender",
      "57_wikidatais_alive",
      "58_wikidatapolitical_party",
      "59_wikidata_occupation_isjournalist",
      "5_hist_fig_ismale",
      "60_wikidata_occupation_isathlete",
      "61_wikidata_occupation_isactor",
      "62_wikidata_occupation_ispolitician",
      "63_wikidata_occupation_issinger",
      "64_wikidata_occupation_isresearcher",
      "65_high-school",
      "66_living-room",
      "67_social-security",
      "68_credit-card",
      "69_blood-pressure",
      "6_hist_fig_isamerican",
      "70_prime-factors",
      "71_social-media",
      "72_gene-expression",
      "73_control-group",
      "74_magnetic-field",
      "75_cell-lines",
      "76_trial-court",
      "77_second-derivative",
      "78_north-america",
      "79_human-rights",
      "7_hist_fig_ispolitician",
      "80_side-effects",
      "81_public-health",
      "82_federal-government",
      "83_third-party",
      "84_clinical-trials",
      "85_mental-health",
      "87_glue_cola",
      "89_glue_mrpc",
      "90_glue_qnli",
      "91_glue_qqp",
      "92_glue_sst2",
      "94_ai_gen",
      "95_toxic_is",
      "96_spam_is"
    ],
    "dataset_names_inferred_from_cache": true,
    "dataset_count": 113,
    "binarize": false,
    "device": "cuda",
    "dtype": "float32",
    "results_path": "/tmp/husai_saebench_probe_results_seed456",
    "model_cache_path": "/tmp/sae_bench_model_cache",
    "force_rerun": true
  },
  "config_hash": "50e3832e0394d963b9eacdcfee58e213d2bd602675c8727bb4bba41fc3e64603",
  "sae_meta": {
    "d_model": 512,
    "d_sae": 2048,
    "k": 32
  },
  "summary": {
    "result_key": "husai_pythia70m_topk_seed456_custom_sae",
    "llm_metrics": {
      "llm_test_accuracy": 0.6495039546512676,
      "llm_test_auc": 0.6744019001622025,
      "llm_test_f1": 0.617120103509151
    },
    "sae_metrics_by_k": [
      {
        "k": 1,
        "test_accuracy": 0.6003308144301007,
        "test_auc": 0.6029078733674138,
        "test_f1": 0.5340289019760622
      },
      {
        "k": 2,
        "test_accuracy": 0.6052738420113578,
        "test_auc": 0.6120090296822746,
        "test_f1": 0.5474774239131551
      },
      {
        "k": 5,
        "test_accuracy": 0.6100059467818928,
        "test_auc": 0.6222487251237309,
        "test_f1": 0.5583997898219777
      }
    ],
    "best_by_auc": {
      "k": 5,
      "test_accuracy": 0.6100059467818928,
      "test_auc": 0.6222487251237309,
      "test_f1": 0.5583997898219777
    },
    "best_minus_llm_auc": -0.05215317503847161
  }
}
