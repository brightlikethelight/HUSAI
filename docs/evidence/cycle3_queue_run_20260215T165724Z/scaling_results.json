{
  "timestamp_utc": "2026-02-15T18:05:57+00:00",
  "command": "python scripts/experiments/run_external_metric_scaling_study.py --activation-cache-dir /tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped --activation-glob-template *_blocks.{layer}.hook_resid_pre.pt --hook-name-template blocks.{layer}.hook_resid_pre --token-budgets 10000,30000 --hook-layers 0,1 --d-sae-values 1024,2048 --seeds 42,123,456 --k 32 --epochs 6 --batch-size 4096 --learning-rate 0.001 --max-files 16 --max-rows-per-file 2048 --run-saebench --run-cebench --cebench-repo /workspace/CE-Bench --cebench-max-rows 200 --cebench-matched-baseline-summary docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json --cebench-matched-baseline-map docs/evidence/phase4e_cebench_matched200/cebench_baseline_map.json --saebench-datasets 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom --saebench-results-path /tmp/husai_saebench_probe_results_scaling_multiseed --saebench-model-cache-path /tmp/sae_bench_model_cache --cebench-artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --device cuda --dtype float32 --output-dir results/experiments/phase4e_external_scaling_study_multiseed",
  "config": {
    "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
    "activation_glob_template": "*_blocks.{layer}.hook_resid_pre.pt",
    "hook_name_template": "blocks.{layer}.hook_resid_pre",
    "token_budgets": [
      10000,
      30000
    ],
    "hook_layers": [
      0,
      1
    ],
    "d_sae_values": [
      1024,
      2048
    ],
    "seeds": [
      42,
      123,
      456
    ],
    "k": 32,
    "epochs": 6,
    "batch_size": 4096,
    "learning_rate": 0.001,
    "max_files": 16,
    "max_rows_per_file": 2048,
    "model_name": "pythia-70m-deduped",
    "device": "cuda",
    "dtype": "float32",
    "run_saebench": true,
    "run_cebench": true,
    "cebench_repo": "/workspace/CE-Bench",
    "cebench_max_rows": 200,
    "cebench_matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
    "cebench_matched_baseline_map": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_baseline_map.json",
    "resolved_cebench_baseline_map": {
      "default": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
      "0": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
      "blocks.0.hook_resid_pre": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
      "1": null,
      "blocks.1.hook_resid_pre": null
    },
    "saebench_datasets": [
      "100_news_fake",
      "105_click_bait",
      "106_hate_hate",
      "107_hate_offensive",
      "110_aimade_humangpt3",
      "113_movie_sent",
      "114_nyc_borough_Manhattan",
      "115_nyc_borough_Brooklyn",
      "116_nyc_borough_Bronx",
      "117_us_state_FL",
      "118_us_state_CA",
      "119_us_state_TX",
      "120_us_timezone_Chicago",
      "121_us_timezone_New_York",
      "122_us_timezone_Los_Angeles",
      "123_world_country_United_Kingdom"
    ],
    "saebench_dataset_limit": 0,
    "run_id": "run_20260215T165725Z"
  },
  "records": [
    {
      "condition_id": "tok10000_layer0_dsae1024_seed42",
      "token_budget": 10000,
      "hook_layer": 0,
      "hook_name": "blocks.0.hook_resid_pre",
      "d_sae": 1024,
      "seed": 42,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-15T16:57:34+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.0.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 10000,
          "d_sae": 1024,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 42,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae1024_seed42/sae_final.pt",
        "data": {
          "num_files_discovered": 113,
          "num_files_used": 5,
          "total_rows": 10000,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.0.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 0.023924529552459717,
          "mse": 0.0006875979597680271,
          "aux": 0.023236930991212528,
          "l0": 32.0,
          "explained_variance": 0.9982019997501268
        }
      },
      "activation_file_count": 113,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-15T16:57:50+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae1024_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer0_dsae1024_seed42 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae1024_seed42/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae1024_seed42/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok10000_layer0_dsae1024_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "efa83a7400b2d8770424533b0cf354aa2262c4568460049cc5f159795e2f55a7",
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok10000_layer0_dsae1024_seed42_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6605360072070458,
            "llm_test_auc": 0.6928164663947145,
            "llm_test_f1": 0.6491238078533221
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5903987347162004,
              "test_auc": 0.5939589336382974,
              "test_f1": 0.5281342705186512
            },
            {
              "k": 2,
              "test_accuracy": 0.5981931483839118,
              "test_auc": 0.6082231163019333,
              "test_f1": 0.5518259796231364
            },
            {
              "k": 5,
              "test_accuracy": 0.6072726025379925,
              "test_auc": 0.6277230783557071,
              "test_f1": 0.5786097562703737
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6072726025379925,
            "test_auc": 0.6277230783557071,
            "test_f1": 0.5786097562703737
          },
          "best_minus_llm_auc": -0.06509338803900744
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-15T17:00:29+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae1024_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer0_dsae1024_seed42 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae1024_seed42/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "430d03472771df63b661203d2dd6e0963806efb57a9a23b2212cdab0993596b0",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae1024_seed42/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "0418ea95802bdfb167214ed18391fd6530dae850271ab763aaa99ec6c29e1310",
          "sae_release": "husai_scaling_tok10000_layer0_dsae1024_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae1024_seed42/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae1024_seed42/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae1024_seed42/cebench/interpretability_eval/husai_scaling_tok10000_layer0_dsae1024_seed42/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae1024_seed42/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 6.1035255479812625,
          "independent_score_mean_max": 6.32729287147522,
          "interpretability_score_mean_max": 6.1836693835258485,
          "sae_release": "husai_scaling_tok10000_layer0_dsae1024_seed42",
          "sae_id": "custom_sae",
          "date": "2026-02-15 17:00:28",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 6.1035255479812625,
          "independent_score_mean_max": 6.32729287147522,
          "interpretability_score_mean_max": 6.1836693835258485
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -44.40778188228607,
          "independent_score_mean_max": -44.67197347640991,
          "interpretability_score_mean_max": -41.76794220209122
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae1024_seed42/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae1024_seed42/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae1024_seed42/sae_final.pt",
      "cebench_matched_baseline_summary": "docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok10000_layer0_dsae1024_seed123",
      "token_budget": 10000,
      "hook_layer": 0,
      "hook_name": "blocks.0.hook_resid_pre",
      "d_sae": 1024,
      "seed": 123,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-15T17:00:40+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.0.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 10000,
          "d_sae": 1024,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 123,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae1024_seed123/sae_final.pt",
        "data": {
          "num_files_discovered": 113,
          "num_files_used": 5,
          "total_rows": 10000,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.0.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 0.02379264123737812,
          "mse": 0.0006858942215330899,
          "aux": 0.02310674637556076,
          "l0": 32.0,
          "explained_variance": 0.9982084311369147
        }
      },
      "activation_file_count": 113,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-15T17:00:55+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae1024_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer0_dsae1024_seed123 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae1024_seed123/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae1024_seed123/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok10000_layer0_dsae1024_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "27668b7e41f298d9c0d48db801227e4e659e5e5610340c6c09ad2f91bb4e45a7",
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok10000_layer0_dsae1024_seed123_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6605360072070458,
            "llm_test_auc": 0.6928164663947145,
            "llm_test_f1": 0.6491238078533221
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5861750484053766,
              "test_auc": 0.5892655881173839,
              "test_f1": 0.513946961631109
            },
            {
              "k": 2,
              "test_accuracy": 0.5905527849526783,
              "test_auc": 0.6046519431434139,
              "test_f1": 0.5296517280716009
            },
            {
              "k": 5,
              "test_accuracy": 0.6014362414474675,
              "test_auc": 0.6209133024126676,
              "test_f1": 0.5594497759454683
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6014362414474675,
            "test_auc": 0.6209133024126676,
            "test_f1": 0.5594497759454683
          },
          "best_minus_llm_auc": -0.07190316398204688
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-15T17:03:09+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae1024_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer0_dsae1024_seed123 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae1024_seed123/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "93032e3a3843e2703ff17a2319f2d46f32f301bac23233787e30cbf674e91ee4",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae1024_seed123/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "c333ad6f3d948b9a0f4ec4788ef72bba592795291636a69dcb33b89e78a040f2",
          "sae_release": "husai_scaling_tok10000_layer0_dsae1024_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae1024_seed123/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae1024_seed123/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae1024_seed123/cebench/interpretability_eval/husai_scaling_tok10000_layer0_dsae1024_seed123/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae1024_seed123/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 5.634961463212967,
          "independent_score_mean_max": 6.197543411254883,
          "interpretability_score_mean_max": 6.018432220220566,
          "sae_release": "husai_scaling_tok10000_layer0_dsae1024_seed123",
          "sae_id": "custom_sae",
          "date": "2026-02-15 17:03:09",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 5.634961463212967,
          "independent_score_mean_max": 6.197543411254883,
          "interpretability_score_mean_max": 6.018432220220566
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -44.87634596705437,
          "independent_score_mean_max": -44.80172293663025,
          "interpretability_score_mean_max": -41.9331793653965
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae1024_seed123/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae1024_seed123/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae1024_seed123/sae_final.pt",
      "cebench_matched_baseline_summary": "docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok10000_layer0_dsae1024_seed456",
      "token_budget": 10000,
      "hook_layer": 0,
      "hook_name": "blocks.0.hook_resid_pre",
      "d_sae": 1024,
      "seed": 456,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-15T17:03:20+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.0.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 10000,
          "d_sae": 1024,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 456,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae1024_seed456/sae_final.pt",
        "data": {
          "num_files_discovered": 113,
          "num_files_used": 5,
          "total_rows": 10000,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.0.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 0.02399079129099846,
          "mse": 0.0007003333303146064,
          "aux": 0.023290457824865978,
          "l0": 32.0,
          "explained_variance": 0.9981626392627304
        }
      },
      "activation_file_count": 113,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-15T17:03:36+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae1024_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer0_dsae1024_seed456 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae1024_seed456/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae1024_seed456/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok10000_layer0_dsae1024_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "3bbe5535abba6f5913e545ee93608f5c5f7dab7a5624ed63b49fd0576616ebfe",
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok10000_layer0_dsae1024_seed456_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6605360072070458,
            "llm_test_auc": 0.6928164663947145,
            "llm_test_f1": 0.6491238078533221
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5878874773197063,
              "test_auc": 0.5917173748304826,
              "test_f1": 0.5249293113536437
            },
            {
              "k": 2,
              "test_accuracy": 0.5980171604787029,
              "test_auc": 0.6064672182679284,
              "test_f1": 0.5498499317223879
            },
            {
              "k": 5,
              "test_accuracy": 0.5989926897646873,
              "test_auc": 0.6194876406793117,
              "test_f1": 0.5651406367341564
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.5989926897646873,
            "test_auc": 0.6194876406793117,
            "test_f1": 0.5651406367341564
          },
          "best_minus_llm_auc": -0.07332882571540278
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-15T17:05:50+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae1024_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer0_dsae1024_seed456 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae1024_seed456/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "7a5083ec74eb48495c6f92177304262d9eb6fca47279f146264a009a67cb04e5",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae1024_seed456/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "e361f9f3e9e06c0cb329a402be50577cd2b4da425b145a645c5127c6db3bde22",
          "sae_release": "husai_scaling_tok10000_layer0_dsae1024_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae1024_seed456/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae1024_seed456/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae1024_seed456/cebench/interpretability_eval/husai_scaling_tok10000_layer0_dsae1024_seed456/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae1024_seed456/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 5.777439216375351,
          "independent_score_mean_max": 6.15539378285408,
          "interpretability_score_mean_max": 6.017686002254486,
          "sae_release": "husai_scaling_tok10000_layer0_dsae1024_seed456",
          "sae_id": "custom_sae",
          "date": "2026-02-15 17:05:50",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 5.777439216375351,
          "independent_score_mean_max": 6.15539378285408,
          "interpretability_score_mean_max": 6.017686002254486
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -44.73386821389199,
          "independent_score_mean_max": -44.84387256503105,
          "interpretability_score_mean_max": -41.93392558336258
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae1024_seed456/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae1024_seed456/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae1024_seed456/sae_final.pt",
      "cebench_matched_baseline_summary": "docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok10000_layer0_dsae2048_seed42",
      "token_budget": 10000,
      "hook_layer": 0,
      "hook_name": "blocks.0.hook_resid_pre",
      "d_sae": 2048,
      "seed": 42,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-15T17:06:01+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.0.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 10000,
          "d_sae": 2048,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 42,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae2048_seed42/sae_final.pt",
        "data": {
          "num_files_discovered": 113,
          "num_files_used": 5,
          "total_rows": 10000,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.0.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 0.027739140515526135,
          "mse": 0.0007266085982943574,
          "aux": 0.027012531956036884,
          "l0": 32.0,
          "explained_variance": 0.998099990811878
        }
      },
      "activation_file_count": 113,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-15T17:06:17+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae2048_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer0_dsae2048_seed42 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae2048_seed42/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae2048_seed42/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok10000_layer0_dsae2048_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "bd8df7f73c2128dda6e2459b38c7528e8354fb6823e9d2d8c4375635dde679b5",
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok10000_layer0_dsae2048_seed42_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6605360072070458,
            "llm_test_auc": 0.6928164663947145,
            "llm_test_f1": 0.6491238078533221
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5888758791882616,
              "test_auc": 0.5873558965248387,
              "test_f1": 0.5043111853444856
            },
            {
              "k": 2,
              "test_accuracy": 0.5971572239701463,
              "test_auc": 0.6052092540111926,
              "test_f1": 0.5391802561581029
            },
            {
              "k": 5,
              "test_accuracy": 0.6015673758237768,
              "test_auc": 0.6193987212915215,
              "test_f1": 0.5555359738829213
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6015673758237768,
            "test_auc": 0.6193987212915215,
            "test_f1": 0.5555359738829213
          },
          "best_minus_llm_auc": -0.07341774510319299
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-15T17:08:38+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae2048_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer0_dsae2048_seed42 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae2048_seed42/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "7a71bef1071d08081011bf2df22fe8edd6e4d6f92d8395f4fc07af5296a2875a",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae2048_seed42/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "12c2711c0d51c19d5f53f37ab0ad1a7575284e8f0a7940e5453f6f28bbd95f5d",
          "sae_release": "husai_scaling_tok10000_layer0_dsae2048_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae2048_seed42/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae2048_seed42/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae2048_seed42/cebench/interpretability_eval/husai_scaling_tok10000_layer0_dsae2048_seed42/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae2048_seed42/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 7.1261831831932065,
          "independent_score_mean_max": 7.748877003192901,
          "interpretability_score_mean_max": 7.5051356482505795,
          "sae_release": "husai_scaling_tok10000_layer0_dsae2048_seed42",
          "sae_id": "custom_sae",
          "date": "2026-02-15 17:08:38",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 7.1261831831932065,
          "independent_score_mean_max": 7.748877003192901,
          "interpretability_score_mean_max": 7.5051356482505795
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -43.38512424707413,
          "independent_score_mean_max": -43.25038934469223,
          "interpretability_score_mean_max": -40.446475937366486
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae2048_seed42/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae2048_seed42/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae2048_seed42/sae_final.pt",
      "cebench_matched_baseline_summary": "docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok10000_layer0_dsae2048_seed123",
      "token_budget": 10000,
      "hook_layer": 0,
      "hook_name": "blocks.0.hook_resid_pre",
      "d_sae": 2048,
      "seed": 123,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-15T17:08:50+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.0.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 10000,
          "d_sae": 2048,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 123,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae2048_seed123/sae_final.pt",
        "data": {
          "num_files_discovered": 113,
          "num_files_used": 5,
          "total_rows": 10000,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.0.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 0.0276201485345761,
          "mse": 0.0007260841278669735,
          "aux": 0.026894064620137215,
          "l0": 32.0,
          "explained_variance": 0.9981034543306702
        }
      },
      "activation_file_count": 113,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-15T17:09:05+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae2048_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer0_dsae2048_seed123 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae2048_seed123/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae2048_seed123/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok10000_layer0_dsae2048_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "031cf16ec184cd399152f3ac2b3427ce87f7f5406b8a7c4a99b148abe40c781d",
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok10000_layer0_dsae2048_seed123_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6605360072070458,
            "llm_test_auc": 0.6928164663947145,
            "llm_test_f1": 0.6491238078533221
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5806877655941578,
              "test_auc": 0.5809584186787633,
              "test_f1": 0.5013294789747487
            },
            {
              "k": 2,
              "test_accuracy": 0.5850785074879666,
              "test_auc": 0.5872511693861652,
              "test_f1": 0.5222540727705147
            },
            {
              "k": 5,
              "test_accuracy": 0.5903430683233557,
              "test_auc": 0.5991072952056484,
              "test_f1": 0.5366964612789497
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.5903430683233557,
            "test_auc": 0.5991072952056484,
            "test_f1": 0.5366964612789497
          },
          "best_minus_llm_auc": -0.09370917118906608
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-15T17:11:28+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae2048_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer0_dsae2048_seed123 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae2048_seed123/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "a7703f7de2effea53b55d191953e10faeb61c7ced458b7d08f2c97dae2f06871",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae2048_seed123/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "07a72e2b5668b29f0d4a64d6eff32ab3bb1af626bec0889a9424b3c5897bdb98",
          "sae_release": "husai_scaling_tok10000_layer0_dsae2048_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae2048_seed123/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae2048_seed123/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae2048_seed123/cebench/interpretability_eval/husai_scaling_tok10000_layer0_dsae2048_seed123/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae2048_seed123/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 6.752451677322387,
          "independent_score_mean_max": 7.693973977565765,
          "interpretability_score_mean_max": 7.224240756034851,
          "sae_release": "husai_scaling_tok10000_layer0_dsae2048_seed123",
          "sae_id": "custom_sae",
          "date": "2026-02-15 17:11:28",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 6.752451677322387,
          "independent_score_mean_max": 7.693973977565765,
          "interpretability_score_mean_max": 7.224240756034851
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -43.75885575294495,
          "independent_score_mean_max": -43.305292370319364,
          "interpretability_score_mean_max": -40.72737082958221
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae2048_seed123/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae2048_seed123/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae2048_seed123/sae_final.pt",
      "cebench_matched_baseline_summary": "docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok10000_layer0_dsae2048_seed456",
      "token_budget": 10000,
      "hook_layer": 0,
      "hook_name": "blocks.0.hook_resid_pre",
      "d_sae": 2048,
      "seed": 456,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-15T17:11:39+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.0.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 10000,
          "d_sae": 2048,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 456,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae2048_seed456/sae_final.pt",
        "data": {
          "num_files_discovered": 113,
          "num_files_used": 5,
          "total_rows": 10000,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.0.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 0.027181211858987808,
          "mse": 0.0007088804850354791,
          "aux": 0.02647233133514722,
          "l0": 32.0,
          "explained_variance": 0.998140215359969
        }
      },
      "activation_file_count": 113,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-15T17:11:55+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae2048_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer0_dsae2048_seed456 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae2048_seed456/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae2048_seed456/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok10000_layer0_dsae2048_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "08b1846f6809656437f5c81dcdb11f11b6a988e38d3b57c25ae81de31effaf63",
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok10000_layer0_dsae2048_seed456_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6605360072070458,
            "llm_test_auc": 0.6928164663947145,
            "llm_test_f1": 0.6491238078533221
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5891564651852224,
              "test_auc": 0.590753704554468,
              "test_f1": 0.5100593515597587
            },
            {
              "k": 2,
              "test_accuracy": 0.5957378159584937,
              "test_auc": 0.6075970467954673,
              "test_f1": 0.5343365281893284
            },
            {
              "k": 5,
              "test_accuracy": 0.6004439759006254,
              "test_auc": 0.6123442020359172,
              "test_f1": 0.5529446377195382
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6004439759006254,
            "test_auc": 0.6123442020359172,
            "test_f1": 0.5529446377195382
          },
          "best_minus_llm_auc": -0.08047226435879729
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-15T17:14:15+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae2048_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer0_dsae2048_seed456 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae2048_seed456/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "1b0f358d0c0fe03f5723bf66b6cb78f12b5f9634751838814c191c38c84265c6",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae2048_seed456/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "02d2646491628f601bc3658c9a050e3309ccc8f8d97f86aa0817c2ea30876ff8",
          "sae_release": "husai_scaling_tok10000_layer0_dsae2048_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae2048_seed456/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae2048_seed456/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae2048_seed456/cebench/interpretability_eval/husai_scaling_tok10000_layer0_dsae2048_seed456/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae2048_seed456/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 6.836932709217072,
          "independent_score_mean_max": 7.629131286144257,
          "interpretability_score_mean_max": 7.289512159824372,
          "sae_release": "husai_scaling_tok10000_layer0_dsae2048_seed456",
          "sae_id": "custom_sae",
          "date": "2026-02-15 17:14:15",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 6.836932709217072,
          "independent_score_mean_max": 7.629131286144257,
          "interpretability_score_mean_max": 7.289512159824372
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -43.67437472105026,
          "independent_score_mean_max": -43.370135061740875,
          "interpretability_score_mean_max": -40.6620994257927
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae2048_seed456/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer0_dsae2048_seed456/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer0_dsae2048_seed456/sae_final.pt",
      "cebench_matched_baseline_summary": "docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok10000_layer1_dsae1024_seed42",
      "token_budget": 10000,
      "hook_layer": 1,
      "hook_name": "blocks.1.hook_resid_pre",
      "d_sae": 1024,
      "seed": 42,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-15T17:14:27+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.1.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 10000,
          "d_sae": 1024,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 42,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae1024_seed42/sae_final.pt",
        "data": {
          "num_files_discovered": 16,
          "num_files_used": 5,
          "total_rows": 10000,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.1.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 30.693647384643555,
          "mse": 0.09719767173131307,
          "aux": 30.59644953409831,
          "l0": 32.0,
          "explained_variance": 0.9973477099700362
        }
      },
      "activation_file_count": 16,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-15T17:14:43+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae1024_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer1_dsae1024_seed42 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae1024_seed42/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae1024_seed42/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok10000_layer1_dsae1024_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "55e961895a3bacaab4c27b55c9d87977ccdee23121023572ae933ac716f43ae6",
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok10000_layer1_dsae1024_seed42_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6892726671847929,
            "llm_test_auc": 0.7249531199453926,
            "llm_test_f1": 0.6888365744296719
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5951484569633723,
              "test_auc": 0.6059859502291934,
              "test_f1": 0.5580986101623011
            },
            {
              "k": 2,
              "test_accuracy": 0.6055067132435785,
              "test_auc": 0.620196184691548,
              "test_f1": 0.5783452602303576
            },
            {
              "k": 5,
              "test_accuracy": 0.607846635889937,
              "test_auc": 0.6296849225596066,
              "test_f1": 0.5893324753455207
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.607846635889937,
            "test_auc": 0.6296849225596066,
            "test_f1": 0.5893324753455207
          },
          "best_minus_llm_auc": -0.09526819738578596
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-15T17:16:55+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae1024_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer1_dsae1024_seed42 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae1024_seed42/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200",
        "config_hash": "eae631e6e0d2d042a2e006b6112f43849744476c3cc35e81eea5522cf5f7a4da",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae1024_seed42/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "5880a7d2cbae1e0ddd0784c24383a3bf5f40dbdd0406aaed97f2e3cc1d124f8c",
          "sae_release": "husai_scaling_tok10000_layer1_dsae1024_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae1024_seed42/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": null
        },
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae1024_seed42/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae1024_seed42/cebench/interpretability_eval/husai_scaling_tok10000_layer1_dsae1024_seed42/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae1024_seed42/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 8.353993542194367,
          "independent_score_mean_max": 8.643186836242675,
          "interpretability_score_mean_max": 7.965026619434357,
          "sae_release": "husai_scaling_tok10000_layer1_dsae1024_seed42",
          "sae_id": "custom_sae",
          "date": "2026-02-15 17:16:54",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 8.353993542194367,
          "independent_score_mean_max": 8.643186836242675,
          "interpretability_score_mean_max": 7.965026619434357
        },
        "matched_baseline_metrics": null,
        "delta_vs_matched_baseline": null,
        "matched_baseline_payload": null,
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae1024_seed42/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae1024_seed42/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae1024_seed42/sae_final.pt",
      "cebench_matched_baseline_summary": null,
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok10000_layer1_dsae1024_seed123",
      "token_budget": 10000,
      "hook_layer": 1,
      "hook_name": "blocks.1.hook_resid_pre",
      "d_sae": 1024,
      "seed": 123,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-15T17:17:06+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.1.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 10000,
          "d_sae": 1024,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 123,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae1024_seed123/sae_final.pt",
        "data": {
          "num_files_discovered": 16,
          "num_files_used": 5,
          "total_rows": 10000,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.1.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 30.879554748535156,
          "mse": 0.09864334017038345,
          "aux": 30.780911127726238,
          "l0": 32.0,
          "explained_variance": 0.9973074189200457
        }
      },
      "activation_file_count": 16,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-15T17:17:21+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae1024_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer1_dsae1024_seed123 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae1024_seed123/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae1024_seed123/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok10000_layer1_dsae1024_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "34162092d8bbd99c4be9a89c4657a49e35feff25fbc8c52363876583ec514610",
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok10000_layer1_dsae1024_seed123_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6892726671847929,
            "llm_test_auc": 0.7249531199453926,
            "llm_test_f1": 0.6888365744296719
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.58912400007913,
              "test_auc": 0.5980836971051798,
              "test_f1": 0.5322119687237977
            },
            {
              "k": 2,
              "test_accuracy": 0.592629991646939,
              "test_auc": 0.6172858153345406,
              "test_f1": 0.5495387362386535
            },
            {
              "k": 5,
              "test_accuracy": 0.6152279140156325,
              "test_auc": 0.6340163961038806,
              "test_f1": 0.5912966971027499
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6152279140156325,
            "test_auc": 0.6340163961038806,
            "test_f1": 0.5912966971027499
          },
          "best_minus_llm_auc": -0.09093672384151197
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-15T17:19:33+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae1024_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer1_dsae1024_seed123 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae1024_seed123/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200",
        "config_hash": "fa2c1550fc476d1011f7ae75d062b376bc0b629e6e8338a5be7310e84602f146",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae1024_seed123/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "e3c279b792c7e6f82189e235d126012d91a7c803d42e49fc5585a69b8f9f2344",
          "sae_release": "husai_scaling_tok10000_layer1_dsae1024_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae1024_seed123/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": null
        },
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae1024_seed123/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae1024_seed123/cebench/interpretability_eval/husai_scaling_tok10000_layer1_dsae1024_seed123/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae1024_seed123/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 8.256409327983857,
          "independent_score_mean_max": 8.831921746730805,
          "interpretability_score_mean_max": 8.1289102602005,
          "sae_release": "husai_scaling_tok10000_layer1_dsae1024_seed123",
          "sae_id": "custom_sae",
          "date": "2026-02-15 17:19:33",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 8.256409327983857,
          "independent_score_mean_max": 8.831921746730805,
          "interpretability_score_mean_max": 8.1289102602005
        },
        "matched_baseline_metrics": null,
        "delta_vs_matched_baseline": null,
        "matched_baseline_payload": null,
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae1024_seed123/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae1024_seed123/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae1024_seed123/sae_final.pt",
      "cebench_matched_baseline_summary": null,
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok10000_layer1_dsae1024_seed456",
      "token_budget": 10000,
      "hook_layer": 1,
      "hook_name": "blocks.1.hook_resid_pre",
      "d_sae": 1024,
      "seed": 456,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-15T17:19:44+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.1.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 10000,
          "d_sae": 1024,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 456,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae1024_seed456/sae_final.pt",
        "data": {
          "num_files_discovered": 16,
          "num_files_used": 5,
          "total_rows": 10000,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.1.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 31.743549982706707,
          "mse": 0.10233704497416814,
          "aux": 31.641213099161785,
          "l0": 32.0,
          "explained_variance": 0.9972004854643253
        }
      },
      "activation_file_count": 16,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-15T17:20:00+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae1024_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer1_dsae1024_seed456 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae1024_seed456/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae1024_seed456/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok10000_layer1_dsae1024_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "c38b4f09021dca63db9ffc66e92a412f9923cdd93c39365790951847e28f71bd",
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok10000_layer1_dsae1024_seed456_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6892726671847929,
            "llm_test_auc": 0.7249531199453926,
            "llm_test_f1": 0.6888365744296719
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5909786984521694,
              "test_auc": 0.6039557749546629,
              "test_f1": 0.5690719537936866
            },
            {
              "k": 2,
              "test_accuracy": 0.5995531866311412,
              "test_auc": 0.6192446250291722,
              "test_f1": 0.5825595439128594
            },
            {
              "k": 5,
              "test_accuracy": 0.6099313688595435,
              "test_auc": 0.6332066155116611,
              "test_f1": 0.6074349670166826
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6099313688595435,
            "test_auc": 0.6332066155116611,
            "test_f1": 0.6074349670166826
          },
          "best_minus_llm_auc": -0.09174650443373145
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-15T17:22:12+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae1024_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer1_dsae1024_seed456 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae1024_seed456/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200",
        "config_hash": "63d16c46e3fe059a29f05262c5f05d628e26d91db40bdc13c20ba3a3793b709b",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae1024_seed456/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "9280fd108d90c413834131c5c8d60652306f2ee3a6021f12d85bdb2303644ae1",
          "sae_release": "husai_scaling_tok10000_layer1_dsae1024_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae1024_seed456/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": null
        },
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae1024_seed456/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae1024_seed456/cebench/interpretability_eval/husai_scaling_tok10000_layer1_dsae1024_seed456/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae1024_seed456/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 8.209339504241944,
          "independent_score_mean_max": 8.85705245733261,
          "interpretability_score_mean_max": 8.029204256534577,
          "sae_release": "husai_scaling_tok10000_layer1_dsae1024_seed456",
          "sae_id": "custom_sae",
          "date": "2026-02-15 17:22:12",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 8.209339504241944,
          "independent_score_mean_max": 8.85705245733261,
          "interpretability_score_mean_max": 8.029204256534577
        },
        "matched_baseline_metrics": null,
        "delta_vs_matched_baseline": null,
        "matched_baseline_payload": null,
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae1024_seed456/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae1024_seed456/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae1024_seed456/sae_final.pt",
      "cebench_matched_baseline_summary": null,
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok10000_layer1_dsae2048_seed42",
      "token_budget": 10000,
      "hook_layer": 1,
      "hook_name": "blocks.1.hook_resid_pre",
      "d_sae": 2048,
      "seed": 42,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-15T17:22:23+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.1.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 10000,
          "d_sae": 2048,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 42,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae2048_seed42/sae_final.pt",
        "data": {
          "num_files_discovered": 16,
          "num_files_used": 5,
          "total_rows": 10000,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.1.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 35.605055491129555,
          "mse": 0.09882016976674397,
          "aux": 35.50623575846354,
          "l0": 32.0,
          "explained_variance": 0.997303435911961
        }
      },
      "activation_file_count": 16,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-15T17:22:39+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae2048_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer1_dsae2048_seed42 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae2048_seed42/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae2048_seed42/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok10000_layer1_dsae2048_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "03f7cb7fa6d886111a0d70601eac001c1111017f229c6acc14222f427570ae7c",
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok10000_layer1_dsae2048_seed42_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6892726671847929,
            "llm_test_auc": 0.7249531199453926,
            "llm_test_f1": 0.6888365744296719
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5966319218257604,
              "test_auc": 0.6048622500167886,
              "test_f1": 0.5470320917495226
            },
            {
              "k": 2,
              "test_accuracy": 0.5998705692962161,
              "test_auc": 0.6143154286428657,
              "test_f1": 0.5671594830273673
            },
            {
              "k": 5,
              "test_accuracy": 0.6089285038463079,
              "test_auc": 0.6285164386917815,
              "test_f1": 0.5877860803494441
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6089285038463079,
            "test_auc": 0.6285164386917815,
            "test_f1": 0.5877860803494441
          },
          "best_minus_llm_auc": -0.09643668125361105
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-15T17:25:01+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae2048_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer1_dsae2048_seed42 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae2048_seed42/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200",
        "config_hash": "7fb95e1edd8d0db4a093f10e72894a917d361a3d26caf0b8fac42989e168bd54",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae2048_seed42/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "d64c59133df0a7365a4a48504dbbe312e28df770b3d798d821d459c43147c4cb",
          "sae_release": "husai_scaling_tok10000_layer1_dsae2048_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae2048_seed42/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": null
        },
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae2048_seed42/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae2048_seed42/cebench/interpretability_eval/husai_scaling_tok10000_layer1_dsae2048_seed42/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae2048_seed42/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 10.061518630981446,
          "independent_score_mean_max": 11.641879398822784,
          "interpretability_score_mean_max": 10.38789572238922,
          "sae_release": "husai_scaling_tok10000_layer1_dsae2048_seed42",
          "sae_id": "custom_sae",
          "date": "2026-02-15 17:25:01",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 10.061518630981446,
          "independent_score_mean_max": 11.641879398822784,
          "interpretability_score_mean_max": 10.38789572238922
        },
        "matched_baseline_metrics": null,
        "delta_vs_matched_baseline": null,
        "matched_baseline_payload": null,
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae2048_seed42/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae2048_seed42/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae2048_seed42/sae_final.pt",
      "cebench_matched_baseline_summary": null,
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok10000_layer1_dsae2048_seed123",
      "token_budget": 10000,
      "hook_layer": 1,
      "hook_name": "blocks.1.hook_resid_pre",
      "d_sae": 2048,
      "seed": 123,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-15T17:25:12+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.1.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 10000,
          "d_sae": 2048,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 123,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae2048_seed123/sae_final.pt",
        "data": {
          "num_files_discovered": 16,
          "num_files_used": 5,
          "total_rows": 10000,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.1.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 34.886828104654946,
          "mse": 0.0953299676378568,
          "aux": 34.791497548421226,
          "l0": 32.0,
          "explained_variance": 0.9973978611554415
        }
      },
      "activation_file_count": 16,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-15T17:25:27+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae2048_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer1_dsae2048_seed123 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae2048_seed123/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae2048_seed123/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok10000_layer1_dsae2048_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "c131d81ec77eee3523759fe31081b8b442ea62d09e2899bf1c90d89a19eb0785",
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok10000_layer1_dsae2048_seed123_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6892726671847929,
            "llm_test_auc": 0.7249531199453926,
            "llm_test_f1": 0.6888365744296719
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5847308014386691,
              "test_auc": 0.592707811293502,
              "test_f1": 0.5294122725252431
            },
            {
              "k": 2,
              "test_accuracy": 0.5902602681203406,
              "test_auc": 0.6037567233740073,
              "test_f1": 0.5629963259030321
            },
            {
              "k": 5,
              "test_accuracy": 0.6029747907419506,
              "test_auc": 0.6203108683146042,
              "test_f1": 0.5799174292333348
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6029747907419506,
            "test_auc": 0.6203108683146042,
            "test_f1": 0.5799174292333348
          },
          "best_minus_llm_auc": -0.10464225163078833
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-15T17:27:57+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae2048_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer1_dsae2048_seed123 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae2048_seed123/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200",
        "config_hash": "c563f4b20de229215c112604fe072bd999ea3bc5b4953914d450068a8190d61d",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae2048_seed123/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "4cfcc0abd5088b90ac3672ee416b1a5411eb2762c011e2c7ee634175aab4ae54",
          "sae_release": "husai_scaling_tok10000_layer1_dsae2048_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae2048_seed123/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": null
        },
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae2048_seed123/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae2048_seed123/cebench/interpretability_eval/husai_scaling_tok10000_layer1_dsae2048_seed123/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae2048_seed123/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 10.045469830036163,
          "independent_score_mean_max": 10.648979089260102,
          "interpretability_score_mean_max": 9.797955973148346,
          "sae_release": "husai_scaling_tok10000_layer1_dsae2048_seed123",
          "sae_id": "custom_sae",
          "date": "2026-02-15 17:27:57",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 10.045469830036163,
          "independent_score_mean_max": 10.648979089260102,
          "interpretability_score_mean_max": 9.797955973148346
        },
        "matched_baseline_metrics": null,
        "delta_vs_matched_baseline": null,
        "matched_baseline_payload": null,
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae2048_seed123/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae2048_seed123/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae2048_seed123/sae_final.pt",
      "cebench_matched_baseline_summary": null,
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok10000_layer1_dsae2048_seed456",
      "token_budget": 10000,
      "hook_layer": 1,
      "hook_name": "blocks.1.hook_resid_pre",
      "d_sae": 2048,
      "seed": 456,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-15T17:28:08+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.1.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 10000,
          "d_sae": 2048,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 456,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae2048_seed456/sae_final.pt",
        "data": {
          "num_files_discovered": 16,
          "num_files_used": 5,
          "total_rows": 10000,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.1.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 35.46282704671224,
          "mse": 0.09705839306116104,
          "aux": 35.36576843261719,
          "l0": 32.0,
          "explained_variance": 0.9973448873547938
        }
      },
      "activation_file_count": 16,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-15T17:28:24+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae2048_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer1_dsae2048_seed456 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae2048_seed456/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae2048_seed456/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok10000_layer1_dsae2048_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "84b335122b4bb700f74989e30d39dec55528d75a3e5ebc73a5c79b1c6bcde831",
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok10000_layer1_dsae2048_seed456_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6892726671847929,
            "llm_test_auc": 0.7249531199453926,
            "llm_test_f1": 0.6888365744296719
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5854474761347989,
              "test_auc": 0.5918182894676515,
              "test_f1": 0.5339449255505163
            },
            {
              "k": 2,
              "test_accuracy": 0.5834740194584211,
              "test_auc": 0.5950048838587361,
              "test_f1": 0.5446701561640617
            },
            {
              "k": 5,
              "test_accuracy": 0.6024666847331104,
              "test_auc": 0.6264216979460538,
              "test_f1": 0.5681540561280853
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6024666847331104,
            "test_auc": 0.6264216979460538,
            "test_f1": 0.5681540561280853
          },
          "best_minus_llm_auc": -0.09853142199933873
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-15T17:30:45+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae2048_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok10000_layer1_dsae2048_seed456 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae2048_seed456/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200",
        "config_hash": "90f48b6cbce889c548164183ec24b2ea2129464df9659956e245faa100635cb2",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae2048_seed456/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "7d0311fc9294672aca94c60fed0ea24a7a7a828d195d69a634c900fb4bef9664",
          "sae_release": "husai_scaling_tok10000_layer1_dsae2048_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae2048_seed456/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": null
        },
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae2048_seed456/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae2048_seed456/cebench/interpretability_eval/husai_scaling_tok10000_layer1_dsae2048_seed456/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae2048_seed456/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 9.792248265743256,
          "independent_score_mean_max": 11.173166918754578,
          "interpretability_score_mean_max": 9.79960574388504,
          "sae_release": "husai_scaling_tok10000_layer1_dsae2048_seed456",
          "sae_id": "custom_sae",
          "date": "2026-02-15 17:30:45",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 9.792248265743256,
          "independent_score_mean_max": 11.173166918754578,
          "interpretability_score_mean_max": 9.79960574388504
        },
        "matched_baseline_metrics": null,
        "delta_vs_matched_baseline": null,
        "matched_baseline_payload": null,
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae2048_seed456/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok10000_layer1_dsae2048_seed456/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok10000_layer1_dsae2048_seed456/sae_final.pt",
      "cebench_matched_baseline_summary": null,
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok30000_layer0_dsae1024_seed42",
      "token_budget": 30000,
      "hook_layer": 0,
      "hook_name": "blocks.0.hook_resid_pre",
      "d_sae": 1024,
      "seed": 42,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-15T17:30:57+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.0.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 30000,
          "d_sae": 1024,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 42,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae1024_seed42/sae_final.pt",
        "data": {
          "num_files_discovered": 113,
          "num_files_used": 16,
          "total_rows": 29432,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/113_movie_sent_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/114_nyc_borough_Manhattan_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/115_nyc_borough_Brooklyn_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/116_nyc_borough_Bronx_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/117_us_state_FL_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/118_us_state_CA_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/119_us_state_TX_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/120_us_timezone_Chicago_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/121_us_timezone_New_York_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/122_us_timezone_Los_Angeles_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/123_world_country_United_Kingdom_blocks.0.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 0.017084247898310423,
          "mse": 0.0005171554366825148,
          "aux": 0.016567092388868332,
          "l0": 32.0,
          "explained_variance": 0.9988613782775199
        }
      },
      "activation_file_count": 113,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-15T17:31:13+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae1024_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer0_dsae1024_seed42 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae1024_seed42/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae1024_seed42/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok30000_layer0_dsae1024_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "cf0b2fad9f2cbf2eb65d3e322d3ea6ae5ed866e909aecfcee85371e944e6d53e",
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok30000_layer0_dsae1024_seed42_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6605360072070458,
            "llm_test_auc": 0.6928164663947145,
            "llm_test_f1": 0.6491238078533221
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5907864222559945,
              "test_auc": 0.5961540016688295,
              "test_f1": 0.5268683040363374
            },
            {
              "k": 2,
              "test_accuracy": 0.5891605257127436,
              "test_auc": 0.5991051916232417,
              "test_f1": 0.5214799265252326
            },
            {
              "k": 5,
              "test_accuracy": 0.5949139622866223,
              "test_auc": 0.6091209692709305,
              "test_f1": 0.5607896663058237
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.5949139622866223,
            "test_auc": 0.6091209692709305,
            "test_f1": 0.5607896663058237
          },
          "best_minus_llm_auc": -0.08369549712378399
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-15T17:33:23+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae1024_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer0_dsae1024_seed42 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae1024_seed42/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "c46739f99ef6e8e1471f979865a08a8c02dbd2ad44369943c4d8cc66efcc3b10",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae1024_seed42/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "0a680b634db1c845e6a2f360086f42fcbb965c203809f892214fd2d4dda8b059",
          "sae_release": "husai_scaling_tok30000_layer0_dsae1024_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae1024_seed42/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae1024_seed42/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae1024_seed42/cebench/interpretability_eval/husai_scaling_tok30000_layer0_dsae1024_seed42/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae1024_seed42/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 6.311198704242706,
          "independent_score_mean_max": 6.553330156803131,
          "interpretability_score_mean_max": 6.440171301364899,
          "sae_release": "husai_scaling_tok30000_layer0_dsae1024_seed42",
          "sae_id": "custom_sae",
          "date": "2026-02-15 17:33:23",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 6.311198704242706,
          "independent_score_mean_max": 6.553330156803131,
          "interpretability_score_mean_max": 6.440171301364899
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -44.20010872602463,
          "independent_score_mean_max": -44.445936191082,
          "interpretability_score_mean_max": -41.51144028425217
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae1024_seed42/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae1024_seed42/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae1024_seed42/sae_final.pt",
      "cebench_matched_baseline_summary": "docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok30000_layer0_dsae1024_seed123",
      "token_budget": 30000,
      "hook_layer": 0,
      "hook_name": "blocks.0.hook_resid_pre",
      "d_sae": 1024,
      "seed": 123,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-15T17:33:36+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.0.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 30000,
          "d_sae": 1024,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 123,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae1024_seed123/sae_final.pt",
        "data": {
          "num_files_discovered": 113,
          "num_files_used": 16,
          "total_rows": 29432,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/113_movie_sent_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/114_nyc_borough_Manhattan_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/115_nyc_borough_Brooklyn_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/116_nyc_borough_Bronx_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/117_us_state_FL_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/118_us_state_CA_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/119_us_state_TX_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/120_us_timezone_Chicago_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/121_us_timezone_New_York_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/122_us_timezone_Los_Angeles_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/123_world_country_United_Kingdom_blocks.0.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 0.017122834688052535,
          "mse": 0.0005205777415540069,
          "aux": 0.01660225703381002,
          "l0": 32.0,
          "explained_variance": 0.9988539345273441
        }
      },
      "activation_file_count": 113,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-15T17:33:51+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae1024_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer0_dsae1024_seed123 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae1024_seed123/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae1024_seed123/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok30000_layer0_dsae1024_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "c1294bb30c4280cc3600fcbd02aef68757ebb388a24a9edc279f4dce64e236b1",
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok30000_layer0_dsae1024_seed123_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6605360072070458,
            "llm_test_auc": 0.6928164663947145,
            "llm_test_f1": 0.6491238078533221
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5791669073009936,
              "test_auc": 0.5910499058685158,
              "test_f1": 0.5253777385598273
            },
            {
              "k": 2,
              "test_accuracy": 0.5857482683149119,
              "test_auc": 0.6006771826979113,
              "test_f1": 0.5366302199078505
            },
            {
              "k": 5,
              "test_accuracy": 0.6013701368489877,
              "test_auc": 0.6203747097094297,
              "test_f1": 0.566973987944639
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6013701368489877,
            "test_auc": 0.6203747097094297,
            "test_f1": 0.566973987944639
          },
          "best_minus_llm_auc": -0.07244175668528485
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-15T17:36:07+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae1024_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer0_dsae1024_seed123 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae1024_seed123/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "00986c5e3d5dbdfe9cd3ee61c11a346fc80b17c3c4498e482242e93f8059aa2a",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae1024_seed123/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "4ce2bc82b7e4de2fe8d49d91cec633ee86873f3aa8b03f3bd0cc69866ad7bb78",
          "sae_release": "husai_scaling_tok30000_layer0_dsae1024_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae1024_seed123/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae1024_seed123/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae1024_seed123/cebench/interpretability_eval/husai_scaling_tok30000_layer0_dsae1024_seed123/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae1024_seed123/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 5.983096237182617,
          "independent_score_mean_max": 6.326351456642151,
          "interpretability_score_mean_max": 6.146210596561432,
          "sae_release": "husai_scaling_tok30000_layer0_dsae1024_seed123",
          "sae_id": "custom_sae",
          "date": "2026-02-15 17:36:07",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 5.983096237182617,
          "independent_score_mean_max": 6.326351456642151,
          "interpretability_score_mean_max": 6.146210596561432
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -44.52821119308472,
          "independent_score_mean_max": -44.67291489124298,
          "interpretability_score_mean_max": -41.805400989055634
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae1024_seed123/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae1024_seed123/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae1024_seed123/sae_final.pt",
      "cebench_matched_baseline_summary": "docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok30000_layer0_dsae1024_seed456",
      "token_budget": 30000,
      "hook_layer": 0,
      "hook_name": "blocks.0.hook_resid_pre",
      "d_sae": 1024,
      "seed": 456,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-15T17:36:20+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.0.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 30000,
          "d_sae": 1024,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 456,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae1024_seed456/sae_final.pt",
        "data": {
          "num_files_discovered": 113,
          "num_files_used": 16,
          "total_rows": 29432,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/113_movie_sent_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/114_nyc_borough_Manhattan_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/115_nyc_borough_Brooklyn_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/116_nyc_borough_Bronx_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/117_us_state_FL_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/118_us_state_CA_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/119_us_state_TX_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/120_us_timezone_Chicago_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/121_us_timezone_New_York_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/122_us_timezone_Los_Angeles_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/123_world_country_United_Kingdom_blocks.0.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 0.016936915926635265,
          "mse": 0.0005123616574564949,
          "aux": 0.016424554167315364,
          "l0": 32.0,
          "explained_variance": 0.9988714647523498
        }
      },
      "activation_file_count": 113,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-15T17:36:36+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae1024_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer0_dsae1024_seed456 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae1024_seed456/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae1024_seed456/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok30000_layer0_dsae1024_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "c55cb3757c95bda93ab8c5dc8f4004a3a50eafcfbe0a13a734cc1c121737a717",
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok30000_layer0_dsae1024_seed456_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6605360072070458,
            "llm_test_auc": 0.6928164663947145,
            "llm_test_f1": 0.6491238078533221
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5873226835695563,
              "test_auc": 0.5922259225933229,
              "test_f1": 0.5078439165162539
            },
            {
              "k": 2,
              "test_accuracy": 0.5935018038151861,
              "test_auc": 0.6126340028405698,
              "test_f1": 0.52486183465231
            },
            {
              "k": 5,
              "test_accuracy": 0.6022962037010151,
              "test_auc": 0.617735259028003,
              "test_f1": 0.5521057841938213
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6022962037010151,
            "test_auc": 0.617735259028003,
            "test_f1": 0.5521057841938213
          },
          "best_minus_llm_auc": -0.07508120736671153
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-15T17:38:56+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae1024_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer0_dsae1024_seed456 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae1024_seed456/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "1bb2f7f4ee2bee0dcad07c6b8405bac0e7ac8ff795444b17b694afb51e68eb00",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae1024_seed456/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "599ad4f876c18f336b57f5676df5b53de277080322d77d1fe5b26df8a2b8cb90",
          "sae_release": "husai_scaling_tok30000_layer0_dsae1024_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae1024_seed456/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae1024_seed456/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae1024_seed456/cebench/interpretability_eval/husai_scaling_tok30000_layer0_dsae1024_seed456/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae1024_seed456/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 5.862107490301132,
          "independent_score_mean_max": 6.040290415287018,
          "interpretability_score_mean_max": 6.07377854347229,
          "sae_release": "husai_scaling_tok30000_layer0_dsae1024_seed456",
          "sae_id": "custom_sae",
          "date": "2026-02-15 17:38:55",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 5.862107490301132,
          "independent_score_mean_max": 6.040290415287018,
          "interpretability_score_mean_max": 6.07377854347229
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -44.6491999399662,
          "independent_score_mean_max": -44.958975932598115,
          "interpretability_score_mean_max": -41.87783304214477
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae1024_seed456/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae1024_seed456/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae1024_seed456/sae_final.pt",
      "cebench_matched_baseline_summary": "docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok30000_layer0_dsae2048_seed42",
      "token_budget": 30000,
      "hook_layer": 0,
      "hook_name": "blocks.0.hook_resid_pre",
      "d_sae": 2048,
      "seed": 42,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-15T17:39:08+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.0.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 30000,
          "d_sae": 2048,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 42,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae2048_seed42/sae_final.pt",
        "data": {
          "num_files_discovered": 113,
          "num_files_used": 16,
          "total_rows": 29432,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/113_movie_sent_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/114_nyc_borough_Manhattan_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/115_nyc_borough_Brooklyn_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/116_nyc_borough_Bronx_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/117_us_state_FL_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/118_us_state_CA_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/119_us_state_TX_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/120_us_timezone_Chicago_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/121_us_timezone_New_York_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/122_us_timezone_Los_Angeles_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/123_world_country_United_Kingdom_blocks.0.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 0.01843550056219101,
          "mse": 0.00048202962716459297,
          "aux": 0.01795347104780376,
          "l0": 32.0,
          "explained_variance": 0.9989387148129208
        }
      },
      "activation_file_count": 113,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-15T17:39:24+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae2048_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer0_dsae2048_seed42 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae2048_seed42/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae2048_seed42/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok30000_layer0_dsae2048_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "f14baa8a19b5e19c4bc596911e1535cb947dad0aed5407596d14620d00a96a64",
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok30000_layer0_dsae2048_seed42_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6605360072070458,
            "llm_test_auc": 0.6928164663947145,
            "llm_test_f1": 0.6491238078533221
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5917701302212305,
              "test_auc": 0.5965916268977953,
              "test_f1": 0.5062297606159099
            },
            {
              "k": 2,
              "test_accuracy": 0.5960193773385362,
              "test_auc": 0.607571299215313,
              "test_f1": 0.5239699492844394
            },
            {
              "k": 5,
              "test_accuracy": 0.6056306926034268,
              "test_auc": 0.6240643304453132,
              "test_f1": 0.5516114286478107
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6056306926034268,
            "test_auc": 0.6240643304453132,
            "test_f1": 0.5516114286478107
          },
          "best_minus_llm_auc": -0.06875213594940133
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-15T17:41:44+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae2048_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer0_dsae2048_seed42 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae2048_seed42/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "c5fa5269a3683a840ea91009314f91e838ce4d606d257416924e5aae7136e697",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae2048_seed42/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "14c5f86e7c8ff1edffada665973bdfb760f19ea3796db5ac499cbee6ae8b5def",
          "sae_release": "husai_scaling_tok30000_layer0_dsae2048_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae2048_seed42/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae2048_seed42/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae2048_seed42/cebench/interpretability_eval/husai_scaling_tok30000_layer0_dsae2048_seed42/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae2048_seed42/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 7.114756343364715,
          "independent_score_mean_max": 7.556517963409424,
          "interpretability_score_mean_max": 7.378561475276947,
          "sae_release": "husai_scaling_tok30000_layer0_dsae2048_seed42",
          "sae_id": "custom_sae",
          "date": "2026-02-15 17:41:44",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 7.114756343364715,
          "independent_score_mean_max": 7.556517963409424,
          "interpretability_score_mean_max": 7.378561475276947
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -43.39655108690262,
          "independent_score_mean_max": -43.442748384475706,
          "interpretability_score_mean_max": -40.57305011034012
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae2048_seed42/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae2048_seed42/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae2048_seed42/sae_final.pt",
      "cebench_matched_baseline_summary": "docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok30000_layer0_dsae2048_seed123",
      "token_budget": 30000,
      "hook_layer": 0,
      "hook_name": "blocks.0.hook_resid_pre",
      "d_sae": 2048,
      "seed": 123,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-15T17:41:57+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.0.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 30000,
          "d_sae": 2048,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 123,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae2048_seed123/sae_final.pt",
        "data": {
          "num_files_discovered": 113,
          "num_files_used": 16,
          "total_rows": 29432,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/113_movie_sent_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/114_nyc_borough_Manhattan_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/115_nyc_borough_Brooklyn_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/116_nyc_borough_Bronx_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/117_us_state_FL_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/118_us_state_CA_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/119_us_state_TX_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/120_us_timezone_Chicago_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/121_us_timezone_New_York_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/122_us_timezone_Los_Angeles_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/123_world_country_United_Kingdom_blocks.0.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 0.018548463005572557,
          "mse": 0.0004816643413505517,
          "aux": 0.018066798569634557,
          "l0": 32.0,
          "explained_variance": 0.9989396033926008
        }
      },
      "activation_file_count": 113,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-15T17:42:12+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae2048_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer0_dsae2048_seed123 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae2048_seed123/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae2048_seed123/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok30000_layer0_dsae2048_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "32ec9c2914ad7d9163c01a48422751ff75ce44f6a443bd4927dcb681f0bba1d2",
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok30000_layer0_dsae2048_seed123_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6605360072070458,
            "llm_test_auc": 0.6928164663947145,
            "llm_test_f1": 0.6491238078533221
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5853481078052883,
              "test_auc": 0.5971995705147919,
              "test_f1": 0.5091860191433084
            },
            {
              "k": 2,
              "test_accuracy": 0.5880445704635336,
              "test_auc": 0.6004987356799645,
              "test_f1": 0.5137818031907142
            },
            {
              "k": 5,
              "test_accuracy": 0.5985765586010483,
              "test_auc": 0.6122028089610242,
              "test_f1": 0.5500783069860061
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.5985765586010483,
            "test_auc": 0.6122028089610242,
            "test_f1": 0.5500783069860061
          },
          "best_minus_llm_auc": -0.08061365743369031
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-15T17:44:33+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae2048_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer0_dsae2048_seed123 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae2048_seed123/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "3aaf52b69396f356bb4c9345f17462c5aae28203326aa4455dd324ac862e5d5f",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae2048_seed123/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "cc0a6299dce8761413bd6957b24ef67c00b719e26303ce7749dadb5ead1dbdff",
          "sae_release": "husai_scaling_tok30000_layer0_dsae2048_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae2048_seed123/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae2048_seed123/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae2048_seed123/cebench/interpretability_eval/husai_scaling_tok30000_layer0_dsae2048_seed123/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae2048_seed123/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 7.12040402173996,
          "independent_score_mean_max": 7.925388143062592,
          "interpretability_score_mean_max": 7.48186362028122,
          "sae_release": "husai_scaling_tok30000_layer0_dsae2048_seed123",
          "sae_id": "custom_sae",
          "date": "2026-02-15 17:44:33",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 7.12040402173996,
          "independent_score_mean_max": 7.925388143062592,
          "interpretability_score_mean_max": 7.48186362028122
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -43.39090340852738,
          "independent_score_mean_max": -43.07387820482254,
          "interpretability_score_mean_max": -40.46974796533585
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae2048_seed123/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae2048_seed123/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae2048_seed123/sae_final.pt",
      "cebench_matched_baseline_summary": "docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok30000_layer0_dsae2048_seed456",
      "token_budget": 30000,
      "hook_layer": 0,
      "hook_name": "blocks.0.hook_resid_pre",
      "d_sae": 2048,
      "seed": 456,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-15T17:44:46+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.0.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 30000,
          "d_sae": 2048,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 456,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae2048_seed456/sae_final.pt",
        "data": {
          "num_files_discovered": 113,
          "num_files_used": 16,
          "total_rows": 29432,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/113_movie_sent_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/114_nyc_borough_Manhattan_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/115_nyc_borough_Brooklyn_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/116_nyc_borough_Bronx_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/117_us_state_FL_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/118_us_state_CA_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/119_us_state_TX_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/120_us_timezone_Chicago_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/121_us_timezone_New_York_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/122_us_timezone_Los_Angeles_blocks.0.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/123_world_country_United_Kingdom_blocks.0.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 0.018263207282871008,
          "mse": 0.0004765472222061362,
          "aux": 0.017786660231649876,
          "l0": 32.0,
          "explained_variance": 0.9989503501489568
        }
      },
      "activation_file_count": 113,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-15T17:45:01+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae2048_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer0_dsae2048_seed456 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae2048_seed456/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae2048_seed456/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok30000_layer0_dsae2048_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "3c4cfcfe2f505d59d3b91a9985b238e7365309364721d420022264344461dd87",
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok30000_layer0_dsae2048_seed456_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6605360072070458,
            "llm_test_auc": 0.6928164663947145,
            "llm_test_f1": 0.6491238078533221
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5879425482834627,
              "test_auc": 0.5918266378854241,
              "test_f1": 0.5269138666245236
            },
            {
              "k": 2,
              "test_accuracy": 0.5944542386905887,
              "test_auc": 0.5990448347294757,
              "test_f1": 0.5367319066780736
            },
            {
              "k": 5,
              "test_accuracy": 0.5989988405356267,
              "test_auc": 0.6021996326386087,
              "test_f1": 0.5354098693741438
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.5989988405356267,
            "test_auc": 0.6021996326386087,
            "test_f1": 0.5354098693741438
          },
          "best_minus_llm_auc": -0.09061683375610585
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-15T17:48:42+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae2048_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer0_dsae2048_seed456 --model-name pythia-70m-deduped --hook-layer 0 --hook-name blocks.0.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae2048_seed456/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200 --matched-baseline-summary /workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
        "config_hash": "ad18d667af1ce13fc4383accbe873ef9af7d19653c0af6876484b33dba85aee2",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae2048_seed456/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "f60113041bba4348003a2d2deaac5aa91deb470114a79ceadfc56f3e8593341e",
          "sae_release": "husai_scaling_tok30000_layer0_dsae2048_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 0,
          "hook_name": "blocks.0.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae2048_seed456/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": "/workspace/HUSAI/docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json"
        },
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae2048_seed456/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae2048_seed456/cebench/interpretability_eval/husai_scaling_tok30000_layer0_dsae2048_seed456/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae2048_seed456/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 6.934480469226838,
          "independent_score_mean_max": 7.731433269977569,
          "interpretability_score_mean_max": 7.233709025382995,
          "sae_release": "husai_scaling_tok30000_layer0_dsae2048_seed456",
          "sae_id": "custom_sae",
          "date": "2026-02-15 17:48:42",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 6.934480469226838,
          "independent_score_mean_max": 7.731433269977569,
          "interpretability_score_mean_max": 7.233709025382995
        },
        "matched_baseline_metrics": {
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066
        },
        "delta_vs_matched_baseline": {
          "contrastive_score_mean_max": -43.576826961040496,
          "independent_score_mean_max": -43.26783307790756,
          "interpretability_score_mean_max": -40.71790256023407
        },
        "matched_baseline_payload": {
          "output_folder": "results/experiments/phase4e_external_benchmark_official/cebench_matched200",
          "results_json": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/interpretability_eval/pythia-70m-deduped-res-sm/blocks.0.hook_resid_pre/results.json",
          "scores_dump": "results/experiments/phase4e_external_benchmark_official/cebench_matched200/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 50.51130743026734,
          "independent_score_mean_max": 50.99926634788513,
          "interpretability_score_mean_max": 47.951611585617066,
          "sae_release": "pythia-70m-deduped-res-sm",
          "sae_id": "blocks.0.hook_resid_pre",
          "date": "2026-02-13 18:59:31",
          "scores_dump_line_count": 200
        },
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae2048_seed456/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer0_dsae2048_seed456/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer0_dsae2048_seed456/sae_final.pt",
      "cebench_matched_baseline_summary": "docs/evidence/phase4e_cebench_matched200/cebench_matched200_summary.json",
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok30000_layer1_dsae1024_seed42",
      "token_budget": 30000,
      "hook_layer": 1,
      "hook_name": "blocks.1.hook_resid_pre",
      "d_sae": 1024,
      "seed": 42,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-15T17:48:54+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.1.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 30000,
          "d_sae": 1024,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 42,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae1024_seed42/sae_final.pt",
        "data": {
          "num_files_discovered": 16,
          "num_files_used": 16,
          "total_rows": 29432,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/113_movie_sent_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/114_nyc_borough_Manhattan_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/115_nyc_borough_Brooklyn_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/116_nyc_borough_Bronx_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/117_us_state_FL_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/118_us_state_CA_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/119_us_state_TX_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/120_us_timezone_Chicago_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/121_us_timezone_New_York_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/122_us_timezone_Los_Angeles_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/123_world_country_United_Kingdom_blocks.1.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 23.26072907447815,
          "mse": 0.08206475153565407,
          "aux": 23.178664445877075,
          "l0": 32.0,
          "explained_variance": 0.998272660888211
        }
      },
      "activation_file_count": 16,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-15T17:49:09+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae1024_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer1_dsae1024_seed42 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae1024_seed42/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae1024_seed42/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok30000_layer1_dsae1024_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "fecee9d89b67f4c3e2d7746b1ccb85d0313e16c6a863d6ce12bb07e4745fe08d",
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok30000_layer1_dsae1024_seed42_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6892726671847929,
            "llm_test_auc": 0.7249531199453926,
            "llm_test_f1": 0.6888365744296719
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5928992115289298,
              "test_auc": 0.6053345489546763,
              "test_f1": 0.5376086426432782
            },
            {
              "k": 2,
              "test_accuracy": 0.5962918122776254,
              "test_auc": 0.6166895951008803,
              "test_f1": 0.5609721113722823
            },
            {
              "k": 5,
              "test_accuracy": 0.6148064252879937,
              "test_auc": 0.6330983889271419,
              "test_f1": 0.5999645810815082
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6148064252879937,
            "test_auc": 0.6330983889271419,
            "test_f1": 0.5999645810815082
          },
          "best_minus_llm_auc": -0.0918547310182507
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-15T17:52:07+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae1024_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer1_dsae1024_seed42 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae1024_seed42/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200",
        "config_hash": "6c458c15f7ae124383ec331d26fa0d8c80d2a4e7ed41a12b7fd11497eddaac1f",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae1024_seed42/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "ae62f2465ca668a0a01904a1ed102a9729b92acf8355961559775a724ac4fd10",
          "sae_release": "husai_scaling_tok30000_layer1_dsae1024_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae1024_seed42/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": null
        },
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae1024_seed42/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae1024_seed42/cebench/interpretability_eval/husai_scaling_tok30000_layer1_dsae1024_seed42/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae1024_seed42/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 8.475596950054168,
          "independent_score_mean_max": 8.995376138687133,
          "interpretability_score_mean_max": 8.10938639640808,
          "sae_release": "husai_scaling_tok30000_layer1_dsae1024_seed42",
          "sae_id": "custom_sae",
          "date": "2026-02-15 17:52:07",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 8.475596950054168,
          "independent_score_mean_max": 8.995376138687133,
          "interpretability_score_mean_max": 8.10938639640808
        },
        "matched_baseline_metrics": null,
        "delta_vs_matched_baseline": null,
        "matched_baseline_payload": null,
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae1024_seed42/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae1024_seed42/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae1024_seed42/sae_final.pt",
      "cebench_matched_baseline_summary": null,
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok30000_layer1_dsae1024_seed123",
      "token_budget": 30000,
      "hook_layer": 1,
      "hook_name": "blocks.1.hook_resid_pre",
      "d_sae": 1024,
      "seed": 123,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-15T17:52:19+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.1.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 30000,
          "d_sae": 1024,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 123,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae1024_seed123/sae_final.pt",
        "data": {
          "num_files_discovered": 16,
          "num_files_used": 16,
          "total_rows": 29432,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/113_movie_sent_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/114_nyc_borough_Manhattan_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/115_nyc_borough_Brooklyn_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/116_nyc_borough_Bronx_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/117_us_state_FL_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/118_us_state_CA_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/119_us_state_TX_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/120_us_timezone_Chicago_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/121_us_timezone_New_York_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/122_us_timezone_Los_Angeles_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/123_world_country_United_Kingdom_blocks.1.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 22.864981174468994,
          "mse": 0.08093847054988146,
          "aux": 22.784042596817017,
          "l0": 32.0,
          "explained_variance": 0.9982959893654948
        }
      },
      "activation_file_count": 16,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-15T17:52:35+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae1024_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer1_dsae1024_seed123 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae1024_seed123/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae1024_seed123/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok30000_layer1_dsae1024_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "27b274765c673ef1d0f3900fdc349582b8c4470f10d3143f4df281953d5246f0",
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok30000_layer1_dsae1024_seed123_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6892726671847929,
            "llm_test_auc": 0.7249531199453926,
            "llm_test_f1": 0.6888365744296719
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5946997414596433,
              "test_auc": 0.6093530677280196,
              "test_f1": 0.5592797683104771
            },
            {
              "k": 2,
              "test_accuracy": 0.6005921492350107,
              "test_auc": 0.6202095282466353,
              "test_f1": 0.5707644410802721
            },
            {
              "k": 5,
              "test_accuracy": 0.6096729505054878,
              "test_auc": 0.6340908567941328,
              "test_f1": 0.5955278259324764
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6096729505054878,
            "test_auc": 0.6340908567941328,
            "test_f1": 0.5955278259324764
          },
          "best_minus_llm_auc": -0.09086226315125978
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-15T17:54:48+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae1024_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer1_dsae1024_seed123 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae1024_seed123/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200",
        "config_hash": "d1e9d548af483e9736c20dc68a9406cd19069e76978c8555901e92dd8e23e41a",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae1024_seed123/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "975388f4081462d2d24a26d7a4d598583a54f298b39916d435b7df6f6bdf9531",
          "sae_release": "husai_scaling_tok30000_layer1_dsae1024_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae1024_seed123/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": null
        },
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae1024_seed123/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae1024_seed123/cebench/interpretability_eval/husai_scaling_tok30000_layer1_dsae1024_seed123/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae1024_seed123/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 8.789426472187042,
          "independent_score_mean_max": 9.052521500587464,
          "interpretability_score_mean_max": 8.48562894821167,
          "sae_release": "husai_scaling_tok30000_layer1_dsae1024_seed123",
          "sae_id": "custom_sae",
          "date": "2026-02-15 17:54:48",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 8.789426472187042,
          "independent_score_mean_max": 9.052521500587464,
          "interpretability_score_mean_max": 8.48562894821167
        },
        "matched_baseline_metrics": null,
        "delta_vs_matched_baseline": null,
        "matched_baseline_payload": null,
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae1024_seed123/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae1024_seed123/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae1024_seed123/sae_final.pt",
      "cebench_matched_baseline_summary": null,
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok30000_layer1_dsae1024_seed456",
      "token_budget": 30000,
      "hook_layer": 1,
      "hook_name": "blocks.1.hook_resid_pre",
      "d_sae": 1024,
      "seed": 456,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-15T17:55:00+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.1.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 30000,
          "d_sae": 1024,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 456,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae1024_seed456/sae_final.pt",
        "data": {
          "num_files_discovered": 16,
          "num_files_used": 16,
          "total_rows": 29432,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/113_movie_sent_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/114_nyc_borough_Manhattan_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/115_nyc_borough_Brooklyn_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/116_nyc_borough_Bronx_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/117_us_state_FL_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/118_us_state_CA_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/119_us_state_TX_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/120_us_timezone_Chicago_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/121_us_timezone_New_York_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/122_us_timezone_Los_Angeles_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/123_world_country_United_Kingdom_blocks.1.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 23.93584656715393,
          "mse": 0.08592038508504629,
          "aux": 23.849926233291626,
          "l0": 32.0,
          "explained_variance": 0.9981928888569956
        }
      },
      "activation_file_count": 16,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-15T17:55:16+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae1024_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer1_dsae1024_seed456 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae1024_seed456/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae1024_seed456/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok30000_layer1_dsae1024_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "ed73a2a2c12d0d2dda92221d97798ff177e1ef3cc14f010f67578a0a3514423f",
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok30000_layer1_dsae1024_seed456_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6892726671847929,
            "llm_test_auc": 0.7249531199453926,
            "llm_test_f1": 0.6888365744296719
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5978247908704022,
              "test_auc": 0.610535001610099,
              "test_f1": 0.5617475113271754
            },
            {
              "k": 2,
              "test_accuracy": 0.5988035916055388,
              "test_auc": 0.6227141520136907,
              "test_f1": 0.5611541955651869
            },
            {
              "k": 5,
              "test_accuracy": 0.6144484311123015,
              "test_auc": 0.6416989705768275,
              "test_f1": 0.6023428247402924
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6144484311123015,
            "test_auc": 0.6416989705768275,
            "test_f1": 0.6023428247402924
          },
          "best_minus_llm_auc": -0.08325414936856512
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-15T17:57:28+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae1024_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer1_dsae1024_seed456 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae1024_seed456/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200",
        "config_hash": "e67548242da1a2c06a3f7b9e8f17b258dba6c38b3f23e2d6aba3c260c0c8dff2",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae1024_seed456/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "3e2f3848f0f8df5d27e5eaff76f5617eda98a4cf9b780d8fc7808edaae11f296",
          "sae_release": "husai_scaling_tok30000_layer1_dsae1024_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae1024_seed456/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": null
        },
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 1024,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae1024_seed456/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae1024_seed456/cebench/interpretability_eval/husai_scaling_tok30000_layer1_dsae1024_seed456/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae1024_seed456/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 8.613990087509155,
          "independent_score_mean_max": 9.176166086196899,
          "interpretability_score_mean_max": 8.4096173620224,
          "sae_release": "husai_scaling_tok30000_layer1_dsae1024_seed456",
          "sae_id": "custom_sae",
          "date": "2026-02-15 17:57:28",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 8.613990087509155,
          "independent_score_mean_max": 9.176166086196899,
          "interpretability_score_mean_max": 8.4096173620224
        },
        "matched_baseline_metrics": null,
        "delta_vs_matched_baseline": null,
        "matched_baseline_payload": null,
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae1024_seed456/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae1024_seed456/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae1024_seed456/sae_final.pt",
      "cebench_matched_baseline_summary": null,
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok30000_layer1_dsae2048_seed42",
      "token_budget": 30000,
      "hook_layer": 1,
      "hook_name": "blocks.1.hook_resid_pre",
      "d_sae": 2048,
      "seed": 42,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-15T17:57:40+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.1.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 30000,
          "d_sae": 2048,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 42,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae2048_seed42/sae_final.pt",
        "data": {
          "num_files_discovered": 16,
          "num_files_used": 16,
          "total_rows": 29432,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/113_movie_sent_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/114_nyc_borough_Manhattan_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/115_nyc_borough_Brooklyn_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/116_nyc_borough_Bronx_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/117_us_state_FL_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/118_us_state_CA_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/119_us_state_TX_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/120_us_timezone_Chicago_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/121_us_timezone_New_York_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/122_us_timezone_Los_Angeles_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/123_world_country_United_Kingdom_blocks.1.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 25.0125412940979,
          "mse": 0.0762296924367547,
          "aux": 24.93631148338318,
          "l0": 32.0,
          "explained_variance": 0.9983954800719961
        }
      },
      "activation_file_count": 16,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-15T17:57:57+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae2048_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer1_dsae2048_seed42 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae2048_seed42/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae2048_seed42/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok30000_layer1_dsae2048_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "63edc294e4bf3eb029a24cdcf0a54c209e8c7d9cffdf8af3b41fbdfd51b0eff5",
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok30000_layer1_dsae2048_seed42_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6892726671847929,
            "llm_test_auc": 0.7249531199453926,
            "llm_test_f1": 0.6888365744296719
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5815366763566039,
              "test_auc": 0.5964915216637288,
              "test_f1": 0.5302673336345135
            },
            {
              "k": 2,
              "test_accuracy": 0.583767475799262,
              "test_auc": 0.602648481204297,
              "test_f1": 0.5386672207659978
            },
            {
              "k": 5,
              "test_accuracy": 0.606391767279692,
              "test_auc": 0.629366816923878,
              "test_f1": 0.5832864755774729
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.606391767279692,
            "test_auc": 0.629366816923878,
            "test_f1": 0.5832864755774729
          },
          "best_minus_llm_auc": -0.09558630302151461
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-15T18:00:20+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae2048_seed42/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer1_dsae2048_seed42 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae2048_seed42/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200",
        "config_hash": "6c70029dbc926c6be21f5c883d103ea14b68f006d74a1d75696bf2f737bf2a99",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae2048_seed42/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "053b72fbd072223138be2a160e1c515eea1f9990f9e83dfb01ad35f85625d1ce",
          "sae_release": "husai_scaling_tok30000_layer1_dsae2048_seed42",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae2048_seed42/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": null
        },
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae2048_seed42/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae2048_seed42/cebench/interpretability_eval/husai_scaling_tok30000_layer1_dsae2048_seed42/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae2048_seed42/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 10.57554366350174,
          "independent_score_mean_max": 11.559704933166504,
          "interpretability_score_mean_max": 10.552940266132355,
          "sae_release": "husai_scaling_tok30000_layer1_dsae2048_seed42",
          "sae_id": "custom_sae",
          "date": "2026-02-15 18:00:20",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 10.57554366350174,
          "independent_score_mean_max": 11.559704933166504,
          "interpretability_score_mean_max": 10.552940266132355
        },
        "matched_baseline_metrics": null,
        "delta_vs_matched_baseline": null,
        "matched_baseline_payload": null,
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae2048_seed42/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae2048_seed42/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae2048_seed42/sae_final.pt",
      "cebench_matched_baseline_summary": null,
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok30000_layer1_dsae2048_seed123",
      "token_budget": 30000,
      "hook_layer": 1,
      "hook_name": "blocks.1.hook_resid_pre",
      "d_sae": 2048,
      "seed": 123,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-15T18:00:33+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.1.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 30000,
          "d_sae": 2048,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 123,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae2048_seed123/sae_final.pt",
        "data": {
          "num_files_discovered": 16,
          "num_files_used": 16,
          "total_rows": 29432,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/113_movie_sent_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/114_nyc_borough_Manhattan_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/115_nyc_borough_Brooklyn_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/116_nyc_borough_Bronx_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/117_us_state_FL_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/118_us_state_CA_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/119_us_state_TX_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/120_us_timezone_Chicago_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/121_us_timezone_New_York_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/122_us_timezone_Los_Angeles_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/123_world_country_United_Kingdom_blocks.1.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 25.16940712928772,
          "mse": 0.0759917525574565,
          "aux": 25.09341549873352,
          "l0": 32.0,
          "explained_variance": 0.9984001334147674
        }
      },
      "activation_file_count": 16,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-15T18:00:48+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae2048_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer1_dsae2048_seed123 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae2048_seed123/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae2048_seed123/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok30000_layer1_dsae2048_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "4152c35c9ee90dd23f15a7497caac67fa1294f8866a8e9523cb065262c615537",
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok30000_layer1_dsae2048_seed123_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6892726671847929,
            "llm_test_auc": 0.7249531199453926,
            "llm_test_f1": 0.6888365744296719
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5903520764225163,
              "test_auc": 0.5998283166460755,
              "test_f1": 0.5483052332854206
            },
            {
              "k": 2,
              "test_accuracy": 0.5935353183574069,
              "test_auc": 0.6104453717338384,
              "test_f1": 0.5563485862383424
            },
            {
              "k": 5,
              "test_accuracy": 0.6021654189195628,
              "test_auc": 0.6302437785422621,
              "test_f1": 0.5833583091463332
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6021654189195628,
            "test_auc": 0.6302437785422621,
            "test_f1": 0.5833583091463332
          },
          "best_minus_llm_auc": -0.09470934140313048
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-15T18:03:08+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae2048_seed123/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer1_dsae2048_seed123 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae2048_seed123/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200",
        "config_hash": "697d8cd83d391e67728461f32c4faf47ac02772bbd5502a60cbe2271cea278a8",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae2048_seed123/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "fd2e592b1559caf22f6ddbca3e1f5fd64237de130f659ea0e5bca6cf8cf7a56f",
          "sae_release": "husai_scaling_tok30000_layer1_dsae2048_seed123",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae2048_seed123/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": null
        },
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae2048_seed123/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae2048_seed123/cebench/interpretability_eval/husai_scaling_tok30000_layer1_dsae2048_seed123/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae2048_seed123/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 10.810209999084472,
          "independent_score_mean_max": 11.617531554698944,
          "interpretability_score_mean_max": 10.472996156215668,
          "sae_release": "husai_scaling_tok30000_layer1_dsae2048_seed123",
          "sae_id": "custom_sae",
          "date": "2026-02-15 18:03:08",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 10.810209999084472,
          "independent_score_mean_max": 11.617531554698944,
          "interpretability_score_mean_max": 10.472996156215668
        },
        "matched_baseline_metrics": null,
        "delta_vs_matched_baseline": null,
        "matched_baseline_payload": null,
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae2048_seed123/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae2048_seed123/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae2048_seed123/sae_final.pt",
      "cebench_matched_baseline_summary": null,
      "saebench_returncode": 0,
      "cebench_returncode": 0
    },
    {
      "condition_id": "tok30000_layer1_dsae2048_seed456",
      "token_budget": 30000,
      "hook_layer": 1,
      "hook_name": "blocks.1.hook_resid_pre",
      "d_sae": 2048,
      "seed": 456,
      "train_returncode": 0,
      "train_summary": {
        "timestamp_utc": "2026-02-15T18:03:20+00:00",
        "config": {
          "activation_cache_dir": "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped",
          "activation_glob": "*_blocks.1.hook_resid_pre.pt",
          "max_files": 16,
          "max_rows_per_file": 2048,
          "max_total_rows": 30000,
          "d_sae": 2048,
          "k": 32,
          "epochs": 6,
          "batch_size": 4096,
          "learning_rate": 0.001,
          "seed": 456,
          "device": "cuda"
        },
        "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae2048_seed456/sae_final.pt",
        "data": {
          "num_files_discovered": 16,
          "num_files_used": 16,
          "total_rows": 29432,
          "d_model": 512
        },
        "source_files": [
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/100_news_fake_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/105_click_bait_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/106_hate_hate_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/107_hate_offensive_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/110_aimade_humangpt3_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/113_movie_sent_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/114_nyc_borough_Manhattan_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/115_nyc_borough_Brooklyn_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/116_nyc_borough_Bronx_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/117_us_state_FL_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/118_us_state_CA_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/119_us_state_TX_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/120_us_timezone_Chicago_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/121_us_timezone_New_York_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/122_us_timezone_Los_Angeles_blocks.1.hook_resid_pre.pt",
          "/tmp/sae_bench_model_cache/model_activations_pythia-70m-deduped/123_world_country_United_Kingdom_blocks.1.hook_resid_pre.pt"
        ],
        "final_metrics": {
          "epoch": 6,
          "loss": 25.053161144256592,
          "mse": 0.0764489434659481,
          "aux": 24.976712226867676,
          "l0": 32.0,
          "explained_variance": 0.9983920959214571
        }
      },
      "activation_file_count": 16,
      "dataset_names": [
        "100_news_fake",
        "105_click_bait",
        "106_hate_hate",
        "107_hate_offensive",
        "110_aimade_humangpt3",
        "113_movie_sent",
        "114_nyc_borough_Manhattan",
        "115_nyc_borough_Brooklyn",
        "116_nyc_borough_Bronx",
        "117_us_state_FL",
        "118_us_state_CA",
        "119_us_state_TX",
        "120_us_timezone_Chicago",
        "121_us_timezone_New_York",
        "122_us_timezone_Los_Angeles",
        "123_world_country_United_Kingdom"
      ],
      "saebench": {
        "timestamp_utc": "2026-02-15T18:03:36+00:00",
        "command": "python scripts/experiments/run_husai_saebench_custom_eval.py --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae2048_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer1_dsae2048_seed456 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --dtype float32 --results-path /tmp/husai_saebench_probe_results_scaling_multiseed --model-cache-path /tmp/sae_bench_model_cache --output-dir /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae2048_seed456/saebench --force-rerun --dataset-names 100_news_fake,105_click_bait,106_hate_hate,107_hate_offensive,110_aimade_humangpt3,113_movie_sent,114_nyc_borough_Manhattan,115_nyc_borough_Brooklyn,116_nyc_borough_Bronx,117_us_state_FL,118_us_state_CA,119_us_state_TX,120_us_timezone_Chicago,121_us_timezone_New_York,122_us_timezone_Los_Angeles,123_world_country_United_Kingdom",
        "config": {
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae2048_seed456/sae_final.pt",
          "architecture_override": "topk",
          "sae_release": "husai_scaling_tok30000_layer1_dsae2048_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "reg_type": "l1",
          "setting": "normal",
          "ks": [
            1,
            2,
            5
          ],
          "dataset_names": [
            "100_news_fake",
            "105_click_bait",
            "106_hate_hate",
            "107_hate_offensive",
            "110_aimade_humangpt3",
            "113_movie_sent",
            "114_nyc_borough_Manhattan",
            "115_nyc_borough_Brooklyn",
            "116_nyc_borough_Bronx",
            "117_us_state_FL",
            "118_us_state_CA",
            "119_us_state_TX",
            "120_us_timezone_Chicago",
            "121_us_timezone_New_York",
            "122_us_timezone_Los_Angeles",
            "123_world_country_United_Kingdom"
          ],
          "dataset_names_inferred_from_cache": false,
          "dataset_count": 16,
          "binarize": false,
          "device": "cuda",
          "dtype": "float32",
          "results_path": "/tmp/husai_saebench_probe_results_scaling_multiseed",
          "model_cache_path": "/tmp/sae_bench_model_cache",
          "force_rerun": true
        },
        "config_hash": "461a4b1bb8fd6cdbf155813f28ebc520a76e5487f7480b5cd934f2e71ddf58e0",
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "summary": {
          "result_key": "husai_scaling_tok30000_layer1_dsae2048_seed456_custom_sae",
          "llm_metrics": {
            "llm_test_accuracy": 0.6892726671847929,
            "llm_test_auc": 0.7249531199453926,
            "llm_test_f1": 0.6888365744296719
          },
          "sae_metrics_by_k": [
            {
              "k": 1,
              "test_accuracy": 0.5925619807766261,
              "test_auc": 0.6056496069221368,
              "test_f1": 0.5459900533663105
            },
            {
              "k": 2,
              "test_accuracy": 0.5984210712552194,
              "test_auc": 0.6183308386928116,
              "test_f1": 0.5721518989023663
            },
            {
              "k": 5,
              "test_accuracy": 0.6068469104416733,
              "test_auc": 0.6308327485164374,
              "test_f1": 0.5781166001821632
            }
          ],
          "best_by_auc": {
            "k": 5,
            "test_accuracy": 0.6068469104416733,
            "test_auc": 0.6308327485164374,
            "test_f1": 0.5781166001821632
          },
          "best_minus_llm_auc": -0.09412037142895513
        }
      },
      "cebench": {
        "timestamp_utc": "2026-02-15T18:05:56+00:00",
        "command": "python scripts/experiments/run_husai_cebench_custom_eval.py --cebench-repo /workspace/CE-Bench --checkpoint /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae2048_seed456/sae_final.pt --architecture topk --sae-release husai_scaling_tok30000_layer1_dsae2048_seed456 --model-name pythia-70m-deduped --hook-layer 1 --hook-name blocks.1.hook_resid_pre --device cuda --sae-dtype float32 --output-folder /workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae2048_seed456/cebench --artifacts-path /tmp/ce_bench_artifacts_scaling_multiseed --max-rows 200",
        "config_hash": "6d42d8732eeb92290286489500e59d60f8fef589434538f4e6f472c172e09099",
        "config": {
          "cebench_repo": "/workspace/CE-Bench",
          "checkpoint": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae2048_seed456/sae_final.pt",
          "architecture_override": "topk",
          "checkpoint_sha256": "340eff87801eb327d751b3bf9b49cdb9925b1531ebb342cd0263ef1fab8138a5",
          "sae_release": "husai_scaling_tok30000_layer1_dsae2048_seed456",
          "model_name": "pythia-70m-deduped",
          "hook_layer": 1,
          "hook_name": "blocks.1.hook_resid_pre",
          "device": "cuda",
          "sae_dtype": "float32",
          "llm_batch_size": 512,
          "llm_dtype": "float32",
          "random_seed": 42,
          "dataset_name": "GulkoA/contrastive-stories-v4",
          "dataset_rows_total": 5000,
          "dataset_rows_used": 200,
          "max_rows": 200,
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae2048_seed456/cebench",
          "artifacts_path": "/tmp/ce_bench_artifacts_scaling_multiseed",
          "matched_baseline_summary": null
        },
        "sae_meta": {
          "architecture": "topk",
          "eval_architecture": "topk",
          "d_model": 512,
          "d_sae": 2048,
          "k": 32
        },
        "cebench_summary": {
          "output_folder": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae2048_seed456/cebench",
          "results_json": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae2048_seed456/cebench/interpretability_eval/husai_scaling_tok30000_layer1_dsae2048_seed456/custom_sae/results.json",
          "scores_dump": "/workspace/HUSAI/results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae2048_seed456/cebench/scores_dump.txt",
          "total_rows": 200,
          "contrastive_score_mean_max": 9.5802614569664,
          "independent_score_mean_max": 10.785375874042511,
          "interpretability_score_mean_max": 9.53832999944687,
          "sae_release": "husai_scaling_tok30000_layer1_dsae2048_seed456",
          "sae_id": "custom_sae",
          "date": "2026-02-15 18:05:56",
          "scores_dump_line_count": 200
        },
        "custom_metrics": {
          "contrastive_score_mean_max": 9.5802614569664,
          "independent_score_mean_max": 10.785375874042511,
          "interpretability_score_mean_max": 9.53832999944687
        },
        "matched_baseline_metrics": null,
        "delta_vs_matched_baseline": null,
        "matched_baseline_payload": null,
        "artifacts": {
          "cebench_metrics_summary_json": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae2048_seed456/cebench/cebench_metrics_summary.json",
          "cebench_metrics_summary_md": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/external_eval/tok30000_layer1_dsae2048_seed456/cebench/cebench_metrics_summary.md"
        }
      },
      "checkpoint": "results/experiments/phase4e_external_scaling_study_multiseed/run_20260215T165725Z/checkpoints/tok30000_layer1_dsae2048_seed456/sae_final.pt",
      "cebench_matched_baseline_summary": null,
      "saebench_returncode": 0,
      "cebench_returncode": 0
    }
  ],
  "aggregates": {
    "by_token_budget": {
      "10000": {
        "saebench_best_minus_llm_auc": {
          "mean": -0.08629052824435675,
          "std": 0.012793609002197916,
          "min": -0.10464225163078833,
          "max": -0.06509338803900744,
          "n": 12
        },
        "cebench_interpretability_max": {
          "mean": 7.862272895475228,
          "std": 1.4915234425128283,
          "min": 6.017686002254486,
          "max": 10.38789572238922,
          "n": 12
        },
        "cebench_interp_delta_vs_baseline": {
          "mean": -41.24516555726528,
          "std": 0.7024288429051168,
          "min": -41.93392558336258,
          "max": -40.446475937366486,
          "n": 6
        }
      },
      "30000": {
        "saebench_best_minus_llm_auc": {
          "mean": -0.08513235397555448,
          "std": 0.009275035438460156,
          "min": -0.09558630302151461,
          "max": -0.06875213594940133,
          "n": 12
        },
        "cebench_interpretability_max": {
          "mean": 8.026932807564735,
          "std": 1.544148448289346,
          "min": 6.07377854347229,
          "max": 10.552940266132355,
          "n": 12
        },
        "cebench_interp_delta_vs_baseline": {
          "mean": -41.159229158560436,
          "std": 0.6436989376816211,
          "min": -41.87783304214477,
          "max": -40.46974796533585,
          "n": 6
        }
      }
    },
    "by_hook_layer": {
      "0": {
        "saebench_best_minus_llm_auc": {
          "mean": -0.07742713722520761,
          "std": 0.008628240396075121,
          "min": -0.09370917118906608,
          "max": -0.06509338803900744,
          "n": 12
        },
        "cebench_interpretability_max": {
          "mean": 6.749414227704207,
          "std": 0.6439175697142968,
          "min": 6.017686002254486,
          "max": 7.5051356482505795,
          "n": 12
        },
        "cebench_interp_delta_vs_baseline": {
          "mean": -41.20219735791286,
          "std": 0.643917569714296,
          "min": -41.93392558336258,
          "max": -40.446475937366486,
          "n": 12
        }
      },
      "1": {
        "saebench_best_minus_llm_auc": {
          "mean": -0.0939957449947036,
          "std": 0.005123507500565954,
          "min": -0.10464225163078833,
          "max": -0.08325414936856512,
          "n": 12
        },
        "cebench_interpretability_max": {
          "mean": 9.139791475335757,
          "std": 1.045237315836624,
          "min": 7.965026619434357,
          "max": 10.552940266132355,
          "n": 12
        },
        "cebench_interp_delta_vs_baseline": {
          "mean": null,
          "std": null,
          "min": null,
          "max": null,
          "n": 0
        }
      }
    },
    "by_d_sae": {
      "1024": {
        "saebench_best_minus_llm_auc": {
          "mean": -0.08212220067594521,
          "std": 0.010149011936029009,
          "min": -0.09526819738578596,
          "max": -0.06509338803900744,
          "n": 12
        },
        "cebench_interpretability_max": {
          "mean": 7.1673101575175915,
          "std": 1.0807816623762538,
          "min": 6.017686002254486,
          "max": 8.48562894821167,
          "n": 12
        },
        "cebench_interp_delta_vs_baseline": {
          "mean": -41.80495357771715,
          "std": 0.15868657331024685,
          "min": -41.93392558336258,
          "max": -41.51144028425217,
          "n": 6
        }
      },
      "2048": {
        "saebench_best_minus_llm_auc": {
          "mean": -0.08930068154396602,
          "std": 0.01092356774416891,
          "min": -0.10464225163078833,
          "max": -0.06875213594940133,
          "n": 12
        },
        "cebench_interpretability_max": {
          "mean": 8.721895545522372,
          "std": 1.4620249543451833,
          "min": 7.224240756034851,
          "max": 10.552940266132355,
          "n": 12
        },
        "cebench_interp_delta_vs_baseline": {
          "mean": -40.599441138108574,
          "std": 0.12267056171440728,
          "min": -40.72737082958221,
          "max": -40.446475937366486,
          "n": 6
        }
      }
    }
  }
}
