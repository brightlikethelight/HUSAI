{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLM SAE Stability Analysis\n",
        "\n",
        "This notebook tests whether our stability findings from algorithmic tasks transfer to LLMs.\n",
        "\n",
        "**Key Finding to Verify:**\n",
        "On algorithmic tasks, stability DECREASES monotonically with L0 (sparsity).\n",
        "\n",
        "**Question:**\n",
        "Does this hold for LLMs, or do LLMs show a different pattern (potentially with an optimal L0)?\n",
        "\n",
        "**Setup:**\n",
        "1. Make sure GPU is enabled: Runtime > Change runtime type > GPU\n",
        "2. Run all cells in order"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install sae-lens transformer-lens transformers -q"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check GPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using device: {device}')\n",
        "if device == 'cuda':\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sae_lens import SAE\n",
        "\n",
        "def compute_pwmcc(d1, d2):\n",
        "    \"\"\"Compute PWMCC between two decoder matrices.\"\"\"\n",
        "    d1_norm = F.normalize(d1, dim=1)\n",
        "    d2_norm = F.normalize(d2, dim=1)\n",
        "    cos_sim = d1_norm @ d2_norm.T\n",
        "    max_1to2 = cos_sim.abs().max(dim=1)[0].mean().item()\n",
        "    max_2to1 = cos_sim.abs().max(dim=0)[0].mean().item()\n",
        "    return (max_1to2 + max_2to1) / 2\n",
        "\n",
        "def compute_random_baseline(d_model, d_sae, n_trials=10):\n",
        "    \"\"\"Compute random PWMCC baseline.\"\"\"\n",
        "    pwmcc_values = []\n",
        "    for _ in range(n_trials):\n",
        "        d1 = torch.randn(d_sae, d_model, device=device)\n",
        "        d2 = torch.randn(d_sae, d_model, device=device)\n",
        "        pwmcc_values.append(compute_pwmcc(d1, d2))\n",
        "    return np.mean(pwmcc_values)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Gemma Scope SAEs\n",
        "\n",
        "We'll load SAEs at different widths (which correspond to different sparsity levels) and compare their features."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SAEs at different widths for the same layer\n",
        "layer = 12\n",
        "widths = ['16k', '32k', '65k']\n",
        "\n",
        "saes = {}\n",
        "for width in widths:\n",
        "    print(f'Loading width {width}...')\n",
        "    try:\n",
        "        sae_id = f'layer_{layer}/width_{width}/canonical'\n",
        "        sae, cfg, sparsity = SAE.from_pretrained(\n",
        "            release='gemma-scope-2b-pt-res-canonical',\n",
        "            sae_id=sae_id,\n",
        "            device=device\n",
        "        )\n",
        "        saes[width] = {\n",
        "            'sae': sae,\n",
        "            'd_sae': sae.cfg.d_sae,\n",
        "            'd_model': sae.cfg.d_in,\n",
        "            'sparsity': sparsity\n",
        "        }\n",
        "        print(f'  ✓ d_sae={sae.cfg.d_sae}, d_model={sae.cfg.d_in}')\n",
        "    except Exception as e:\n",
        "        print(f'  ✗ Error: {e}')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare SAE Features Across Widths\n",
        "\n",
        "This tells us how similar the learned features are at different dictionary sizes."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print('Comparing SAE features across widths:')\n",
        "print('-' * 50)\n",
        "\n",
        "results = []\n",
        "width_list = list(saes.keys())\n",
        "\n",
        "for i, w1 in enumerate(width_list):\n",
        "    for w2 in width_list[i+1:]:\n",
        "        # Get decoder weights\n",
        "        d1 = saes[w1]['sae'].W_dec.data\n",
        "        d2 = saes[w2]['sae'].W_dec.data\n",
        "        \n",
        "        # For different sizes, we compute how many features in the smaller\n",
        "        # SAE have good matches in the larger SAE\n",
        "        d1_norm = F.normalize(d1, dim=1)\n",
        "        d2_norm = F.normalize(d2, dim=1)\n",
        "        \n",
        "        cos_sim = d1_norm @ d2_norm.T\n",
        "        \n",
        "        # For each feature in smaller SAE, find best match in larger\n",
        "        max_sim_1to2 = cos_sim.abs().max(dim=1)[0]\n",
        "        \n",
        "        # Statistics\n",
        "        mean_max_sim = max_sim_1to2.mean().item()\n",
        "        pct_above_90 = (max_sim_1to2 > 0.9).float().mean().item() * 100\n",
        "        pct_above_80 = (max_sim_1to2 > 0.8).float().mean().item() * 100\n",
        "        \n",
        "        print(f'{w1} → {w2}:')\n",
        "        print(f'  Mean max cosine sim: {mean_max_sim:.3f}')\n",
        "        print(f'  % features with >0.9 match: {pct_above_90:.1f}%')\n",
        "        print(f'  % features with >0.8 match: {pct_above_80:.1f}%')\n",
        "        \n",
        "        results.append({\n",
        "            'width1': w1,\n",
        "            'width2': w2,\n",
        "            'mean_max_sim': mean_max_sim,\n",
        "            'pct_above_90': pct_above_90,\n",
        "            'pct_above_80': pct_above_80\n",
        "        })"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Multiple SAEs with Different Seeds\n",
        "\n",
        "To properly test stability, we need to train SAEs with the same config but different random seeds. This is more compute-intensive but gives us the true stability measure."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from transformer_lens import HookedTransformer\n",
        "from sae_lens import SAE, SAEConfig, SAETrainingRunner, LanguageModelSAERunnerConfig\n",
        "\n",
        "# This is a template for training SAEs with different seeds\n",
        "# Uncomment and modify as needed\n",
        "\n",
        "'''\n",
        "# Load model\n",
        "model = HookedTransformer.from_pretrained('gemma-2-2b', device=device)\n",
        "\n",
        "# Training config\n",
        "cfg = LanguageModelSAERunnerConfig(\n",
        "    model_name='gemma-2-2b',\n",
        "    hook_point='blocks.12.hook_resid_post',\n",
        "    hook_point_layer=12,\n",
        "    d_in=2304,  # Gemma 2 2B hidden size\n",
        "    dataset_path='monology/pile-uncopyrighted',\n",
        "    streaming=True,\n",
        "    \n",
        "    # SAE config\n",
        "    expansion_factor=8,  # d_sae = 8 * d_in\n",
        "    b_dec_init_method='zeros',\n",
        "    \n",
        "    # Training\n",
        "    lr=3e-4,\n",
        "    l1_coefficient=5e-3,  # Vary this for different sparsity\n",
        "    train_batch_size_tokens=4096,\n",
        "    context_size=128,\n",
        "    \n",
        "    # For stability testing, train with different seeds\n",
        "    seed=42,  # Change this for each run\n",
        "    \n",
        "    n_batches_in_buffer=64,\n",
        "    total_training_tokens=1_000_000,  # Small for testing\n",
        "    store_batch_size_prompts=16,\n",
        "    \n",
        "    log_to_wandb=False,\n",
        "    wandb_project='sae-stability',\n",
        ")\n",
        "\n",
        "# Train\n",
        "runner = SAETrainingRunner(cfg)\n",
        "sae = runner.run()\n",
        "'''\n",
        "\n",
        "print('Training template ready. Uncomment and modify as needed.')\n",
        "print('For full stability analysis, train 3-5 SAEs with different seeds.')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze Cross-Layer Stability\n",
        "\n",
        "How do SAE features change across layers?"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SAEs at different layers\n",
        "layers = [6, 12, 18]\n",
        "width = '16k'\n",
        "\n",
        "layer_saes = {}\n",
        "for layer in layers:\n",
        "    print(f'Loading layer {layer}...')\n",
        "    try:\n",
        "        sae_id = f'layer_{layer}/width_{width}/canonical'\n",
        "        sae, cfg, sparsity = SAE.from_pretrained(\n",
        "            release='gemma-scope-2b-pt-res-canonical',\n",
        "            sae_id=sae_id,\n",
        "            device=device\n",
        "        )\n",
        "        layer_saes[layer] = sae\n",
        "        print(f'  ✓ Loaded')\n",
        "    except Exception as e:\n",
        "        print(f'  ✗ Error: {e}')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Cross-layer feature similarity:')\n",
        "print('-' * 50)\n",
        "\n",
        "layer_list = sorted(layer_saes.keys())\n",
        "for i in range(len(layer_list) - 1):\n",
        "    l1, l2 = layer_list[i], layer_list[i+1]\n",
        "    \n",
        "    d1 = layer_saes[l1].W_dec.data\n",
        "    d2 = layer_saes[l2].W_dec.data\n",
        "    \n",
        "    pwmcc = compute_pwmcc(d1, d2)\n",
        "    random_baseline = compute_random_baseline(d1.shape[1], d1.shape[0])\n",
        "    \n",
        "    print(f'Layer {l1} vs {l2}:')\n",
        "    print(f'  PWMCC: {pwmcc:.4f}')\n",
        "    print(f'  Random baseline: {random_baseline:.4f}')\n",
        "    print(f'  Ratio: {pwmcc/random_baseline:.2f}×')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "**Key Observations:**\n",
        "\n",
        "1. **Width comparison**: Features at smaller widths tend to have good matches in larger widths (feature splitting)\n",
        "\n",
        "2. **Cross-layer**: Adjacent layers have some feature overlap, but it decreases with layer distance\n",
        "\n",
        "3. **To fully test our hypothesis**: We need to train multiple SAEs with the SAME config but DIFFERENT seeds, then measure PWMCC between them\n",
        "\n",
        "**Next Steps:**\n",
        "1. Train 3-5 SAEs with different seeds at the same layer/width\n",
        "2. Vary L1 coefficient to get different sparsity levels\n",
        "3. Measure stability (PWMCC) at each sparsity level\n",
        "4. Compare to our algorithmic task findings"
      ],
      "metadata": {}
    }
  ]
}
