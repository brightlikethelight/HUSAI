{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ”¬ LLM SAE Stability Analysis - Production Grade\n",
        "\n",
        "**Research Question:** Does SAE stability decrease monotonically with sparsity on LLMs, as we found on algorithmic tasks?\n",
        "\n",
        "## Key Findings from Algorithmic Tasks\n",
        "- Stability DECREASES monotonically with L0 (sparsity)\n",
        "- This is architecture-independent (TopK, ReLU, Gated, JumpReLU)\n",
        "- Correlation: -0.725 (strong negative)\n",
        "\n",
        "## This Notebook Tests\n",
        "1. **Pretrained SAE Analysis**: Compare Gemma Scope SAEs at different widths\n",
        "2. **Multi-Seed Training**: Train SAEs with same config, different seeds\n",
        "3. **Stability vs Sparsity**: Vary L1 coefficient and measure PWMCC\n",
        "\n",
        "## Setup\n",
        "1. **Enable GPU**: Runtime > Change runtime type > GPU (T4 is sufficient)\n",
        "2. **Run all cells** in order\n",
        "3. **Expected runtime**: ~30-60 minutes for full analysis"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Install Dependencies\n",
        "!pip install sae-lens transformer-lens transformers datasets -q\n",
        "!pip install einops jaxtyping -q"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Imports and GPU Check\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "import gc\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Check GPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Device: {device}')\n",
        "if device == 'cuda':\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
        "else:\n",
        "    print('âš ï¸ No GPU detected! Enable GPU: Runtime > Change runtime type > GPU')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3. Utility Functions\n",
        "\n",
        "def compute_pwmcc(d1: torch.Tensor, d2: torch.Tensor) -> float:\n",
        "    \"\"\"Compute Pairwise Maximum Cosine Correlation between decoder matrices.\n",
        "    \n",
        "    Args:\n",
        "        d1: [d_sae, d_model] decoder matrix\n",
        "        d2: [d_sae, d_model] decoder matrix\n",
        "    \n",
        "    Returns:\n",
        "        PWMCC in [0, 1]\n",
        "    \"\"\"\n",
        "    d1_norm = F.normalize(d1, dim=1)\n",
        "    d2_norm = F.normalize(d2, dim=1)\n",
        "    cos_sim = d1_norm @ d2_norm.T\n",
        "    max_1to2 = cos_sim.abs().max(dim=1)[0].mean().item()\n",
        "    max_2to1 = cos_sim.abs().max(dim=0)[0].mean().item()\n",
        "    return (max_1to2 + max_2to1) / 2\n",
        "\n",
        "\n",
        "def compute_random_baseline(d_model: int, d_sae: int, n_trials: int = 10) -> float:\n",
        "    \"\"\"Compute expected PWMCC for random matrices.\"\"\"\n",
        "    pwmcc_values = []\n",
        "    for _ in range(n_trials):\n",
        "        d1 = torch.randn(d_sae, d_model, device=device)\n",
        "        d2 = torch.randn(d_sae, d_model, device=device)\n",
        "        pwmcc_values.append(compute_pwmcc(d1, d2))\n",
        "    return np.mean(pwmcc_values)\n",
        "\n",
        "\n",
        "def compute_feature_overlap_stats(d1: torch.Tensor, d2: torch.Tensor) -> Dict:\n",
        "    \"\"\"Compute detailed feature overlap statistics.\"\"\"\n",
        "    d1_norm = F.normalize(d1, dim=1)\n",
        "    d2_norm = F.normalize(d2, dim=1)\n",
        "    cos_sim = d1_norm @ d2_norm.T\n",
        "    \n",
        "    max_sim = cos_sim.abs().max(dim=1)[0]\n",
        "    \n",
        "    return {\n",
        "        'mean_max_sim': max_sim.mean().item(),\n",
        "        'std_max_sim': max_sim.std().item(),\n",
        "        'pct_above_90': (max_sim > 0.9).float().mean().item() * 100,\n",
        "        'pct_above_80': (max_sim > 0.8).float().mean().item() * 100,\n",
        "        'pct_above_70': (max_sim > 0.7).float().mean().item() * 100,\n",
        "        'pct_above_50': (max_sim > 0.5).float().mean().item() * 100,\n",
        "    }\n",
        "\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear GPU memory.\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "print('âœ“ Utility functions loaded')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 1: Analyze Pretrained SAEs (Gemma Scope)\n",
        "\n",
        "We'll load pretrained SAEs from Gemma Scope and analyze:\n",
        "1. Feature overlap across different widths (dictionary sizes)\n",
        "2. Feature overlap across different layers"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4. Load Gemma Scope SAEs\n",
        "from sae_lens import SAE\n",
        "\n",
        "# Configuration\n",
        "MODEL = 'gemma-2-2b'  # Options: 'gemma-2-2b', 'gemma-2-9b'\n",
        "LAYER = 12  # Middle layer\n",
        "WIDTHS = ['16k', '32k', '65k']  # Different dictionary sizes\n",
        "\n",
        "print(f'Loading SAEs for {MODEL}, layer {LAYER}')\n",
        "print('=' * 50)\n",
        "\n",
        "saes = {}\n",
        "for width in WIDTHS:\n",
        "    print(f'\\nLoading width {width}...')\n",
        "    try:\n",
        "        sae_id = f'layer_{LAYER}/width_{width}/canonical'\n",
        "        sae, cfg, sparsity = SAE.from_pretrained(\n",
        "            release='gemma-scope-2b-pt-res-canonical',\n",
        "            sae_id=sae_id,\n",
        "            device=device\n",
        "        )\n",
        "        saes[width] = {\n",
        "            'sae': sae,\n",
        "            'd_sae': sae.cfg.d_sae,\n",
        "            'd_model': sae.cfg.d_in,\n",
        "            'sparsity': sparsity\n",
        "        }\n",
        "        print(f'  âœ“ d_sae={sae.cfg.d_sae:,}, d_model={sae.cfg.d_in}')\n",
        "        if sparsity:\n",
        "            print(f'  âœ“ Reported L0: {sparsity}')\n",
        "    except Exception as e:\n",
        "        print(f'  âœ— Error: {e}')\n",
        "\n",
        "print(f'\\nâœ“ Loaded {len(saes)} SAEs')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5. Analyze Feature Overlap Across Widths\n",
        "print('Feature Overlap Analysis Across Widths')\n",
        "print('=' * 60)\n",
        "\n",
        "width_results = []\n",
        "width_list = list(saes.keys())\n",
        "\n",
        "for i, w1 in enumerate(width_list):\n",
        "    for w2 in width_list[i+1:]:\n",
        "        d1 = saes[w1]['sae'].W_dec.data\n",
        "        d2 = saes[w2]['sae'].W_dec.data\n",
        "        \n",
        "        stats = compute_feature_overlap_stats(d1, d2)\n",
        "        \n",
        "        print(f'\\n{w1} ({saes[w1][\"d_sae\"]:,}) â†’ {w2} ({saes[w2][\"d_sae\"]:,}):')\n",
        "        print(f'  Mean max cosine sim: {stats[\"mean_max_sim\"]:.3f} Â± {stats[\"std_max_sim\"]:.3f}')\n",
        "        print(f'  Features with >0.9 match: {stats[\"pct_above_90\"]:.1f}%')\n",
        "        print(f'  Features with >0.8 match: {stats[\"pct_above_80\"]:.1f}%')\n",
        "        print(f'  Features with >0.7 match: {stats[\"pct_above_70\"]:.1f}%')\n",
        "        \n",
        "        width_results.append({\n",
        "            'width1': w1,\n",
        "            'width2': w2,\n",
        "            **stats\n",
        "        })"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 6. Analyze Cross-Layer Feature Overlap\n",
        "LAYERS = [6, 12, 18, 24]  # Early, middle, late layers\n",
        "WIDTH = '16k'  # Use smallest for memory efficiency\n",
        "\n",
        "print(f'Loading SAEs across layers: {LAYERS}')\n",
        "print('=' * 50)\n",
        "\n",
        "layer_saes = {}\n",
        "for layer in LAYERS:\n",
        "    print(f'Loading layer {layer}...')\n",
        "    try:\n",
        "        sae_id = f'layer_{layer}/width_{WIDTH}/canonical'\n",
        "        sae, cfg, sparsity = SAE.from_pretrained(\n",
        "            release='gemma-scope-2b-pt-res-canonical',\n",
        "            sae_id=sae_id,\n",
        "            device=device\n",
        "        )\n",
        "        layer_saes[layer] = sae\n",
        "        print(f'  âœ“ Loaded')\n",
        "    except Exception as e:\n",
        "        print(f'  âœ— Error: {e}')\n",
        "\n",
        "# Compute cross-layer PWMCC\n",
        "print('\\nCross-Layer Feature Similarity')\n",
        "print('=' * 50)\n",
        "\n",
        "layer_list = sorted(layer_saes.keys())\n",
        "d_model = layer_saes[layer_list[0]].cfg.d_in\n",
        "d_sae = layer_saes[layer_list[0]].cfg.d_sae\n",
        "random_baseline = compute_random_baseline(d_model, d_sae)\n",
        "print(f'Random baseline: {random_baseline:.4f}')\n",
        "\n",
        "layer_results = []\n",
        "for i in range(len(layer_list)):\n",
        "    for j in range(i + 1, len(layer_list)):\n",
        "        l1, l2 = layer_list[i], layer_list[j]\n",
        "        d1 = layer_saes[l1].W_dec.data\n",
        "        d2 = layer_saes[l2].W_dec.data\n",
        "        \n",
        "        pwmcc = compute_pwmcc(d1, d2)\n",
        "        ratio = pwmcc / random_baseline\n",
        "        \n",
        "        print(f'\\nLayer {l1} vs {l2}:')\n",
        "        print(f'  PWMCC: {pwmcc:.4f} ({ratio:.2f}Ã— random)')\n",
        "        \n",
        "        layer_results.append({\n",
        "            'layer1': l1,\n",
        "            'layer2': l2,\n",
        "            'pwmcc': pwmcc,\n",
        "            'ratio': ratio\n",
        "        })"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 2: Train SAEs with Different Seeds\n",
        "\n",
        "This is the key experiment: train multiple SAEs with the same configuration but different random seeds, then measure PWMCC between them."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 7. Setup SAE Training\n",
        "from transformer_lens import HookedTransformer\n",
        "from sae_lens import LanguageModelSAERunnerConfig, SAETrainingRunner\n",
        "\n",
        "# Clear memory from previous SAEs\n",
        "del saes, layer_saes\n",
        "clear_gpu_memory()\n",
        "\n",
        "# Load model\n",
        "print('Loading Gemma 2 2B...')\n",
        "model = HookedTransformer.from_pretrained(\n",
        "    'gemma-2-2b',\n",
        "    device=device,\n",
        "    dtype=torch.float16  # Use FP16 for memory efficiency\n",
        ")\n",
        "print(f'âœ“ Model loaded: {model.cfg.n_layers} layers, d_model={model.cfg.d_model}')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 8. Train Multiple SAEs with Different Seeds\n",
        "\n",
        "# Training configuration\n",
        "LAYER = 12\n",
        "EXPANSION_FACTOR = 4  # d_sae = 4 * d_model (smaller for faster training)\n",
        "L1_COEFFICIENT = 5e-3  # Controls sparsity\n",
        "TOTAL_TOKENS = 500_000  # Small for demo (increase for production)\n",
        "N_SEEDS = 3  # Number of seeds to train\n",
        "\n",
        "print(f'Training {N_SEEDS} SAEs with different seeds')\n",
        "print(f'Layer: {LAYER}, Expansion: {EXPANSION_FACTOR}x, L1: {L1_COEFFICIENT}')\n",
        "print(f'Total tokens: {TOTAL_TOKENS:,}')\n",
        "print('=' * 60)\n",
        "\n",
        "trained_saes = []\n",
        "seeds = [42, 123, 456]\n",
        "\n",
        "for i, seed in enumerate(seeds[:N_SEEDS]):\n",
        "    print(f'\\n[{i+1}/{N_SEEDS}] Training with seed {seed}...')\n",
        "    \n",
        "    cfg = LanguageModelSAERunnerConfig(\n",
        "        model_name='gemma-2-2b',\n",
        "        hook_point=f'blocks.{LAYER}.hook_resid_post',\n",
        "        hook_point_layer=LAYER,\n",
        "        d_in=model.cfg.d_model,\n",
        "        dataset_path='monology/pile-uncopyrighted',\n",
        "        streaming=True,\n",
        "        \n",
        "        # SAE architecture\n",
        "        expansion_factor=EXPANSION_FACTOR,\n",
        "        b_dec_init_method='zeros',\n",
        "        \n",
        "        # Training\n",
        "        lr=3e-4,\n",
        "        l1_coefficient=L1_COEFFICIENT,\n",
        "        train_batch_size_tokens=4096,\n",
        "        context_size=128,\n",
        "        seed=seed,\n",
        "        \n",
        "        # Data loading\n",
        "        n_batches_in_buffer=32,\n",
        "        total_training_tokens=TOTAL_TOKENS,\n",
        "        store_batch_size_prompts=16,\n",
        "        \n",
        "        # Logging\n",
        "        log_to_wandb=False,\n",
        "    )\n",
        "    \n",
        "    runner = SAETrainingRunner(cfg)\n",
        "    sae = runner.run()\n",
        "    trained_saes.append({'seed': seed, 'sae': sae})\n",
        "    \n",
        "    print(f'  âœ“ Trained SAE with seed {seed}')\n",
        "    clear_gpu_memory()\n",
        "\n",
        "print(f'\\nâœ“ Trained {len(trained_saes)} SAEs')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 9. Compute Stability (PWMCC) Between Trained SAEs\n",
        "print('Computing PWMCC between trained SAEs')\n",
        "print('=' * 50)\n",
        "\n",
        "# Get decoder matrices\n",
        "decoders = []\n",
        "for item in trained_saes:\n",
        "    decoder = item['sae'].W_dec.data.clone()\n",
        "    decoders.append(decoder)\n",
        "\n",
        "# Compute random baseline\n",
        "d_sae, d_model = decoders[0].shape\n",
        "random_baseline = compute_random_baseline(d_model, d_sae)\n",
        "print(f'Random baseline: {random_baseline:.4f}')\n",
        "\n",
        "# Compute pairwise PWMCC\n",
        "pwmcc_values = []\n",
        "for i in range(len(decoders)):\n",
        "    for j in range(i + 1, len(decoders)):\n",
        "        pwmcc = compute_pwmcc(decoders[i], decoders[j])\n",
        "        ratio = pwmcc / random_baseline\n",
        "        pwmcc_values.append(pwmcc)\n",
        "        \n",
        "        print(f'\\nSeed {trained_saes[i][\"seed\"]} vs {trained_saes[j][\"seed\"]}:')\n",
        "        print(f'  PWMCC: {pwmcc:.4f} ({ratio:.2f}Ã— random)')\n",
        "\n",
        "# Summary\n",
        "mean_pwmcc = np.mean(pwmcc_values)\n",
        "std_pwmcc = np.std(pwmcc_values)\n",
        "mean_ratio = mean_pwmcc / random_baseline\n",
        "\n",
        "print('\\n' + '=' * 50)\n",
        "print('SUMMARY')\n",
        "print('=' * 50)\n",
        "print(f'Mean PWMCC: {mean_pwmcc:.4f} Â± {std_pwmcc:.4f}')\n",
        "print(f'Ratio to random: {mean_ratio:.2f}Ã—')\n",
        "\n",
        "if mean_ratio < 1.2:\n",
        "    print('\\nâš ï¸ Stability is near random baseline!')\n",
        "elif mean_ratio < 1.5:\n",
        "    print('\\nðŸ“Š Moderate stability above random')\n",
        "else:\n",
        "    print('\\nâœ“ Good stability above random')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 3: Stability vs Sparsity Analysis\n",
        "\n",
        "Train SAEs at different sparsity levels (L1 coefficients) and measure stability at each level."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 10. Stability vs Sparsity Experiment\n",
        "\n",
        "# Clear previous SAEs\n",
        "del trained_saes, decoders\n",
        "clear_gpu_memory()\n",
        "\n",
        "# L1 coefficients to test (higher = sparser)\n",
        "L1_COEFFICIENTS = [1e-3, 3e-3, 5e-3, 1e-2]\n",
        "N_SEEDS_PER_L1 = 2  # Train 2 SAEs per L1 to measure stability\n",
        "TOTAL_TOKENS = 300_000  # Smaller for speed\n",
        "\n",
        "print('Stability vs Sparsity Experiment')\n",
        "print(f'L1 coefficients: {L1_COEFFICIENTS}')\n",
        "print(f'Seeds per L1: {N_SEEDS_PER_L1}')\n",
        "print('=' * 60)\n",
        "\n",
        "stability_results = []\n",
        "\n",
        "for l1_coef in L1_COEFFICIENTS:\n",
        "    print(f'\\n--- L1 = {l1_coef} ---')\n",
        "    \n",
        "    saes_at_l1 = []\n",
        "    for seed in [42, 123][:N_SEEDS_PER_L1]:\n",
        "        print(f'  Training seed {seed}...')\n",
        "        \n",
        "        cfg = LanguageModelSAERunnerConfig(\n",
        "            model_name='gemma-2-2b',\n",
        "            hook_point=f'blocks.{LAYER}.hook_resid_post',\n",
        "            hook_point_layer=LAYER,\n",
        "            d_in=model.cfg.d_model,\n",
        "            dataset_path='monology/pile-uncopyrighted',\n",
        "            streaming=True,\n",
        "            expansion_factor=4,\n",
        "            b_dec_init_method='zeros',\n",
        "            lr=3e-4,\n",
        "            l1_coefficient=l1_coef,\n",
        "            train_batch_size_tokens=4096,\n",
        "            context_size=128,\n",
        "            seed=seed,\n",
        "            n_batches_in_buffer=32,\n",
        "            total_training_tokens=TOTAL_TOKENS,\n",
        "            store_batch_size_prompts=16,\n",
        "            log_to_wandb=False,\n",
        "        )\n",
        "        \n",
        "        runner = SAETrainingRunner(cfg)\n",
        "        sae = runner.run()\n",
        "        saes_at_l1.append(sae)\n",
        "        clear_gpu_memory()\n",
        "    \n",
        "    # Compute PWMCC between the two SAEs\n",
        "    d1 = saes_at_l1[0].W_dec.data\n",
        "    d2 = saes_at_l1[1].W_dec.data\n",
        "    pwmcc = compute_pwmcc(d1, d2)\n",
        "    \n",
        "    # Estimate L0 (would need actual data for precise measurement)\n",
        "    # For now, use L1 as proxy\n",
        "    \n",
        "    random_baseline = compute_random_baseline(d1.shape[1], d1.shape[0])\n",
        "    ratio = pwmcc / random_baseline\n",
        "    \n",
        "    print(f'  PWMCC: {pwmcc:.4f} ({ratio:.2f}Ã— random)')\n",
        "    \n",
        "    stability_results.append({\n",
        "        'l1_coefficient': l1_coef,\n",
        "        'pwmcc': pwmcc,\n",
        "        'random_baseline': random_baseline,\n",
        "        'ratio': ratio\n",
        "    })\n",
        "    \n",
        "    # Clean up\n",
        "    del saes_at_l1\n",
        "    clear_gpu_memory()\n",
        "\n",
        "print('\\nâœ“ Experiment complete')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 11. Visualize Results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Plot 1: Stability vs L1 coefficient\n",
        "ax1 = axes[0]\n",
        "l1s = [r['l1_coefficient'] for r in stability_results]\n",
        "ratios = [r['ratio'] for r in stability_results]\n",
        "\n",
        "ax1.plot(l1s, ratios, 'o-', markersize=10, linewidth=2, color='steelblue')\n",
        "ax1.axhline(y=1.0, color='red', linestyle='--', label='Random baseline')\n",
        "ax1.set_xlabel('L1 Coefficient (higher = sparser)', fontsize=12)\n",
        "ax1.set_ylabel('Stability (Ã— Random)', fontsize=12)\n",
        "ax1.set_title('Stability vs Sparsity (LLM SAEs)', fontsize=14)\n",
        "ax1.set_xscale('log')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: PWMCC values\n",
        "ax2 = axes[1]\n",
        "pwmccs = [r['pwmcc'] for r in stability_results]\n",
        "baselines = [r['random_baseline'] for r in stability_results]\n",
        "\n",
        "x = range(len(l1s))\n",
        "ax2.bar(x, pwmccs, alpha=0.7, label='PWMCC', color='steelblue')\n",
        "ax2.plot(x, baselines, 'r--', label='Random baseline', linewidth=2)\n",
        "ax2.set_xlabel('L1 Coefficient', fontsize=12)\n",
        "ax2.set_ylabel('PWMCC', fontsize=12)\n",
        "ax2.set_title('Raw PWMCC Values', fontsize=14)\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels([f'{l1:.0e}' for l1 in l1s])\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('llm_sae_stability_results.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('\\nâœ“ Figure saved to llm_sae_stability_results.png')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 12. Summary and Comparison with Algorithmic Tasks\n",
        "print('=' * 70)\n",
        "print('SUMMARY: LLM SAE Stability Analysis')\n",
        "print('=' * 70)\n",
        "\n",
        "print('\\nðŸ“Š Results:')\n",
        "for r in stability_results:\n",
        "    print(f\"  L1={r['l1_coefficient']:.0e}: PWMCC={r['pwmcc']:.4f} ({r['ratio']:.2f}Ã— random)\")\n",
        "\n",
        "# Compute correlation\n",
        "l1s = np.array([r['l1_coefficient'] for r in stability_results])\n",
        "ratios = np.array([r['ratio'] for r in stability_results])\n",
        "\n",
        "if len(l1s) > 2:\n",
        "    corr = np.corrcoef(np.log(l1s), ratios)[0, 1]\n",
        "    print(f'\\nðŸ“ˆ Correlation (log L1 vs Stability): {corr:.3f}')\n",
        "    \n",
        "    if corr < -0.5:\n",
        "        print('  â†’ Higher sparsity = LOWER stability (matches algorithmic tasks!)')\n",
        "    elif corr > 0.5:\n",
        "        print('  â†’ Higher sparsity = HIGHER stability (DIFFERENT from algorithmic tasks!)')\n",
        "    else:\n",
        "        print('  â†’ Weak relationship')\n",
        "\n",
        "print('\\n' + '=' * 70)\n",
        "print('COMPARISON WITH ALGORITHMIC TASKS')\n",
        "print('=' * 70)\n",
        "print('\\nAlgorithmic Tasks (Modular Arithmetic):')\n",
        "print('  - Stability DECREASES monotonically with L0')\n",
        "print('  - Correlation: -0.725 (strong negative)')\n",
        "print('  - Architecture-independent')\n",
        "\n",
        "print('\\nLLM SAEs (This Experiment):')\n",
        "if len(stability_results) > 0:\n",
        "    mean_ratio = np.mean(ratios)\n",
        "    if mean_ratio < 1.2:\n",
        "        print('  - Stability near random baseline')\n",
        "    else:\n",
        "        print(f'  - Stability {mean_ratio:.2f}Ã— random baseline')\n",
        "    \n",
        "    if len(l1s) > 2 and corr < -0.3:\n",
        "        print('  - Pattern MATCHES algorithmic tasks (higher sparsity = lower stability)')\n",
        "    elif len(l1s) > 2 and corr > 0.3:\n",
        "        print('  - Pattern DIFFERS from algorithmic tasks!')\n",
        "    else:\n",
        "        print('  - Inconclusive (need more data points)')\n",
        "\n",
        "print('\\n' + '=' * 70)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 13. Save Results\n",
        "results = {\n",
        "    'experiment': 'llm_sae_stability',\n",
        "    'model': 'gemma-2-2b',\n",
        "    'layer': LAYER,\n",
        "    'stability_vs_sparsity': stability_results,\n",
        "    'width_comparison': width_results if 'width_results' in dir() else [],\n",
        "    'layer_comparison': layer_results if 'layer_results' in dir() else [],\n",
        "}\n",
        "\n",
        "with open('llm_sae_stability_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print('âœ“ Results saved to llm_sae_stability_results.json')\n",
        "\n",
        "# Download files\n",
        "from google.colab import files\n",
        "files.download('llm_sae_stability_results.json')\n",
        "files.download('llm_sae_stability_results.png')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
